<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Apache Flink (incubating): Cluster Setup</title>
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="css/bootstrap.css">
    <link rel="stylesheet" href="css/bootstrap-lumen-custom.css">
    <link rel="stylesheet" href="css/syntax.css">
    <link rel="stylesheet" href="css/custom.css">
    <link href="css/main/main.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codetabs.js"></script>
  </head>
  <body>

    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="row">
      <div class="col-md-1 af-mobile-nav-bar">
	<a href="index.html" title="Home">
	  <img class="hidden-xs hidden-sm img-responsive"
	       src="img/logo.png" alt="Apache Flink Logo">
	</a>	
	<div class="row visible-xs">
	  <div class="col-xs-3">
	    <a href="index.html" title="Home">  
	      <img class="hidden-x hidden-sm img-responsive"
		   src="img/logo.png" alt="Apache Flink Logo">
	    </a>	
	  </div>
	  <div class="col-xs-5"></div>
	  <div class="col-xs-4">
	    <div class="af-mobile-btn">
	      <span class="glyphicon glyphicon-plus"></span>
	    </div>
	  </div>
	</div>
      </div>
      <!-- Navigation -->
      <div class="col-md-11">
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav">

	    <li>
	      <a href="index.html" class="">Documentation</a>
	    </li>

	    <li>
	      <a href="api/java/index.html">Javadoc</a>
	    </li>

	    <li>
	      <a href="api/scala/index.html#org.apache.flink.api.scala.package">Scaladoc</a>
	    </li>

	  </ul>
	</div>
      </div>
    </div>
  </div>
</nav>


    <div style="padding-top:120px" class="container">

      <div class="row">
        <div class="col-md-3">
          <ul>
  <li><a href="faq.html">FAQ</a></li>
  <li>Quickstart
    <ul>
      <li><a href="setup_quickstart.html">Setup</a></li>
      <li><a href="run_example_quickstart.html">Run Example</a></li>
      <li><a href="java_api_quickstart.html">Java API</a></li>
      <li><a href="scala_api_quickstart.html">Scala API</a></li>
    </ul>
  </li>

  <li>Setup &amp; Configuration
    <ul>
      <li><a href="local_setup.html">Local Setup</a></li>
      <li><a href="building.html">Build Flink</a></li>
      <li><a href="cluster_setup.html">Cluster Setup</a></li>
      <li><a href="yarn_setup.html">YARN Setup</a></li>
      <li><a href="config.html">Configuration</a></li>
    </ul>
  </li>

  <li>Programming Guides
    <ul>
      <li><a href="programming_guide.html">Programming Guide</a></li>
      <li><a href="dataset_transformations.html">DataSet Transformations</a></li>
      <li><a href="java8_programming_guide.html">Java 8 Programming Guide</a></li>
      <li><a href="streaming_guide.html">Streaming Guide</a></li>
      <li><a href="iterations.html">Iterations</a></li>
      <li><a href="spargel_guide.html">Spargel Graph API</a></li>
      <li><a href="hadoop_compatibility.html">Hadoop Compatibility</a></li>
    </ul>
  </li>

  <li>Examples
    <ul>
      <li><a href="examples.html">Bundled Examples</a></li>
      <li><a href="example_connectors.html">Connecting to other systems</a></li>
    </ul>
  </li>

  <li>Execution
    <ul>
      <li><a href="local_execution.html">Local/Debugging</a></li>
      <li><a href="cluster_execution.html">Cluster</a></li>
      <li><a href="cli.html">Command-Line Interface</a></li>
      <li><a href="web_client.html">Web Interface</a></li>
    </ul>
  </li>

  <li>Internals
    <ul>
      <li><a href="internal_overview.html">Overview</a></li>
    </ul>
  </li>
</ul>

        </div>  
        <div class="col-md-9">
          <h1>Cluster Setup</h1>
	  
          <ul id="markdown-toc">
  <li><a href="#preparing-the-cluster">Preparing the Cluster</a>    <ul>
      <li><a href="#software-requirements">Software Requirements</a></li>
      <li><a href="#configuring-remote-access-with-ssh">Configuring Remote Access with ssh</a></li>
      <li><a href="#setting-javahome-on-each-node">Setting JAVA_HOME on each Node</a></li>
    </ul>
  </li>
  <li><a href="#hadoop-distributed-filesystem-hdfs-setup">Hadoop Distributed Filesystem (HDFS) Setup</a>    <ul>
      <li><a href="#downloading-installing-and-configuring-hdfs">Downloading, Installing, and Configuring HDFS</a></li>
      <li><a href="#starting-hdfs">Starting HDFS</a></li>
    </ul>
  </li>
  <li><a href="#flink-setup">Flink Setup</a>    <ul>
      <li><a href="#configuring-the-cluster">Configuring the Cluster</a></li>
      <li><a href="#configuring-the-network-buffers">Configuring the Network Buffers</a></li>
      <li><a href="#configuring-temporary-io-directories">Configuring Temporary I/O Directories</a></li>
      <li><a href="#starting-flink">Starting Flink</a></li>
    </ul>
  </li>
</ul>

<p>This documentation is intended to provide instructions on how to run
Flink in a fully distributed fashion on a static (but possibly
heterogeneous) cluster.</p>

<p>This involves two steps. First, installing and configuring Flink and
second installing and configuring the <a href="http://hadoop.apache.org/">Hadoop Distributed
Filesystem</a> (HDFS).</p>

<h2 id="preparing-the-cluster">Preparing the Cluster</h2>

<h3 id="software-requirements">Software Requirements</h3>

<p>Flink runs on all <em>UNIX-like environments</em>, e.g. <strong>Linux</strong>, <strong>Mac OS X</strong>,
and <strong>Cygwin</strong> (for Windows) and expects the cluster to consist of <strong>one master
node</strong> and <strong>one or more worker nodes</strong>. Before you start to setup the system,
make sure you have the following software installed <strong>on each node</strong>:</p>

<ul>
  <li><strong>Java 1.6.x</strong> or higher,</li>
  <li><strong>ssh</strong> (sshd must be running to use the Flink scripts that manage
remote components)</li>
</ul>

<p>If your cluster does not fulfill these software requirements you will need to
install/upgrade it.</p>

<p>For example, on Ubuntu Linux, type in the following commands to install Java and
ssh:</p>

<div class="highlight"><pre><code class="language-bash">sudo apt-get install ssh 
sudo apt-get install openjdk-7-jre</code></pre></div>

<p>You can check the correct installation of Java by issuing the following command:</p>

<div class="highlight"><pre><code class="language-bash">java -version</code></pre></div>

<p>The command should output something comparable to the following on every node of
your cluster (depending on your Java version, there may be small differences):</p>

<div class="highlight"><pre><code class="language-bash">java version <span class="s2">&quot;1.6.0_22&quot;</span>
Java<span class="o">(</span>TM<span class="o">)</span> SE Runtime Environment <span class="o">(</span>build 1.6.0_22-b04<span class="o">)</span>
Java HotSpot<span class="o">(</span>TM<span class="o">)</span> 64-Bit Server VM <span class="o">(</span>build 17.1-b03, mixed mode<span class="o">)</span></code></pre></div>

<p>To make sure the ssh daemon is running properly, you can use the command</p>

<div class="highlight"><pre><code class="language-bash">ps aux <span class="p">|</span> grep sshd</code></pre></div>

<p>Something comparable to the following line should appear in the output
of the command on every host of your cluster:</p>

<div class="highlight"><pre><code class="language-bash">root       <span class="m">894</span>  0.0  0.0  <span class="m">49260</span>   <span class="m">320</span> ?        Ss   Jan09   0:13 /usr/sbin/sshd</code></pre></div>

<h3 id="configuring-remote-access-with-ssh">Configuring Remote Access with ssh</h3>

<p>In order to start/stop the remote processes, the master node requires access via
ssh to the worker nodes. It is most convenient to use ssh’s public key
authentication for this. To setup public key authentication, log on to the
master as the user who will later execute all the Flink components. <strong>The
same user (i.e. a user with the same user name) must also exist on all worker
nodes</strong>. For the remainder of this instruction we will refer to this user as
<em>flink</em>. Using the super user <em>root</em> is highly discouraged for security
reasons.</p>

<p>Once you logged in to the master node as the desired user, you must generate a
new public/private key pair. The following command will create a new
public/private key pair into the <em>.ssh</em> directory inside the home directory of
the user <em>flink</em>. See the ssh-keygen man page for more details. Note that
the private key is not protected by a passphrase.</p>

<div class="highlight"><pre><code class="language-bash">ssh-keygen -b <span class="m">2048</span> -P <span class="s1">&#39;&#39;</span> -f ~/.ssh/id_rsa</code></pre></div>

<p>Next, copy/append the content of the file <em>.ssh/id_rsa.pub</em> to your
authorized_keys file. The content of the authorized_keys file defines which
public keys are considered trustworthy during the public key authentication
process. On most systems the appropriate command is</p>

<div class="highlight"><pre><code class="language-bash">cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys</code></pre></div>

<p>On some Linux systems, the authorized keys file may also be expected by the ssh
daemon under <em>.ssh/authorized_keys2</em>. In either case, you should make sure the
file only contains those public keys which you consider trustworthy for each
node of cluster.</p>

<p>Finally, the authorized keys file must be copied to every worker node of your
cluster. You can do this by repeatedly typing in</p>

<div class="highlight"><pre><code class="language-bash">scp .ssh/authorized_keys &lt;worker&gt;:~/.ssh/</code></pre></div>

<p>and replacing <em>&lt;worker&gt;</em> with the host name of the respective worker node.
After having finished the copy process, you should be able to log on to each
worker node from your master node via ssh without a password.</p>

<h3 id="setting-javahome-on-each-node">Setting JAVA_HOME on each Node</h3>

<p>Flink requires the <code>JAVA_HOME</code> environment variable to be set on the
master and all worker nodes and point to the directory of your Java
installation.</p>

<p>You can set this variable in <code>conf/flink-conf.yaml</code> via the
<code>env.java.home</code> key.</p>

<p>Alternatively, add the following line to your shell profile. If you use the
<em>bash</em> shell (probably the most common shell), the shell profile is located in
<em>\~/.bashrc</em>:</p>

<div class="highlight"><pre><code class="language-bash"><span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/path/to/java_home/</code></pre></div>

<p>If your ssh daemon supports user environments, you can also add <code>JAVA_HOME</code> to
<em>.\~/.ssh/environment</em>. As super user <em>root</em> you can enable ssh user
environments with the following commands:</p>

<div class="highlight"><pre><code class="language-bash"><span class="nb">echo</span> <span class="s2">&quot;PermitUserEnvironment yes&quot;</span> &gt;&gt; /etc/ssh/sshd_config
/etc/init.d/ssh restart</code></pre></div>

<h2 id="hadoop-distributed-filesystem-hdfs-setup">Hadoop Distributed Filesystem (HDFS) Setup</h2>

<p>The Flink system currently uses the Hadoop Distributed Filesystem (HDFS)
to read and write data in a distributed fashion.</p>

<p>Make sure to have a running HDFS installation. The following instructions are
just a general overview of some required settings. Please consult one of the
many installation guides available online for more detailed instructions.</p>

<p>**Note that the following instructions are based on Hadoop 1.2 and might differ
**for Hadoop 2.</p>

<h3 id="downloading-installing-and-configuring-hdfs">Downloading, Installing, and Configuring HDFS</h3>

<p>Similar to the Flink system HDFS runs in a distributed fashion. HDFS
consists of a <strong>NameNode</strong> which manages the distributed file system’s meta
data. The actual data is stored by one or more <strong>DataNodes</strong>. For the remainder
of this instruction we assume the HDFS’s NameNode component runs on the master
node while all the worker nodes run an HDFS DataNode.</p>

<p>To start, log on to your master node and download Hadoop (which includes  HDFS)
from the Apache <a href="http://hadoop.apache.org/releases.html">Hadoop Releases</a> page.</p>

<p>Next, extract the Hadoop archive.</p>

<p>After having extracted the Hadoop archive, change into the Hadoop directory and
edit the Hadoop environment configuration file:</p>

<div class="highlight"><pre><code class="language-bash"><span class="nb">cd </span>hadoop-*
vi conf/hadoop-env.sh</code></pre></div>

<p>Uncomment and modify the following line in the file according to the path of
your Java installation.</p>

<div class="highlight"><pre><code>export JAVA_HOME=/path/to/java_home/
</code></pre></div>

<p>Save the changes and open the HDFS configuration file <em>conf/hdfs-site.xml</em>. HDFS
offers multiple configuration parameters which affect the behavior of the
distributed file system in various ways. The following excerpt shows a minimal
configuration which is required to make HDFS work. More information on how to
configure HDFS can be found in the <a href="http://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html">HDFS User
Guide</a> guide.</p>

<div class="highlight"><pre><code class="language-xml"><span class="nt">&lt;configuration&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.default.name<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>hdfs://MASTER:50040/<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>dfs.data.dir<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>DATAPATH<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span></code></pre></div>

<p>Replace <em>MASTER</em> with the IP/host name of your master node which runs the
<em>NameNode</em>. <em>DATAPATH</em> must be replaced with path to the directory in which the
actual HDFS data shall be stored on each worker node. Make sure that the
<em>flink</em> user has sufficient permissions to read and write in that
directory.</p>

<p>After having saved the HDFS configuration file, open the file <em>conf/slaves</em> and
enter the IP/host name of those worker nodes which shall act as <em>DataNode</em>s.
Each entry must be separated by a line break.</p>

<div class="highlight"><pre><code>&lt;worker 1&gt;
&lt;worker 2&gt;
.
.
.
&lt;worker n&gt;
</code></pre></div>

<p>Initialize the HDFS by typing in the following command. Note that the
command will <strong>delete all data</strong> which has been previously stored in the
HDFS. However, since we have just installed a fresh HDFS, it should be
safe to answer the confirmation with <em>yes</em>.</p>

<div class="highlight"><pre><code class="language-bash">bin/hadoop namenode -format</code></pre></div>

<p>Finally, we need to make sure that the Hadoop directory is available to
all worker nodes which are intended to act as DataNodes and that all nodes
<strong>find the directory under the same path</strong>. We recommend to use a shared network
directory (e.g. an NFS share) for that. Alternatively, one can copy the
directory to all nodes (with the disadvantage that all configuration and
code updates need to be synced to all nodes).</p>

<h3 id="starting-hdfs">Starting HDFS</h3>

<p>To start the HDFS log on to the master and type in the following
commands</p>

<div class="highlight"><pre><code class="language-bash"><span class="nb">cd </span>hadoop-*
binn/start-dfs.sh</code></pre></div>

<p>If your HDFS setup is correct, you should be able to open the HDFS
status website at <em>http://MASTER:50070</em>. In a matter of a seconds,
all DataNodes should appear as live nodes. For troubleshooting we would
like to point you to the <a href="http://wiki.apache.org/hadoop/QuickStart">Hadoop Quick
Start</a>
guide.</p>

<h2 id="flink-setup">Flink Setup</h2>

<p>Go to the <a href="/downloads.html">downloads page</a> and get the ready to run
package. Make sure to pick the Flink package <strong>matching your Hadoop
version</strong>.</p>

<p>After downloading the latest release, copy the archive to your master node and
extract it:</p>

<div class="highlight"><pre><code class="language-bash">tar xzf flink-*.tgz
<span class="nb">cd </span>flink-*</code></pre></div>

<h3 id="configuring-the-cluster">Configuring the Cluster</h3>

<p>After having extracted the system files, you need to configure Flink for
the cluster by editing <em>conf/flink-conf.yaml</em>.</p>

<p>Set the <code>jobmanager.rpc.address</code> key to point to your master node. Furthermode
define the maximum amount of main memory the JVM is allowed to allocate on each
node by setting the <code>jobmanager.heap.mb</code> and <code>taskmanager.heap.mb</code> keys.</p>

<p>The value is given in MB. If some worker nodes have more main memory which you
want to allocate to the Flink system you can overwrite the default value
by setting an environment variable <code>FLINK_TM_HEAP</code> on the respective
node.</p>

<p>Finally you must provide a list of all nodes in your cluster which shall be used
as worker nodes. Therefore, similar to the HDFS configuration, edit the file
<em>conf/slaves</em> and enter the IP/host name of each worker node. Each worker node
will later run a TaskManager.</p>

<p>Each entry must be separated by a new line, as in the following example:</p>

<div class="highlight"><pre><code>192.168.0.100
192.168.0.101
.
.
.
192.168.0.150
</code></pre></div>

<p>The Flink directory must be available on every worker under the same
path. Similarly as for HDFS, you can use a shared NSF directory, or copy the
entire Flink directory to every worker node.</p>

<h3 id="configuring-the-network-buffers">Configuring the Network Buffers</h3>

<p>Network buffers are a critical resource for the communication layers. They are
used to buffer records before transmission over a network, and to buffer
incoming data before dissecting it into records and handing them to the
application. A sufficient number of network buffers are critical to achieve a
good throughput.</p>

<p>In general, configure the task manager to have so many buffers that each logical
network connection on you expect to be open at the same time has a dedicated
buffer. A logical network connection exists for each point-to-point exchange of
data over the network, which typically happens at repartitioning- or
broadcasting steps. In those, each parallel task inside the TaskManager has to
be able to talk to all other parallel tasks. Hence, the required number of
buffers on a task manager is <em>total-degree-of-parallelism</em> (number of targets)
* <em>intra-node-parallelism</em> (number of sources in one task manager) * <em>n</em>.
Here, <em>n</em> is a constant that defines how many repartitioning-/broadcasting steps
you expect to be active at the same time.</p>

<p>Since the <em>intra-node-parallelism</em> is typically the number of cores, and more
than 4 repartitioning or broadcasting channels are rarely active in parallel, it
frequently boils down to <em>#cores\^2\^</em> * <em>#machines</em> * 4. To support for
example a cluster of 20 8-core machines, you should use roughly 5000 network
buffers for optimal throughput.</p>

<p>Each network buffer is by default 64 KiBytes large. In the above example, the
system would allocate roughly 300 MiBytes for network buffers.</p>

<p>The number and size of network buffers can be configured with the following
parameters:</p>

<ul>
  <li><code>taskmanager.network.numberOfBuffers</code>, and</li>
  <li><code>taskmanager.network.bufferSizeInBytes</code>.</li>
</ul>

<h3 id="configuring-temporary-io-directories">Configuring Temporary I/O Directories</h3>

<p>Although Flink aims to process as much data in main memory as possible,
it is not uncommon that  more data needs to be processed than memory is
available. Flink’s runtime is designed to  write temporary data to disk
to handle these situations.</p>

<p>The <code>taskmanager.tmp.dirs</code> parameter specifies a list of directories into which
Flink writes temporary files. The paths of the directories need to be
separated by ‘:’ (colon character).  Flink will concurrently write (or
read) one temporary file to (from) each configured directory.  This way,
temporary I/O can be evenly distributed over multiple independent I/O devices
such as hard disks to improve performance.  To leverage fast I/O devices (e.g.,
SSD, RAID, NAS), it is possible to specify a directory multiple times.</p>

<p>If the <code>taskmanager.tmp.dirs</code> parameter is not explicitly specified,
Flink writes temporary data to the temporary  directory of the operating
system, such as <em>/tmp</em> in Linux systems.</p>

<p>Please see the <a href="config.html">configuration page</a> for details and additional
configuration options.</p>

<h3 id="starting-flink">Starting Flink</h3>

<p>The following script starts a JobManager on the local node and connects via
SSH to all worker nodes listed in the <em>slaves</em> file to start the
TaskManager on each node. Now your Flink system is up and
running. The JobManager running on the local node will now accept jobs
at the configured RPC port.</p>

<p>Assuming that you are on the master node and inside the Flink directory:</p>

<div class="highlight"><pre><code class="language-bash">bin/start-cluster.sh</code></pre></div>

	  
        <!-- Disqus Area -->
          <div style="padding-top:30px" id="disqus_thread"></div>
      
            <script type="text/javascript">
                /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
                var disqus_shortname = 'stratosphere-eu'; // required: replace example with your forum shortname

                /* * * DON'T EDIT BELOW THIS LINE * * */
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
          </div>
        </div>

        <div class="footer">
          
          <hr class="divider" />

<p><small>Apache Flink is an effort undergoing incubation at The Apache Software
Foundation (ASF), sponsored by the Apache Incubator PMC. Incubation is
required of all newly accepted projects until a further review indicates that
the infrastructure, communications, and decision making process have
stabilized in a manner consistent with other successful ASF projects. While
incubation status is not necessarily a reflection of the completeness or
stability of the code, it does indicate that the project has yet to be fully
endorsed by the ASF.</small></p>

<p><a href="http://incubator.apache.org/"><img src="/img/apache-incubator-logo.png" alt="Incubator Logo" /></a></p>

<p class="text-center"><a href="privacy-policy.html">Privacy Policy<a>
</a></a></p>

        </div>
      </div>
    </div>

    

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');

    </script>

  </body>
</html>
