---
layout: post
title:  "Flink 1.0.1 Released"
date:   2016-04-06 08:00:00
categories: news
---

Today, the Flink community released Flink version **1.0.1**, the first bugfix release of the 1.0 series.

We **recommend all users updating to this release** by bumping the version of your Flink dependencies to `1.0.1` and updating the binaries on the server. You can find the binaries on the updated [Downloads page]({{ site.baseurl }}/downloads.html).

## Fixed Issues

<h3>Bug</h3>
<ul>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3179'>FLINK-3179</a>] -         Combiner is not injected if Reduce or GroupReduce input is explicitly partitioned
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3472'>FLINK-3472</a>] -         JDBCInputFormat.nextRecord(..) has misleading message on NPE
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3491'>FLINK-3491</a>] -         HDFSCopyUtilitiesTest fails on Windows
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3495'>FLINK-3495</a>] -         RocksDB Tests can&#39;t run on Windows
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3533'>FLINK-3533</a>] -         Update the Gelly docs wrt examples and cluster execution
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3563'>FLINK-3563</a>] -         .returns() doesn&#39;t compile when using .map() with a custom MapFunction
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3566'>FLINK-3566</a>] -         Input type validation often fails on custom TypeInfo implementations
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3578'>FLINK-3578</a>] -         Scala DataStream API does not support Rich Window Functions
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3595'>FLINK-3595</a>] -         Kafka09 consumer thread does not interrupt when stuck in record emission
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3602'>FLINK-3602</a>] -         Recursive Types are not supported / crash TypeExtractor
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3621'>FLINK-3621</a>] -         Misleading documentation of memory configuration parameters
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3629'>FLINK-3629</a>] -         In wikiedits Quick Start example, &quot;The first call, .window()&quot; should be &quot;The first call, .timeWindow()&quot;
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3651'>FLINK-3651</a>] -         Fix faulty RollingSink Restore
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3653'>FLINK-3653</a>] -         recovery.zookeeper.storageDir is not documented on the configuration page
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3663'>FLINK-3663</a>] -         FlinkKafkaConsumerBase.logPartitionInfo is missing a log marker
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3681'>FLINK-3681</a>] -         CEP library does not support Java 8 lambdas as select function
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3682'>FLINK-3682</a>] -         CEP operator does not set the processing timestamp correctly
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3684'>FLINK-3684</a>] -         CEP operator does not forward watermarks properly
</li>
</ul>

<h3>Improvement</h3>
<ul>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3570'>FLINK-3570</a>] -         Replace random NIC selection heuristic by InetAddress.getLocalHost
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3575'>FLINK-3575</a>] -         Update Working With State Section in Doc
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-3591'>FLINK-3591</a>] -         Replace Quickstart K-Means Example by Streaming Example
</li>
</ul>

<h2>Test</h2>
<ul>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-2444'>FLINK-2444</a>] -         Add tests for HadoopInputFormats
</li>
<li>[<a href='https://issues.apache.org/jira/browse/FLINK-2445'>FLINK-2445</a>] -         Add tests for HadoopOutputFormats
</li>
</ul>
