<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink: Blog</title>
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/flink.css">
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Blog RSS feed -->
    <link href="/blog/feed.xml" rel="alternate" type="application/rss+xml" title="Apache Flink Blog: RSS feed" />

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!-- We need to load Jquery in the header for custom google analytics event tracking-->
    <script src="/js/jquery.min.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>  
    

    <!-- Main content. -->
    <div class="container">
    <div class="row">

      
     <div id="sidebar" class="col-sm-3">
        

<!-- Top navbar. -->
    <nav class="navbar navbar-default">
        <!-- The logo. -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="navbar-logo">
            <a href="/">
              <img alt="Apache Flink" src="/img/flink-header-logo.svg" width="147px" height="73px">
            </a>
          </div>
        </div><!-- /.navbar-header -->

        <!-- The navigation links. -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav navbar-main">

            <!-- First menu section explains visitors what Flink is -->

            <!-- What is Stream Processing? -->
            <!--
            <li><a href="/streamprocessing1.html">What is Stream Processing?</a></li>
            -->

            <!-- What is Flink? -->
            <li><a href="/flink-architecture.html">What is Apache Flink?</a></li>

            

            <!-- What is Stateful Functions? -->

            <li><a href="/stateful-functions.html">What is Stateful Functions?</a></li>

            <!-- Use cases -->
            <li><a href="/usecases.html">Use Cases</a></li>

            <!-- Powered by -->
            <li><a href="/poweredby.html">Powered By</a></li>


            &nbsp;
            <!-- Second menu section aims to support Flink users -->

            <!-- Downloads -->
            <li><a href="/downloads.html">Downloads</a></li>

            <!-- Getting Started -->
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Getting Started<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/index.html" target="_blank">With Flink <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/getting-started/project-setup.html" target="_blank">With Flink Stateful Functions <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="/training.html">Training Course</a></li>
              </ul>
            </li>

            <!-- Documentation -->
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Documentation<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12" target="_blank">Flink 1.12 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-master" target="_blank">Flink Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2" target="_blank">Flink Stateful Functions 2.2 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-master" target="_blank">Flink Stateful Functions Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
              </ul>
            </li>

            <!-- getting help -->
            <li><a href="/gettinghelp.html">Getting Help</a></li>

            <!-- Blog -->
            <li class="active"><a href="/blog/"><b>Flink Blog</b></a></li>


            <!-- Flink-packages -->
            <li>
              <a href="https://flink-packages.org" target="_blank">flink-packages.org <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>
            &nbsp;

            <!-- Third menu section aim to support community and contributors -->

            <!-- Community -->
            <li><a href="/community.html">Community &amp; Project Info</a></li>

            <!-- Roadmap -->
            <li><a href="/roadmap.html">Roadmap</a></li>

            <!-- Contribute -->
            <li><a href="/contributing/how-to-contribute.html">How to Contribute</a></li>
            

            <!-- GitHub -->
            <li>
              <a href="https://github.com/apache/flink" target="_blank">Flink on GitHub <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>

            &nbsp;

            <!-- Language Switcher -->
            <li>
              
                
                  <!-- link to the Chinese home page when current is blog page -->
                  <a href="/zh">中文版</a>
                
              
            </li>

          </ul>

          <style>
            .smalllinks:link {
              display: inline-block !important; background: none; padding-top: 0px; padding-bottom: 0px; padding-right: 0px; min-width: 75px;
            }
          </style>

          <ul class="nav navbar-nav navbar-bottom">
          <hr />

            <!-- Twitter -->
            <li><a href="https://twitter.com/apacheflink" target="_blank">@ApacheFlink <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <!-- Visualizer -->
            <li class=" hidden-md hidden-sm"><a href="/visualizer/" target="_blank">Plan Visualizer <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <li >
                  <a href="/security.html">Flink Security</a>
            </li>

          <hr />

            <li><a href="https://apache.org" target="_blank">Apache Software Foundation <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <li>

              <a class="smalllinks" href="https://www.apache.org/licenses/" target="_blank">License</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/security/" target="_blank">Security</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/foundation/sponsorship.html" target="_blank">Donate</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/foundation/thanks.html" target="_blank">Thanks</a> <small><span class="glyphicon glyphicon-new-window"></span></small>
            </li>

          </ul>
        </div><!-- /.navbar-collapse -->
    </nav>

      </div>
      <div class="col-sm-9">
      <h1>Blog</h1>
<hr />

<div class="row">
  <div class="col-sm-8">
    <!-- Blog posts -->
    
    <article>
      <h2 class="blog-title"><a href="/news/2019/08/22/release-1.9.0.html">Apache Flink 1.9.0 Release Announcement</a></h2>

      <p>22 Aug 2019
      </p>

      <p><p>The Apache Flink community is proud to announce the release of Apache Flink
1.9.0.</p>

<p>The Apache Flink project’s goal is to develop a stream processing system to
unify and power many forms of real-time and offline data processing
applications as well as event-driven applications. In this release, we have
made a huge step forward in that effort, by integrating Flink’s stream and
batch processing capabilities under a single, unified runtime.</p>

<p>Significant features on this path are batch-style recovery for batch jobs and
a preview of the new Blink-based query engine for Table API and SQL queries.
We are also excited to announce the availability of the State Processor API,
which is one of the most frequently requested features and enables users to
read and write savepoints with Flink DataSet jobs. Finally, Flink 1.9 includes
a reworked WebUI and previews of Flink’s new Python Table API and its
integration with the Apache Hive ecosystem.</p>

<p>This blog post describes all major new features and improvements, important
changes to be aware of and what to expect moving forward. For more details,
check the <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12344601">complete release
changelog</a>.</p>

<p>The binary distribution and source artifacts for this release are now
available via the <a href="https://flink.apache.org/downloads.html">Downloads</a> page of
the Flink project, along with the updated
<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/">documentation</a>.
Flink 1.9 is API-compatible with previous 1.x releases for APIs annotated with
the <code>@Public</code> annotation.</p>

<p>Please feel encouraged to download the release and share your thoughts with
the community through the Flink <a href="https://flink.apache.org/community.html#mailing-lists">mailing
lists</a> or
<a href="https://issues.apache.org/jira/projects/FLINK/summary">JIRA</a>. As always,
feedback is very much appreciated!</p>

<div class="page-toc">
<ul id="markdown-toc">
  <li><a href="#new-features-and-improvements" id="markdown-toc-new-features-and-improvements">New Features and Improvements</a>    <ul>
      <li><a href="#fine-grained-batch-recovery-flip-1" id="markdown-toc-fine-grained-batch-recovery-flip-1">Fine-grained Batch Recovery (FLIP-1)</a></li>
      <li><a href="#state-processor-api-flip-43" id="markdown-toc-state-processor-api-flip-43">State Processor API (FLIP-43)</a></li>
      <li><a href="#stop-with-savepoint-flip-34" id="markdown-toc-stop-with-savepoint-flip-34">Stop-with-Savepoint (FLIP-34)</a></li>
      <li><a href="#flink-webui-rework" id="markdown-toc-flink-webui-rework">Flink WebUI Rework</a></li>
      <li><a href="#preview-of-the-new-blink-sql-query-processor" id="markdown-toc-preview-of-the-new-blink-sql-query-processor">Preview of the new Blink SQL Query Processor</a></li>
      <li><a href="#preview-of-full-hive-integration-flink-10556" id="markdown-toc-preview-of-full-hive-integration-flink-10556">Preview of Full Hive Integration (FLINK-10556)</a></li>
      <li><a href="#preview-of-the-new-python-table-api-flip-38" id="markdown-toc-preview-of-the-new-python-table-api-flip-38">Preview of the new Python Table API (FLIP-38)</a></li>
    </ul>
  </li>
  <li><a href="#important-changes" id="markdown-toc-important-changes">Important Changes</a></li>
  <li><a href="#release-notes" id="markdown-toc-release-notes">Release Notes</a></li>
  <li><a href="#list-of-contributors" id="markdown-toc-list-of-contributors">List of Contributors</a></li>
</ul>

</div>

<h2 id="new-features-and-improvements">New Features and Improvements</h2>

<h3 id="fine-grained-batch-recovery-flip-1">Fine-grained Batch Recovery (FLIP-1)</h3>

<p>The time to recover a batch (DataSet, Table API and SQL) job from a task
failure was significantly reduced. Until Flink 1.9, task failures in batch
jobs were recovered by canceling all tasks and restarting the whole job, i.e,
the job was started from scratch and all progress was voided. With this
release, Flink can be configured to limit the recovery to only those tasks
that are in the same <strong>failover region</strong>. A failover region is the set of
tasks that are connected via pipelined data exchanges. Hence, the
batch-shuffle connections of a job define the boundaries of its failover
regions. More details are available in
<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures">FLIP-1</a>.
<img src="/img/blog/release-19-flip1.png" alt="alt_text" title="Fine-grained Batch
Recovery" /></p>

<p>To use this new failover strategy, you need to do the following
settings:</p>

<ul>
  <li>Make sure you have the entry <code>jobmanager.execution.failover-strategy:
region</code> in your <code>flink-conf.yaml</code>.</li>
</ul>

<p><strong>Note:</strong> The configuration of the 1.9 distribution has that entry by default,
  but when reusing a configuration file from previous setups, you have to add
  it manually.</p>

<p>Moreover, you need to set the <code>ExecutionMode</code> of batch jobs in the
<code>ExecutionConfig</code> to <code>BATCH</code> to configure that data shuffles are not pipelined
and jobs have more than one failover region.</p>

<p>The “Region” failover strategy also improves the recovery of “embarrassingly
parallel” streaming jobs, i.e., jobs without any shuffle like keyBy() or
rebalance. When such a job is recovered, only the tasks of the affected
pipeline (failover region) are restarted. For all other streaming jobs, the
recovery behavior is the same as in prior Flink versions.</p>

<h3 id="state-processor-api-flip-43">State Processor API (FLIP-43)</h3>

<p>Up to Flink 1.9, accessing the state of a job from the outside was limited to
the (still) experimental <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/queryable_state.html">Queryable
State</a>.
This release introduces a new, powerful library to read, write and modify
state snapshots using the batch DataSet API. In practice, this means:</p>

<ul>
  <li>Flink job state can be bootstrapped by reading data from external systems,
such as external databases, and converting it into a savepoint.</li>
  <li>State in savepoints can be queried using any of Flink’s batch APIs
(DataSet, Table, SQL), for example to analyze relevant state patterns or
check for discrepancies in state that can support application auditing or
troubleshooting.</li>
  <li>The schema of state in savepoints can be migrated offline, compared to the
previous approach requiring online migration on schema access.</li>
  <li>Invalid data in savepoints can be identified and corrected.</li>
</ul>

<p>The new State Processor API covers all variations of snapshots: savepoints,
full checkpoints and incremental checkpoints. More details are available in
<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-43%3A+State+Processor+API">FLIP-43</a></p>

<h3 id="stop-with-savepoint-flip-34">Stop-with-Savepoint (FLIP-34)</h3>

<p><a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/savepoints.html#operations">Cancelling with a
savepoint</a>
is a common operation for stopping/restarting, forking or updating Flink jobs.
However, the existing implementation did not guarantee output persistence to
external storage systems for exactly-once sinks. To improve the end-to-end
semantics when stopping a job, Flink 1.9 introduces a new <code>SUSPEND</code> mode to
stop a job with a savepoint that is consistent with the emitted data.
You can suspend a job with Flink’s CLI client as follows:</p>

<div class="highlight"><pre><code>bin/flink stop -p [:targetDirectory] :jobId
</code></pre></div>

<p>The final job state is set to <code>FINISHED</code> on success, allowing
users to detect failures of the requested operation.</p>

<p>More details are available in
<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103090212">FLIP-34</a></p>

<h3 id="flink-webui-rework">Flink WebUI Rework</h3>

<p>After a
<a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Change-underlying-Frontend-Architecture-for-Flink-Web-Dashboard-td24902.html">discussion</a>
about modernizing the internals of Flink’s WebUI, this component was
reconstructed using the latest stable version of Angular — basically, a bump
from Angular 1.x to 7.x. The redesigned version is the default in 1.9.0,
however there is a link to switch to the old WebUI.</p>

<div class="row"> <div class="col-sm-6"> <span><img class="thumbnail" src="/img/blog/release-19-web1.png" /></span> </div> <div class="col-sm-6"> <span><img class="thumbnail" src="/img/blog/release-19-web2.png" /></span> </div>
    </div>

<p><strong>Note:</strong> Moving forward, feature parity for the old version of the WebUI 
will not be guaranteed.</p>

<h3 id="preview-of-the-new-blink-sql-query-processor">Preview of the new Blink SQL Query Processor</h3>

<p>Following the <a href="/news/2019/02/13/unified-batch-streaming-blink.html">donation of
Blink</a> to
Apache Flink, the community worked on integrating Blink’s query optimizer and
runtime for the Table API and SQL. As a first step, we refactored the
monolithic <code>flink-table</code> module into smaller modules
(<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions">FLIP-32</a>).
This resulted in a clear separation of and well-defined interfaces between the
Java and Scala API modules and the optimizer and runtime modules.</p>

<p><span><img style="width:50%" src="/img/blog/release-19-stack.png" /></span></p>

<p>Next, we extended Blink’s planner to implement the new optimizer interface
such that there are now two pluggable query processors to execute Table API
and SQL statements: the pre-1.9 Flink processor and the new Blink-based query
processor. The Blink-based query processor offers better SQL coverage (full TPC-H
coverage in 1.9, TPC-DS coverage is planned for the next release) and improved
performance for batch queries as the result of more extensive query
optimization (cost-based plan selection and more optimization rules), improved
code-generation, and tuned operator implementations.
The Blink-based query processor also provides a more powerful streaming runner,
with some new features (e.g. dimension table join, TopN, deduplication) and 
optimizations to solve data-skew in aggregation and more useful built-in
functions.</p>

<p><strong>Note:</strong> The semantics and set of supported operations of the query
processors are mostly, but not fully aligned.</p>

<p>However, the integration of Blink’s query processor is not fully completed
yet. Therefore, the pre-1.9 Flink processor is still the default processor in
Flink 1.9 and recommended for production settings. You can enable the Blink
processor by configuring it via the <code>EnvironmentSettings</code> when creating a
<code>TableEnvironment</code>. The selected processor must be on the classpath of the
executing Java process. For cluster setups, both query processors are
automatically loaded with the default configuration. When running a query from
your IDE you need to explicitly <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/#table-program-dependencies">add a planner
dependency</a>
to your project.</p>

<h4 id="other-improvements-to-the-table-api-and-sql"><strong>Other Improvements to the Table API and SQL</strong></h4>

<p>Besides the exciting progress around the Blink planner, the community worked
on a whole set of other improvements to these interfaces, including:</p>

<ul>
  <li>
    <p><strong>Scala-free Table API and SQL for Java users
(<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions">FLIP-32</a>)</strong></p>

    <p>As part of the refactoring and splitting of the flink-table module, two
separate API modules for Java and Scala were created. For Scala users,
nothing really changes, but Java users can use the Table API and/or SQL now
without pulling in a Scala dependency.</p>
  </li>
  <li>
    <p><strong>Rework of the Table API Type System</strong>
<strong>(<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-37%3A+Rework+of+the+Table+API+Type+System">FLIP-37</a>)</strong></p>

    <p>The community implemented a <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/types.html#data-types">new data type
system</a>
to detach the Table API from Flink’s
<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/types_serialization.html#flinks-typeinformation-class">TypeInformation</a>
class and improve its compliance with the SQL standard. This is still a
work in progress and expected to be completed in the next release. In
Flink 1.9, UDFs are―among other things―not ported to the new type system
yet.</p>
  </li>
  <li>
    <p><strong>Multi-column and Multi-row Transformations for Table API</strong>
<strong>(<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97552739">FLIP-29</a>)</strong></p>

    <p>The functionality of the Table API was extended with a set of
transformations that support multi-row and/or multi-column inputs and
outputs. These transformations significantly ease the implementation of
processing logic that would be cumbersome to implement with relational
operators.</p>
  </li>
  <li>
    <p><strong>New, Unified Catalog APIs</strong>
<strong>(<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-30%3A+Unified+Catalog+APIs">FLIP-30</a>)</strong></p>

    <p>We reworked the catalog APIs to store metadata and unified the handling of
internal and external catalogs. This effort was mainly initiated as a
prerequisite for the Hive integration (see below), but improves the overall
convenience of managing catalog metadata in Flink. Besides improving the
catalog interfaces, we also extended their functionality. Previously table
definitions for Table API or SQL queries were volatile. With Flink 1.9, the
metadata of tables which are registered with a SQL DDL statement can be
persisted in a catalog. This means you can add a table that is backed by a
Kafka topic to a Metastore catalog and from then on query this table
whenever your catalog is connected to Metastore.</p>
  </li>
  <li>
    <p><strong>DDL Support in the SQL API
(<a href="https://issues.apache.org/jira/browse/FLINK-10232">FLINK-10232</a>)</strong></p>

    <p>Up to this point, Flink SQL only supported DML statements (e.g. <code>SELECT</code>,
<code>INSERT</code>). External tables (table sources and sinks) had to be registered
via Java/Scala code or configuration files. For 1.9, we added support for
SQL DDL statements to register and remove tables and views (<code>CREATE TABLE,
DROP TABLE)</code>. However, we did not add
stream-specific syntax extensions to define timestamp extraction and
watermark generation, yet. Full support for streaming use cases is planned
for the next release.</p>
  </li>
</ul>

<h3 id="preview-of-full-hive-integration-flink-10556">Preview of Full Hive Integration (FLINK-10556)</h3>

<p>Apache Hive is widely used in Hadoop’s ecosystem to store and query large
amounts of structured data. Besides being a query processor, Hive features a
catalog called Metastore to manage and organize large datasets. A common
integration point for query processors is to integrate with Hive’s Metastore
in order to be able to tap into the data managed by Hive.</p>

<p>Recently, the community started implementing an external catalog for Flink’s
Table API and SQL that connects to Hive’s Metastore. In Flink 1.9, users will
be able to query and process all data that is stored in Hive. As described
earlier, you will also be able to persist metadata of Flink tables in Metastore.
Moreover, the Hive integration includes support to use Hive’s UDFs in Flink
Table API or SQL queries. More details are available in
<a href="https://issues.apache.org/jira/browse/FLINK-10556">FLINK-10556</a>.</p>

<p>While, previously, table definitions for Table API or SQL queries were always
volatile, the new catalog connector additionally allows persisting a table in
Metastore that is created with a SQL DDL statement (see above). This means
that you connect to Metastore and register a table that is, for example,
backed by a Kafka topic. From now on, you can query that table whenever your
catalog is connected to Metastore.</p>

<p>Please note that the Hive support in Flink 1.9 is experimental. We are
planning to stabilize these features for the next release and are looking
forward to your feedback.</p>

<h3 id="preview-of-the-new-python-table-api-flip-38">Preview of the new Python Table API (FLIP-38)</h3>

<p>This release also introduces a first version of a Python Table API
(<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-38%3A+Python+Table+API">FLIP-38</a>).
This marks the start towards our goal of bringing
full-fledged Python support to Flink. The feature was designed as a slim
Python API wrapper around the Table API, basically translating Python Table
API method calls into Java Table API calls. In the initial version that ships
with Flink 1.9, the Python Table API does not support UDFs yet, but just
standard relational operations. Support for UDFs implemented in Python is on
the roadmap for future releases.</p>

<p>If you’d like to try the new Python API, you have to manually <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/flinkDev/building.html#build-pyflink">install
PyFlink</a>.
From there, you can have a look at <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/tutorials/python_table_api.html">this
walkthrough</a>
or explore it on your own. The <a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Publish-the-PyFlink-into-PyPI-td31201.html">community is currently
working</a>
on preparing a <code>pyflink</code> Python package that will be made available for
installation via <code>pip</code>.</p>

<h2 id="important-changes">Important Changes</h2>

<ul>
  <li>The Table API and SQL are now part of the default configuration of the
Flink distribution. Before, the Table API and SQL had to be enabled by
moving the corresponding JAR file from ./opt to ./lib.</li>
  <li>The machine learning library (flink-ml) has been removed in preparation for
<a href="https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo/edit">FLIP-39</a>.</li>
  <li>The old DataSet and DataStream Python APIs have been removed in favor of
<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-38%3A+Python+Table+API">FLIP-38</a>.</li>
  <li>Flink can be compiled and run on Java 9. Note that certain components
interacting with external systems (connectors, filesystems, reporters) may
not work since the respective projects may have skipped Java 9 support.</li>
</ul>

<h2 id="release-notes">Release Notes</h2>

<p>Please review the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/release-notes/flink-1.9.html">release
notes</a>
for a more detailed list of changes and new features if you plan to upgrade
your Flink setup to Flink 1.9.0.</p>

<h2 id="list-of-contributors">List of Contributors</h2>

<p>We would like to thank all contributors who have made this release possible:</p>

<p>Abdul Qadeer (abqadeer), Aitozi, Alberto Romero, Aleksey Pak, Alexander
Fedulov, Alice Yan, Aljoscha Krettek, Aloys, Andrew Duffy, Andrey Zagrebin,
Ankur, Artsem Semianenka, Benchao Li, Biao Liu, Bo WANG, Bowen L, Chesnay
Schepler, Clark Yang, Congxian Qiu, Cristian, Danny Chan, David Moravek, Dawid
Wysakowicz, Dian Fu, EronWright, Fabian Hueske, Fabio Lombardelli, Fokko
Driesprong, Gao Yun, Gary Yao, Gen Luo, Gyula Fora, Hequn Cheng,
Hongtao Zhang, Huang Xingbo, HuangXingBo, Hugo Da Cruz Louro, Humberto
Rodríguez A, Hwanju Kim, Igal Shilman, Jamie Grier, Jark Wu, Jason, Jasper
Yue, Jeff Zhang, Jiangjie (Becket) Qin, Jiezhi.G, Jincheng Sun, Jing Zhang,
Jingsong Lee, Juan Gentile, Jungtaek Lim, Kailash Dayanand, Kevin
Bohinski, Konstantin Knauf, Konstantinos Papadopoulos, Kostas Kloudas, Kurt
Young, Lakshmi, Lakshmi Gururaja Rao, Leeviiii, LouisXu, Maximilian Michels,
Nico Kruber, Niels Basjes, Paul Lam, PengFei Li, Peter Huang, Pierre Zemb,
Piotr Nowojski, Piyush Narang, Richard Deurwaarder, Robert Metzger, Robert
Stoll, Romano Vacca, Rong Rong, Rui Li, Ryantaocer, Scott Mitchell, Seth
Wiesman, Shannon Carey, Shimin Yang, Stefan Richter, Stephan Ewen, Stephen
Connolly, Steven Wu, SuXingLee, TANG Wen-hui, Thomas Weise, Till Rohrmann,
Timo Walther, Tom Goong, TsReaper, Tzu-Li (Gordon) Tai, Ufuk Celebi,
Victor Wong, WangHengwei, Wei Zhong, WeiZhong94, Xintong Song, Xpray,
XuQianJin-Stars, Xuefu Zhang, Xupingyong, Yangze Guo, Yu Li, Yun Gao, Yun
Tang, Zhanchun Zhang, Zhenghua Gao, Zhijiang, Zhu Zhu, Zili
Chen, aloys, arganzheng, azagrebin, bd2019us, beyond1920, biao.liub,
blueszheng, boshu Zheng, chenqi, chummyhe89, chunpinghe, dcadmin,
dianfu, godfrey he, guanghui01.rong, hehuiyuan, hello, hequn8128, 
jackyyin, joongkeun.yang, klion26, lamber-ken, leesf, liguowei,
lincoln-lil, liyafan82, luoqi, mans2singh, maqingxiang, maxin, mjl, okidogi,
ozan, potseluev, qiangsi.lq, qiaoran, robbinli, shaoxuan-wang, shengqian.zhou,
shenlang.sl, shuai-xu, sunhaibotb, tianchen, tianchen92,
tison, tom_gong, vinoyang, vthinkxie, wanggeng3, wenhuitang, winifredtamg,
xl38154, xuyang1706, yangfei5, yanghua, yuzhao.cyz,
zhangxin516, zhangxinxing, zhaofaxian, zhijiang, zjuwangg, 林小铂,
黄培松, 时无两丶.</p>
</p>

      <p><a href="/news/2019/08/22/release-1.9.0.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2019/07/23/flink-network-stack-2.html">Flink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing</a></h2>

      <p>23 Jul 2019
       Nico Kruber  &amp; Piotr Nowojski </p>

      <p>In a previous blog post, we presented how Flink’s network stack works from the high-level abstractions to the low-level details. This second  post discusses monitoring network-related metrics to identify backpressure or bottlenecks in throughput and latency.</p>

      <p><a href="/2019/07/23/flink-network-stack-2.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2019/07/02/release-1.8.1.html">Apache Flink 1.8.1 Released</a></h2>

      <p>02 Jul 2019
       Jincheng Sun (<a href="https://twitter.com/sunjincheng121">@sunjincheng121</a>)</p>

      <p><p>The Apache Flink community released the first bugfix version of the Apache Flink 1.8 series.</p>

<p>This release includes more than 40 fixes and minor improvements for Flink 1.8.1. The list below includes a detailed list of all improvements, sub-tasks and bug fixes.</p>

<p>We highly recommend all users to upgrade to Flink 1.8.1.</p>

<p>Updated Maven dependencies:</p>

<div class="highlight"><pre><code class="language-xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-java<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.8.1<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-streaming-java_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.8.1<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-clients_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.8.1<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></div>

<p>You can find the binaries on the updated <a href="/downloads.html">Downloads page</a>.</p>

<p>List of resolved issues:</p>

<h2>        Sub-task
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-10921">FLINK-10921</a>] -         Prioritize shard consumers in Kinesis Consumer by event time 
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12617">FLINK-12617</a>] -         StandaloneJobClusterEntrypoint should default to random JobID for non-HA setups 
</li>
</ul>

<h2>        Bug
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-9445">FLINK-9445</a>] -         scala-shell uses plain java command
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-10455">FLINK-10455</a>] -         Potential Kafka producer leak in case of failures
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-10941">FLINK-10941</a>] -         Slots prematurely released which still contain unconsumed data 
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-11059">FLINK-11059</a>] -         JobMaster may continue using an invalid slot if releasing idle slot meet a timeout
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-11107">FLINK-11107</a>] -         Avoid memory stateBackend to create arbitrary folders under HA path when no checkpoint path configured
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-11897">FLINK-11897</a>] -         ExecutionGraphSuspendTest does not wait for all tasks to be submitted
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-11915">FLINK-11915</a>] -         DataInputViewStream skip returns wrong value
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-11987">FLINK-11987</a>] -         Kafka producer occasionally throws NullpointerException
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12009">FLINK-12009</a>] -         Wrong check message about heartbeat interval for HeartbeatServices
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12042">FLINK-12042</a>] -         RocksDBStateBackend mistakenly uses default filesystem
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12112">FLINK-12112</a>] -         AbstractTaskManagerProcessFailureRecoveryTest process output logging does not work properly
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12132">FLINK-12132</a>] -         The example in /docs/ops/deployment/yarn_setup.md should be updated due to the change FLINK-2021
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12184">FLINK-12184</a>] -         HistoryServerArchiveFetcher isn&#39;t compatible with old version
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12219">FLINK-12219</a>] -         Yarn application can&#39;t stop when flink job failed in per-job yarn cluster mode
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12247">FLINK-12247</a>] -         fix NPE when writing an archive file to a FileSystem
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12260">FLINK-12260</a>] -         Slot allocation failure by taskmanager registration timeout and race
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12296">FLINK-12296</a>] -         Data loss silently in RocksDBStateBackend when more than one operator(has states) chained in a single task 
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12297">FLINK-12297</a>] -         Make ClosureCleaner recursive
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12301">FLINK-12301</a>] -         Scala value classes inside case classes cannot be serialized anymore in Flink 1.8.0
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12342">FLINK-12342</a>] -         Yarn Resource Manager Acquires Too Many Containers
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12375">FLINK-12375</a>] -         flink-container job jar does not have read permissions
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12416">FLINK-12416</a>] -         Docker build script fails on symlink creation ln -s
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12544">FLINK-12544</a>] -         Deadlock while releasing memory and requesting segment concurrent in SpillableSubpartition
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12547">FLINK-12547</a>] -         Deadlock when the task thread downloads jars using BlobClient
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12646">FLINK-12646</a>] -         Use reserved IP as unrouteable IP in RestClientTest
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12688">FLINK-12688</a>] -         Make serializer lazy initialization thread safe in StateDescriptor
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12740">FLINK-12740</a>] -         SpillableSubpartitionTest deadlocks on Travis
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12835">FLINK-12835</a>] -         Time conversion is wrong in ManualClock
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12863">FLINK-12863</a>] -         Race condition between slot offerings and AllocatedSlotReport
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12865">FLINK-12865</a>] -         State inconsistency between RM and TM on the slot status
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12871">FLINK-12871</a>] -         Wrong SSL setup examples in docs
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12895">FLINK-12895</a>] -         TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerProcessFailure failed on travis 
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12896">FLINK-12896</a>] -         TaskCheckpointStatisticDetailsHandler uses wrong value for JobID when archiving
</li>
</ul>

<h2>        Improvement
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-11126">FLINK-11126</a>] -         Filter out AMRMToken in the TaskManager credentials
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12137">FLINK-12137</a>] -         Add more proper explanation on flink streaming connectors 
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12169">FLINK-12169</a>] -         Improve Javadoc of MessageAcknowledgingSourceBase
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12378">FLINK-12378</a>] -         Consolidate FileSystem Documentation
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12391">FLINK-12391</a>] -         Add timeout to transfer.sh
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12539">FLINK-12539</a>] -         StreamingFileSink: Make the class extendable to customize for different usecases
</li>
</ul>

<h2>        Test
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12350">FLINK-12350</a>] -         RocksDBStateBackendTest doesn&#39;t cover the incremental checkpoint code path
</li>
</ul>

<h2>        Task
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-12460">FLINK-12460</a>] -         Change taskmanager.tmp.dirs to io.tmp.dirs in configuration docs
</li>
</ul>

</p>

      <p><a href="/news/2019/07/02/release-1.8.1.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2019/06/26/broadcast-state.html">A Practical Guide to Broadcast State in Apache Flink</a></h2>

      <p>26 Jun 2019
       Fabian Hueske (<a href="https://twitter.com/fhueske">@fhueske</a>)</p>

      <p>Apache Flink has multiple types of operator state, one of which is called Broadcast State. In this post, we explain what Broadcast State is, and show an example of how it can be applied to an application that evaluates dynamic patterns on an event stream.</p>

      <p><a href="/2019/06/26/broadcast-state.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2019/06/05/flink-network-stack.html">A Deep-Dive into Flink's Network Stack</a></h2>

      <p>05 Jun 2019
       Nico Kruber </p>

      <p>Flink’s network stack is one of the core components that make up Apache Flink's runtime module sitting at the core of every Flink job. In this post, which is the first in a series of posts about the network stack, we look at the abstractions exposed to the stream operators and detail their physical implementation and various optimisations in Apache Flink.</p>

      <p><a href="/2019/06/05/flink-network-stack.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2019/05/19/state-ttl.html">State TTL in Flink 1.8.0: How to Automatically Cleanup Application State in Apache Flink</a></h2>

      <p>19 May 2019
       Fabian Hueske (<a href="https://twitter.com/fhueske">@fhueske</a>) &amp; Andrey Zagrebin </p>

      <p>A common requirement for many stateful streaming applications is to automatically cleanup application state for effective management of your state size, or to control how long the application state can be accessed. State TTL enables application state cleanup and efficient state size management in Apache Flink</p>

      <p><a href="/2019/05/19/state-ttl.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2019/05/14/temporal-tables.html">Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL</a></h2>

      <p>14 May 2019
       Marta Paes (<a href="https://twitter.com/morsapaes">@morsapaes</a>)</p>

      <p>Apache Flink natively supports temporal table joins since the 1.7 release for straightforward temporal data handling. In this blog post, we provide an overview of how this new concept can be leveraged for effective point-in-time analysis in streaming scenarios.</p>

      <p><a href="/2019/05/14/temporal-tables.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2019/05/03/pulsar-flink.html">When Flink & Pulsar Come Together</a></h2>

      <p>03 May 2019
       Sijie Guo (<a href="https://twitter.com/sijieg">@sijieg</a>)</p>

      <p>Apache Flink and Apache Pulsar are distributed data processing systems. When combined, they offer elastic data processing at large scale. This post describes how Pulsar and Flink can work together to provide a seamless developer experience.</p>

      <p><a href="/2019/05/03/pulsar-flink.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2019/04/17/sod.html">Apache Flink's Application to Season of Docs</a></h2>

      <p>17 Apr 2019
       Konstantin Knauf (<a href="https://twitter.com/snntrable">@snntrable</a>)</p>

      <p><p>The Apache Flink community is happy to announce its application to the first edition of <a href="https://developers.google.com/season-of-docs/">Season of Docs</a> by Google. The program is bringing together Open Source projects and technical writers to raise awareness for and improve documentation of Open Source projects. While the community is continuously looking for new contributors to collaborate on our documentation, we would like to take this chance to work with one or two technical writers to extend and restructure parts of <a href="https://ci.apache.org/projects/flink/flink-docs-stable/">our documentation</a> (details below).</p>

<p>The community has discussed this opportunity on the <a href="https://lists.apache.org/thread.html/3c789b6187da23ad158df59bbc598543b652e3cfc1010a14e294e16a@%3Cdev.flink.apache.org%3E">dev mailinglist</a> and agreed on three project ideas to submit to the program. We have a great team of mentors (Stephan, Fabian, David, Jark &amp; Konstantin) lined up and are very much looking forward to the first proposals by potential technical writers (given we are admitted to the program ;)). In case of questions feel free to reach out to the community via <a href="../../../../community.html#mailing-lists">dev@flink.apache.org</a>.</p>

<h2 id="project-ideas-list">Project Ideas List</h2>

<h3 id="project-1-improve-documentation-of-stream-processing-concepts">Project 1: Improve Documentation of Stream Processing Concepts</h3>

<p><strong>Description:</strong> Stream processing is the processing of data in motion―in other words, computing on data directly as it is produced or received. Apache Flink has pioneered the field of distributed, stateful stream processing over the last several years. As the community has pushed the boundaries of stream processing, we have introduced new concepts that users need to become familiar with to develop and operate Apache Flink applications efficiently.
The Apache Flink documentation [1] already contains a “concepts” section, but it is a ) incomplete and b) lacks an overall structure &amp; reading flow. In addition, “concepts”-content is also spread over the development [2] &amp; operations [3] documentation without references to the “concepts” section. An example of this can be found in [4] and [5].</p>

<p>In this project, we would like to restructure, consolidate and extend the concepts documentation for Apache Flink to better guide users who want to become productive as quickly as possible. This includes better conceptual introductions to topics such as event time, state, and fault tolerance with proper linking to and from relevant deployment and development guides.</p>

<p><strong>Related material:</strong></p>

<ol>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/">https://ci.apache.org/projects/flink/flink-docs-release-1.8/</a></li>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev">https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev</a></li>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops</a></li>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/programming-model.html#time">https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/programming-model.html#time</a></li>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/event_time.html">https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/event_time.html</a></li>
</ol>

<h3 id="project-2-improve-documentation-of-flink-deployments--operations">Project 2: Improve Documentation of Flink Deployments &amp; Operations</h3>

<p><strong>Description:</strong> Stream processing is the processing of data in motion―in other words, computing on data directly as it is produced or received. Apache Flink has pioneered the field of distributed, stateful stream processing for the last few years. As a stateful distributed system in general and a continuously running, low-latency system in particular, Apache Flink deployments are non-trivial to setup and manage.
Unfortunately, the operations [1] and monitoring documentation [2] are arguably the weakest spots of the Apache Flink documentation. While it is comprehensive and often goes into a lot of detail, it lacks an overall structure and does not address common overarching concerns of operations teams in an efficient way.</p>

<p>In this project, we would like to restructure this part of the documentation and extend it if possible. Ideas for extension include: discussion of session and per-job clusters, better documentation for containerized deployments (incl. K8s), capacity planning &amp; integration into CI/CD pipelines.</p>

<p><strong>Related material:</strong></p>

<ol>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops</a></li>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring">https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring</a></li>
</ol>

<h3 id="project-3-improve-documentation-for-relational-apis-table-api--sql">Project 3: Improve Documentation for Relational APIs (Table API &amp; SQL)</h3>

<p><strong>Description:</strong> Apache Flink features APIs at different levels of abstraction which enables its users to trade conciseness for expressiveness. Flink’s relational APIs, SQL and the Table API, are “younger” than the DataStream and DataSet APIs, more high-level and focus on data analytics use cases. A core principle of Flink’s SQL and Table API is that they can be used to process static (batch) and continuous (streaming) data and that a program or query produces the same result in both cases.
The documentation of Flink’s relational APIs has organically grown and can be improved in a few areas. There are several on-going development efforts (e.g. Hive Integration, Python Support or Support for Interactive Programming) that aim to extend the scope of the Table API and SQL.</p>

<p>The existing documentation could be reorganized to prepare for covering the new features. Moreover, it could be improved by adding a concepts section that describes the use cases and internals of the APIs in more detail. Moreover, the documentation of built-in functions could be improved by adding more concrete examples.</p>

<p><strong>Related material:</strong></p>

<ol>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/table">Table API &amp; SQL docs main page</a></li>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/table/functions.html">Built-in functions</a></li>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/table/common.html">Concepts</a></li>
  <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/table/streaming/">Streaming Concepts</a></li>
</ol>

</p>

      <p><a href="/news/2019/04/17/sod.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2019/04/09/release-1.8.0.html">Apache Flink 1.8.0 Release Announcement</a></h2>

      <p>09 Apr 2019
       Aljoscha Krettek (<a href="https://twitter.com/aljoscha">@aljoscha</a>)</p>

      <p><p>The Apache Flink community is pleased to announce Apache Flink 1.8.0.  The
latest release includes more than 420 resolved issues and some exciting
additions to Flink that we describe in the following sections of this post.
Please check the <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12344274">complete changelog</a>
for more details.</p>

<p>Flink 1.8.0 is API-compatible with previous 1.x.y releases for APIs annotated
with the <code>@Public</code> annotation.  The release is available now and we encourage
everyone to <a href="/downloads.html">download the release</a> and
check out the updated
<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/">documentation</a>.
Feedback through the Flink <a href="/community.html#mailing-lists">mailing
lists</a> or
<a href="https://issues.apache.org/jira/projects/FLINK/summary">JIRA</a> is, as always,
very much appreciated!</p>

<p>You can find the binaries on the updated <a href="/downloads.html">Downloads page</a> on the Flink project site.</p>

<div class="page-toc">
<ul id="markdown-toc">
  <li><a href="#new-features-and-improvements" id="markdown-toc-new-features-and-improvements">New Features and Improvements</a></li>
  <li><a href="#important-changes" id="markdown-toc-important-changes">Important Changes</a></li>
  <li><a href="#known-issues" id="markdown-toc-known-issues">Known Issues</a></li>
  <li><a href="#release-notes" id="markdown-toc-release-notes">Release Notes</a></li>
  <li><a href="#list-of-contributors" id="markdown-toc-list-of-contributors">List of Contributors</a></li>
</ul>

</div>

<p>With Flink 1.8.0 we come closer to our goals of enabling fast data processing
and building data-intensive applications for the Flink community in a seamless
way. We do this by cleaning up and refactoring Flink under the hood to allow
more efficient feature development in the future. This includes removal of the
legacy runtime components that were subsumed in the major rework of Flink’s
underlying distributed system architecture
(<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077">FLIP-6</a>)
as well as refactorings on the Table API that prepare it for the future
addition of the Blink enhancements
(<a href="https://issues.apache.org/jira/browse/FLINK-11439">FLINK-11439</a>).</p>

<p>Nevertheless, this release includes some important new features and bug fixes.
The most interesting of those are highlighted below. Please consult the
<a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12344274">complete changelog</a>
and the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/release-notes/flink-1.8.html">release notes</a>
for more details.</p>

<h2 id="new-features-and-improvements">New Features and Improvements</h2>

<ul>
  <li>
    <p><strong>Finalized State Schema Evolution Story</strong>: This release completes
the community driven effort to provide a schema evolution story for
user state managed by Flink. This has been an effort that spanned 2
releases, starting from 1.7.0 with the introduction of support for
Avro state schema evolution as well as a revamped serialization
compatibility abstraction.</p>

    <p>Flink 1.8.0 finalizes this effort by extending support for schema
evolution to POJOs, upgrading all Flink built-in serializers to use
the new serialization compatibility abstractions, as well as making it
easier for advanced users who use custom state serializers to
implement the abstractions.  These different aspects for a complete
out-of-the-box schema evolution story are explained in detail below:</p>

    <ol>
      <li>
        <p>Support for POJO state schema evolution: The pool of data types
that support state schema evolution has been expanded to include
POJOs. For state types that use POJOs, you can now add or remove
fields from your POJO while retaining backwards
compatibility. For a full overview of the list of data types that
now support schema evolution as well as their evolution
specifications and limitations, please refer to the State Schema
Evolution documentation page.</p>
      </li>
      <li>
        <p>Upgrade all Flink serializers to use new serialization
compatibility asbtractions: Back in 1.7.0, we introduced the new
serialization compatibility abstractions <code>TypeSerializerSnapshot</code>
and <code>TypeSerializerSchemaCompatibility</code>. Besides providing a more
expressible API to reflect schema compatibility between the data
stored in savepoints and the data registered at runtime, another
important aspect about the new abstraction is that it avoids the
need for Flink to Java-serialize the state serializer as state
metadata in savepoints.</p>

        <p>In 1.8.0, all of Flink’s built-in serializers have been upgraded to
use the new abstractions, and therefore the serializers
themselves are no longer Java-serialized into savepoints. This
greatly improves interoperability of Flink savepoints, in terms
of state schema evolvability. For example, one outcome was the
support for POJO schema evolution, as previously mentioned
above. Another outcome is that all composite data types supported
by Flink (such as <code>Either</code>, Scala case classes, Flink Java
<code>Tuple</code>s, etc.) are generally evolve-able as well when they have
a nested evolvable type, such as a POJO. For example, the <code>MyPojo</code>
type in <code>ValueState&lt;Tuple2&lt;Integer, MyPojo&gt;&gt;</code> or
<code>ListState&lt;Either&lt;Integer, MyPojo&gt;&gt;</code>, which is a POJO, is allowed
to evolve its schema.</p>

        <p>For users who are using custom <code>TypeSerializer</code> implementations
for their state serializer and are still using the outdated
abstractions (i.e. <code>TypeSerializerConfigSnapshot</code> and
<code>CompatiblityResult</code>), we highly recommend upgrading to the new
abstractions to be future proof. Please refer to the Custom State
Serialization documentation page for a detailed description on
the new abstractions.</p>
      </li>
      <li>
        <p>Provide pre-defined snapshot implementations for common
serializers: For convenience, Flink 1.8.0 comes with two
predefined implementations for the <code>TypeSerializerSnapshot</code> that
make the task of implementing these new abstractions easier
for most implementations of <code>TypeSerializer</code>s -
<code>SimpleTypeSerializerSnapshot</code> and
<code>CompositeTypeSerializerSnapshot</code>. This section in the
documentation provides information on how to use these classes.</p>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>Continuous cleanup of old state based on TTL
(<a href="https://issues.apache.org/jira/browse/FLINK-7811">FLINK-7811</a>)</strong>: We
introduced TTL (time-to-live) for Keyed state in Flink 1.6
(<a href="https://issues.apache.org/jira/browse/FLINK-9510">FLINK-9510</a>). This
feature enabled cleanup and made keyed state entries inaccessible after a
defined timeout. In addition state would now also be cleaned up when
writing a savepoint/checkpoint.</p>

    <p>Flink 1.8 introduces continuous cleanup of old entries for both the RocksDB
state backend
(<a href="https://issues.apache.org/jira/browse/FLINK-10471">FLINK-10471</a>) and the heap
state backend
(<a href="https://issues.apache.org/jira/browse/FLINK-10473">FLINK-10473</a>). This means
that old entries (according to the TTL setting) are continuously cleaned up.</p>
  </li>
  <li>
    <p><strong>SQL pattern detection with user-defined functions and
aggregations</strong>: The support of the MATCH_RECOGNIZE clause has been
extended by multiple features.  The addition of user-defined
functions allows for custom logic during pattern detection
(<a href="https://issues.apache.org/jira/browse/FLINK-10597">FLINK-10597</a>),
while adding aggregations allows for more complex CEP definitions,
such as the following
(<a href="https://issues.apache.org/jira/browse/FLINK-7599">FLINK-7599</a>).</p>

    <div class="highlight"><pre><code>SELECT *
FROM Ticker
    MATCH_RECOGNIZE (
        ORDER BY rowtime
        MEASURES
            AVG(A.price) AS avgPrice
        ONE ROW PER MATCH
        AFTER MATCH SKIP TO FIRST B
        PATTERN (A+ B)
        DEFINE
            A AS AVG(A.price) &lt; 15
    ) MR;
</code></pre></div>
  </li>
  <li>
    <p><strong>RFC-compliant CSV format (<a href="https://issues.apache.org/jira/browse/FLINK-9964">FLINK-9964</a>)</strong>: The SQL tables can now be read and written in
an RFC-4180 standard compliant CSV table format. The format might also be
useful for general DataStream API users.</p>
  </li>
  <li>
    <p><strong>New KafkaDeserializationSchema that gives direct access to ConsumerRecord
(<a href="https://issues.apache.org/jira/browse/FLINK-8354">FLINK-8354</a>)</strong>: For the
Flink <code>KafkaConsumers</code>, we introduced a new <code>KafkaDeserializationSchema</code> that
gives direct access to the Kafka <code>ConsumerRecord</code>. This now allows access to
all data that Kafka provides for a record, including the headers. This
subsumes the <code>KeyedSerializationSchema</code> functionality, which is deprecated but
still available for now.</p>
  </li>
  <li>
    <p><strong>Per-shard watermarking option in FlinkKinesisConsumer
(<a href="https://issues.apache.org/jira/browse/FLINK-5697">FLINK-5697</a>)</strong>: The Kinesis
Consumer can now emit periodic watermarks that are derived from per-shard watermarks,
for correct event time processing with subtasks that consume multiple Kinesis shards.</p>
  </li>
  <li>
    <p><strong>New consumer for DynamoDB Streams to capture table changes
(<a href="https://issues.apache.org/jira/browse/FLINK-4582">FLINK-4582</a>)</strong>: <code>FlinkDynamoDBStreamsConsumer</code>
is a variant of the Kinesis consumer that supports retrieval of CDC-like streams from DynamoDB tables.</p>
  </li>
  <li>
    <p><strong>Support for global aggregates for subtask coordination
(<a href="https://issues.apache.org/jira/browse/FLINK-10887">FLINK-10887</a>)</strong>:
Designed as a solution for global source watermark tracking, <code>GlobalAggregateManager</code>
allows sharing of information between parallel subtasks. This feature will
be integrated into streaming connectors for watermark synchronization and
can be used for other purposes with a user defined aggregator.</p>
  </li>
</ul>

<h2 id="important-changes">Important Changes</h2>

<ul>
  <li>
    <p><strong>Changes to bundling of Hadoop libraries with Flink
(<a href="https://issues.apache.org/jira/browse/FLINK-11266">FLINK-11266</a>)</strong>:
Convenience binaries that include hadoop are no longer released.</p>

    <p>If a deployment relies on <code>flink-shaded-hadoop2</code> being included in
<code>flink-dist</code>, then you must manually download a pre-packaged Hadoop
jar from the optional components section of the <a href="/downloads.html">download
page</a> and copy it into the
<code>/lib</code> directory.  Alternatively, a Flink distribution that includes
hadoop can be built by packaging <code>flink-dist</code> and activating the
<code>include-hadoop</code> maven profile.</p>

    <p>As hadoop is no longer included in <code>flink-dist</code> by default, specifying
<code>-DwithoutHadoop</code> when packaging <code>flink-dist</code> no longer impacts the build.</p>
  </li>
  <li>
    <p><strong>FlinkKafkaConsumer will now filter restored partitions based on topic
specification
(<a href="https://issues.apache.org/jira/browse/FLINK-10342">FLINK-10342</a>)</strong>:
Starting from Flink 1.8.0, the <code>FlinkKafkaConsumer</code> now always filters out
restored partitions that are no longer associated with a specified topic to
subscribe to in the restored execution. This behaviour did not exist in
previous versions of the <code>FlinkKafkaConsumer</code>. If you wish to retain the
previous behaviour, please use the
<code>disableFilterRestoredPartitionsWithSubscribedTopics()</code> configuration method
on the <code>FlinkKafkaConsumer</code>.</p>

    <p>Consider this example: if you had a Kafka Consumer that was consuming from
topic <code>A</code>, you did a savepoint, then changed your Kafka consumer to instead
consume from topic <code>B</code>, and then restarted your job from the savepoint.
Before this change, your consumer would now consume from both topic <code>A</code> and
<code>B</code> because it was stored in state that the consumer was consuming from topic
<code>A</code>. With the change, your consumer would only consume from topic <code>B</code> after
restore because it now filters the topics that are stored in state using the
configured topics.</p>
  </li>
  <li>
    <p><strong>Change in the Maven modules of Table API
(<a href="https://issues.apache.org/jira/browse/FLINK-11064">FLINK-11064</a>)</strong>: Users
that had a <code>flink-table</code> dependency before, need to update their
dependencies to <code>flink-table-planner</code> and the correct dependency of
<code>flink-table-api-*</code>, depending on whether Java or Scala is used: one of
<code>flink-table-api-java-bridge</code> or <code>flink-table-api-scala-bridge</code>.</p>
  </li>
</ul>

<h2 id="known-issues">Known Issues</h2>

<ul>
  <li><strong>Discarded checkpoint can cause Tasks to fail
(<a href="https://issues.apache.org/jira/browse/FLINK-11662">FLINK-11662</a>)</strong>: There is
a race condition that can lead to erroneous checkpoint failures. This mostly
occurs when restarting from a savepoint or checkpoint takes a long time at the
sources of a job. If you see random checkpointing failures that don’t seem to
have a good explanation you might be affected. Please see the Jira issue for
more details and a workaround for the problem.</li>
</ul>

<h2 id="release-notes">Release Notes</h2>

<p>Please review the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/release-notes/flink-1.8.html">release
notes</a>
for a more detailed list of changes and new features if you plan to upgrade
your Flink setup to Flink 1.8.</p>

<h2 id="list-of-contributors">List of Contributors</h2>

<p>We would like to acknowledge all community members for contributing to this
release.  Special credits go to the following members for contributing to the
1.8.0 release (according to <code>git log --pretty="%an" release-1.7.0..release-1.8.0 | sort | uniq</code> without manual deduplication):</p>

<p>Addison Higham, Aitozi, Aleksey Pak, Alexander Fedulov, Alexey Trenikhin, Aljoscha Krettek, Andrey Zagrebin, Artsem Semianenka, Asura7969, Avi, Barisa Obradovic, Benchao Li, Bo WANG, Chesnay Schepler, Congxian Qiu, Cristian, David Anderson, Dawid Wysakowicz, Dian Fu, DuBin, EAlexRojas, EronWright, Eugen Yushin, Fabian Hueske, Fokko Driesprong, Gary Yao, Hequn Cheng, Igal Shilman, Jamie Grier, JaryZhen, Jeff Zhang, Jihyun Cho, Jinhu Wu, Joerg Schad, KarmaGYZ, Kezhu Wang, Konstantin Knauf, Kostas Kloudas, Lakshmi, Lakshmi Gururaja Rao, Lavkesh Lahngir, Li, Shuangjiang, Mai Nakagawa, Matrix42, Matt, Maximilian Michels, Mododo, Nico Kruber, Paul Lin, Piotr Nowojski, Qi Yu, Qin, Robert, Robert Metzger, Romano Vacca, Rong Rong, Rune Skou Larsen, Seth Wiesman, Shannon Carey, Shimin Yang, Shuyi Chen, Stefan Richter, Stephan Ewen, SuXingLee, TANG Wen-hui, Tao Yang, Thomas Weise, Till Rohrmann, Timo Walther, Tom Goong, Tony Feng, Tony Wei, Tzu-Li (Gordon) Tai, Tzu-Li Chen, Ufuk Celebi, Xingcan Cui, Xpray, XuQianJin-Stars, Xue Yu, Yangze Guo, Ying Xu, Yiqun Lin, Yu Li, Yuanyang Wu, Yun Tang, ZILI CHEN, Zhanchun Zhang, Zhijiang, ZiLi Chen, acqua.csq, alex04.wang, ap, azagrebin, blueszheng, boshu Zheng, chengjie.wu, chensq, chummyhe89, eaglewatcherwb, hequn8128, ifndef-SleePy, intsmaze, jackyyin, jinhu.wjh, jparkie, jrthe42, junsheng.wu, kgorman, kkloudas, kkolman, klion26, lamber-ken, leesf, libenchao, lining, liuzhaokun, lzh3636, maqingxiang, mb-datadome, okidogi, park.yq, sunhaibotb, sunjincheng121, tison, unknown, vinoyang, wenhuitang, wind, xueyu, xuqianjin, yanghua, zentol, zhangzhanchun, zhijiang, zhuzhu.zz, zy, 仲炜, 砚田, 谢磊</p>

</p>

      <p><a href="/news/2019/04/09/release-1.8.0.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    

    <!-- Pagination links -->
    
    <ul class="pager">
      <li>
      
        <a href="/blog/page6" class="previous">Previous</a>
      
      </li>
      <li>
        <span class="page_number ">Page: 7 of 15</span>
      </li>
      <li>
      
        <a href="/blog/page8" class="next">Next</a>
      
      </li>
    </ul>
    
  </div>

  <div class="col-sm-4" markdown="1">
    <!-- Blog posts by YEAR -->
    
      
      

      
    <h2>2021</h2>

    <ul id="markdown-toc">
      
      <li><a href="/2021/03/11/batch-execution-mode.html">A Rundown of Batch Execution Mode in the DataStream API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/03/03/release-1.12.2.html">Apache Flink 1.12.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/02/10/native-k8s-with-ha.html">How to natively deploy Flink on Kubernetes with High-Availability (HA)</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/29/release-1.10.3.html">Apache Flink 1.10.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/19/release-1.12.1.html">Apache Flink 1.12.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/01/18/rocksdb.html">Using RocksDB State Backend in Apache Flink: When and How</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/11/batch-fine-grained-fault-tolerance.html">Exploring fine-grained recovery of bounded data sets on Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/01/07/pulsar-flink-connector-270.html">What's New in the Pulsar Flink Connector 2.7.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/02/release-statefun-2.2.2.html">Stateful Functions 2.2.2 Release Announcement</a></li>

      
        
    </ul>
        <hr>
        <h2>2020</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2020/12/18/release-1.11.3.html">Apache Flink 1.11.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/12/15/pipelined-region-sheduling.html">Improvements in task scheduling for batch workloads in Apache Flink 1.12</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/12/10/release-1.12.0.html">Apache Flink 1.12.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/11/11/release-statefun-2.2.1.html">Stateful Functions 2.2.1 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1.html">From Aligned to Unaligned Checkpoints - Part 1: Checkpoints, Alignment, and Backpressure</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/10/13/stateful-serverless-internals.html">Stateful Functions Internals: Behind the scenes of Stateful Serverless</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/28/release-statefun-2.2.0.html">Stateful Functions 2.2.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/17/release-1.11.2.html">Apache Flink 1.11.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/04/community-update.html">Flink Community Update - August'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/09/01/flink-1.11-memory-management-improvements.html">Memory Management improvements for Flink’s JobManager in Apache Flink 1.11</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/25/release-1.10.2.html">Apache Flink 1.10.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/20/flink-docker.html">The State of Flink on Docker</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/08/19/statefun.html">Monitoring and Controlling Networks of IoT Devices with Flink Stateful Functions</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/06/external-resource.html">Accelerating your workload with GPU and other external resources</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/08/04/pyflink-pandas-udf-support-flink.html">PyFlink: The integration of Pandas into PyFlink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/30/demo-fraud-detection-3.html">Advanced Flink Application Patterns Vol.3: Custom Window Processing</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/07/28/flink-sql-demo-building-e2e-streaming-application.html">Flink SQL Demo: Building an End-to-End Streaming Application</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/27/community-update.html">Flink Community Update - July'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/07/23/catalogs.html">Sharing is caring - Catalogs in Flink SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/21/release-1.11.1.html">Apache Flink 1.11.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/14/application-mode.html">Application Deployment in Flink: Current State and the new Application Mode</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/06/release-1.11.0.html">Apache Flink 1.11.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/ecosystem/2020/06/23/flink-on-zeppelin-part2.html">Flink on Zeppelin Notebooks for Interactive Data Analysis - Part 2</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/15/flink-on-zeppelin-part1.html">Flink on Zeppelin Notebooks for Interactive Data Analysis - Part 1</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/11/community-update.html">Flink Community Update - June'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/09/release-statefun-2.1.0.html">Stateful Functions 2.1.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/12/release-1.10.1.html">Apache Flink 1.10.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/07/community-update.html">Flink Community Update - May'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/04/season-of-docs.html">Applying to Google Season of Docs 2020</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/24/release-1.9.3.html">Apache Flink 1.9.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/21/memory-management-improvements-flink-1.10.html">Memory Management Improvements with Apache Flink 1.10</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/15/flink-serialization-tuning-vol-1.html">Flink Serialization Tuning Vol. 1: Choosing your Serializer — if you can</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/04/09/pyflink-udf-support-flink.html">PyFlink: Introducing Python Support for UDFs in Flink's Table API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/07/release-statefun-2.0.0.html">Stateful Functions 2.0 - An Event-driven Database on Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/01/community-update.html">Flink Community Update - April'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2020/03/27/flink-for-data-warehouse.html">Flink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/03/24/demo-fraud-detection-2.html">Advanced Flink Application Patterns Vol.2: Dynamic Updates of Application Logic</a></li>

      
        
      
    
      
      

      
      <li><a href="/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html">Apache Beam: How Beam Runs on Top of Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/20/ddl.html">No Java Required: Configuring Sources and Sinks in SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/11/release-1.10.0.html">Apache Flink 1.10.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/07/a-guide-for-unit-testing-in-apache-flink.html">A Guide for Unit Testing in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/30/release-1.9.2.html">Apache Flink 1.9.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/29/state-unlocked-interacting-with-state-in-apache-flink.html">State Unlocked: Interacting with State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/15/demo-fraud-detection.html">Advanced Flink Application Patterns Vol.1: Case Study of a Fraud Detection System</a></li>

      
        
    </ul>
        <hr>
        <h2>2019</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2019/12/11/release-1.8.3.html">Apache Flink 1.8.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/12/09/flink-kubernetes-kudo.html">Running Apache Flink on Kubernetes with KUDO</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/11/25/query-pulsar-streams-using-apache-flink.html">How to query Pulsar Streams using Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/10/18/release-1.9.1.html">Apache Flink 1.9.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/feature/2019/09/13/state-processor-api.html">The State Processor API: How to Read, write and modify the state of Flink applications</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/09/11/release-1.8.2.html">Apache Flink 1.8.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/09/10/community-update.html">Flink Community Update - September'19</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/08/22/release-1.9.0.html">Apache Flink 1.9.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/07/23/flink-network-stack-2.html">Flink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/07/02/release-1.8.1.html">Apache Flink 1.8.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/06/26/broadcast-state.html">A Practical Guide to Broadcast State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/06/05/flink-network-stack.html">A Deep-Dive into Flink's Network Stack</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/19/state-ttl.html">State TTL in Flink 1.8.0: How to Automatically Cleanup Application State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/14/temporal-tables.html">Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/03/pulsar-flink.html">When Flink & Pulsar Come Together</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/04/17/sod.html">Apache Flink's Application to Season of Docs</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/04/09/release-1.8.0.html">Apache Flink 1.8.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2019/03/11/prometheus-monitoring.html">Flink and Prometheus: Cloud-native monitoring of streaming applications</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/03/06/ffsf-preview.html">What to expect from Flink Forward San Francisco 2019</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/25/monitoring-best-practices.html">Monitoring Apache Flink Applications 101</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/25/release-1.6.4.html">Apache Flink 1.6.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/15/release-1.7.2.html">Apache Flink 1.7.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/13/unified-batch-streaming-blink.html">Batch as a Special Case of Streaming and Alibaba's contribution of Blink</a></li>

      
        
    </ul>
        <hr>
        <h2>2018</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2018/12/26/release-1.5.6.html">Apache Flink 1.5.6 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/12/22/release-1.6.3.html">Apache Flink 1.6.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/12/21/release-1.7.1.html">Apache Flink 1.7.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/11/30/release-1.7.0.html">Apache Flink 1.7.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/10/29/release-1.6.2.html">Apache Flink 1.6.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/10/29/release-1.5.5.html">Apache Flink 1.5.5 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/09/20/release-1.6.1.html">Apache Flink 1.6.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/09/20/release-1.5.4.html">Apache Flink 1.5.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/08/21/release-1.5.3.html">Apache Flink 1.5.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/08/09/release-1.6.0.html">Apache Flink 1.6.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/07/31/release-1.5.2.html">Apache Flink 1.5.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/07/12/release-1.5.1.html">Apache Flink 1.5.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/05/25/release-1.5.0.html">Apache Flink 1.5.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/03/15/release-1.3.3.html">Apache Flink 1.3.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/03/08/release-1.4.2.html">Apache Flink 1.4.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2018/03/01/end-to-end-exactly-once-apache-flink.html">An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/02/15/release-1.4.1.html">Apache Flink 1.4.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2018/01/30/incremental-checkpointing.html">Managing Large State in Apache Flink: An Intro to Incremental Checkpointing</a></li>

      
        
    </ul>
        <hr>
        <h2>2017</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2017/12/21/2017-year-in-review.html">Apache Flink in 2017: Year in Review</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/12/12/release-1.4.0.html">Apache Flink 1.4.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/11/22/release-1.4-and-1.5-timeline.html">Looking Ahead to Apache Flink 1.4.0 and 1.5.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/08/05/release-1.3.2.html">Apache Flink 1.3.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2017/07/04/flink-rescalable-state.html">A Deep Dive into Rescalable State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/06/23/release-1.3.1.html">Apache Flink 1.3.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/06/01/release-1.3.0.html">Apache Flink 1.3.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/05/16/official-docker-image.html">Introducing Docker Images for Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/04/26/release-1.2.1.html">Apache Flink 1.2.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/04/04/dynamic-tables.html">Continuous Queries on Dynamic Tables</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/03/29/table-sql-api-update.html">From Streams to Tables and Back Again: An Update on Flink's Table & SQL API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/03/23/release-1.1.5.html">Apache Flink 1.1.5 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/02/06/release-1.2.0.html">Announcing Apache Flink 1.2.0</a></li>

      
        
    </ul>
        <hr>
        <h2>2016</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2016/12/21/release-1.1.4.html">Apache Flink 1.1.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/12/19/2016-year-in-review.html">Apache Flink in 2016: Year in Review</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/10/12/release-1.1.3.html">Apache Flink 1.1.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/09/05/release-1.1.2.html">Apache Flink 1.1.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/24/ff16-keynotes-panels.html">Flink Forward 2016: Announcing Schedule, Keynotes, and Panel Discussion</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/11/release-1.1.1.html">Flink 1.1.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/08/release-1.1.0.html">Announcing Apache Flink 1.1.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/05/24/stream-sql.html">Stream Processing for Everyone with SQL and Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/05/11/release-1.0.3.html">Flink 1.0.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/22/release-1.0.2.html">Flink 1.0.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/14/flink-forward-announce.html">Flink Forward 2016 Call for Submissions Is Now Open</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/06/cep-monitoring.html">Introducing Complex Event Processing (CEP) with Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/06/release-1.0.1.html">Flink 1.0.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/03/08/release-1.0.0.html">Announcing Apache Flink 1.0.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/02/11/release-0.10.2.html">Flink 0.10.2 Released</a></li>

      
        
    </ul>
        <hr>
        <h2>2015</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2015/12/18/a-year-in-review.html">Flink 2015: A year in review, and a lookout to 2016</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/12/11/storm-compatibility.html">Storm Compatibility in Apache Flink: How to run existing Storm topologies on Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/12/04/Introducing-windows.html">Introducing Stream Windows in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/11/27/release-0.10.1.html">Flink 0.10.1 released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/11/16/release-0.10.0.html">Announcing Apache Flink 0.10.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/16/off-heap-memory.html">Off-heap Memory in Apache Flink and the curious JIT compiler</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/03/flink-forward.html">Announcing Flink Forward 2015</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/01/release-0.9.1.html">Apache Flink 0.9.1 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/08/24/introducing-flink-gelly.html">Introducing Gelly: Graph Processing with Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/06/24/announcing-apache-flink-0.9.0-release.html">Announcing Apache Flink 0.9.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/05/14/Community-update-April.html">April 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/05/11/Juggling-with-Bits-and-Bytes.html">Juggling with Bits and Bytes</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/04/13/release-0.9.0-milestone1.html">Announcing Flink 0.9.0-milestone1 preview release</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/04/07/march-in-flink.html">March 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html">Peeking into Apache Flink's Engine Room</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/03/02/february-2015-in-flink.html">February 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/02/09/streaming-example.html">Introducing Flink Streaming</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/02/04/january-in-flink.html">January 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/01/21/release-0.8.html">Apache Flink 0.8.0 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/01/06/december-in-flink.html">December 2014 in the Flink community</a></li>

      
        
    </ul>
        <hr>
        <h2>2014</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2014/11/18/hadoop-compatibility.html">Hadoop Compatibility in Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/11/04/release-0.7.0.html">Apache Flink 0.7.0 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/10/03/upcoming_events.html">Upcoming Events</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/09/26/release-0.6.1.html">Apache Flink 0.6.1 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/08/26/release-0.6.html">Apache Flink 0.6 available</a></li>

      
    </ul>
      
    
  </div>
</div>
      </div>
    </div>

    <hr />

    <div class="row">
      <div class="footer text-center col-sm-12">
        <p>Copyright © 2014-2021 <a href="http://apache.org">The Apache Software Foundation</a>. All Rights Reserved.</p>
        <p>Apache Flink, Flink®, Apache®, the squirrel logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation.</p>
        <p><a href="/privacy-policy.html">Privacy Policy</a> &middot; <a href="/blog/feed.xml">RSS feed</a></p>
      </div>
    </div>
    </div><!-- /.container -->

    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/js/jquery.matchHeight-min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/codetabs.js"></script>
    <script src="/js/stickysidebar.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>
  </body>
</html>
