<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink: Blog</title>
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/flink.css">
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Blog RSS feed -->
    <link href="/blog/feed.xml" rel="alternate" type="application/rss+xml" title="Apache Flink Blog: RSS feed" />

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!-- We need to load Jquery in the header for custom google analytics event tracking-->
    <script src="/js/jquery.min.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>  
    

    <!-- Main content. -->
    <div class="container">
    <div class="row">

      
     <div id="sidebar" class="col-sm-3">
        

<!-- Top navbar. -->
    <nav class="navbar navbar-default">
        <!-- The logo. -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="navbar-logo">
            <a href="/">
              <img alt="Apache Flink" src="/img/flink-header-logo.svg" width="147px" height="73px">
            </a>
          </div>
        </div><!-- /.navbar-header -->

        <!-- The navigation links. -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav navbar-main">

            <!-- First menu section explains visitors what Flink is -->

            <!-- What is Stream Processing? -->
            <!--
            <li><a href="/streamprocessing1.html">What is Stream Processing?</a></li>
            -->

            <!-- What is Flink? -->
            <li><a href="/flink-architecture.html">What is Apache Flink?</a></li>

            

            <!-- What is Stateful Functions? -->

            <li><a href="/stateful-functions.html">What is Stateful Functions?</a></li>

            <!-- Use cases -->
            <li><a href="/usecases.html">Use Cases</a></li>

            <!-- Powered by -->
            <li><a href="/poweredby.html">Powered By</a></li>


            &nbsp;
            <!-- Second menu section aims to support Flink users -->

            <!-- Downloads -->
            <li><a href="/downloads.html">Downloads</a></li>

            <!-- Getting Started -->
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Getting Started<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/index.html" target="_blank">With Flink <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/getting-started/project-setup.html" target="_blank">With Flink Stateful Functions <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="/training.html">Training Course</a></li>
              </ul>
            </li>

            <!-- Documentation -->
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Documentation<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12" target="_blank">Flink 1.12 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-master" target="_blank">Flink Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2" target="_blank">Flink Stateful Functions 2.2 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-master" target="_blank">Flink Stateful Functions Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
              </ul>
            </li>

            <!-- getting help -->
            <li><a href="/gettinghelp.html">Getting Help</a></li>

            <!-- Blog -->
            <li class="active"><a href="/blog/"><b>Flink Blog</b></a></li>


            <!-- Flink-packages -->
            <li>
              <a href="https://flink-packages.org" target="_blank">flink-packages.org <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>
            &nbsp;

            <!-- Third menu section aim to support community and contributors -->

            <!-- Community -->
            <li><a href="/community.html">Community &amp; Project Info</a></li>

            <!-- Roadmap -->
            <li><a href="/roadmap.html">Roadmap</a></li>

            <!-- Contribute -->
            <li><a href="/contributing/how-to-contribute.html">How to Contribute</a></li>
            

            <!-- GitHub -->
            <li>
              <a href="https://github.com/apache/flink" target="_blank">Flink on GitHub <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>

            &nbsp;

            <!-- Language Switcher -->
            <li>
              
                
                  <!-- link to the Chinese home page when current is blog page -->
                  <a href="/zh">中文版</a>
                
              
            </li>

          </ul>

          <style>
            .smalllinks:link {
              display: inline-block !important; background: none; padding-top: 0px; padding-bottom: 0px; padding-right: 0px; min-width: 75px;
            }
          </style>

          <ul class="nav navbar-nav navbar-bottom">
          <hr />

            <!-- Twitter -->
            <li><a href="https://twitter.com/apacheflink" target="_blank">@ApacheFlink <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <!-- Visualizer -->
            <li class=" hidden-md hidden-sm"><a href="/visualizer/" target="_blank">Plan Visualizer <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <li >
                  <a href="/security.html">Flink Security</a>
            </li>

          <hr />

            <li><a href="https://apache.org" target="_blank">Apache Software Foundation <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <li>

              <a class="smalllinks" href="https://www.apache.org/licenses/" target="_blank">License</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/security/" target="_blank">Security</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/foundation/sponsorship.html" target="_blank">Donate</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/foundation/thanks.html" target="_blank">Thanks</a> <small><span class="glyphicon glyphicon-new-window"></span></small>
            </li>

          </ul>
        </div><!-- /.navbar-collapse -->
    </nav>

      </div>
      <div class="col-sm-9">
      <h1>Blog</h1>
<hr />

<div class="row">
  <div class="col-sm-8">
    <!-- Blog posts -->
    
    <article>
      <h2 class="blog-title"><a href="/news/2020/08/20/flink-docker.html">The State of Flink on Docker</a></h2>

      <p>20 Aug 2020
       Robert Metzger (<a href="https://twitter.com/rmetzger_">@rmetzger_</a>)</p>

      <p>This blog post gives an update on the recent developments of Flink's support for Docker.</p>

      <p><a href="/news/2020/08/20/flink-docker.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2020/08/19/statefun.html">Monitoring and Controlling Networks of IoT Devices with Flink Stateful Functions</a></h2>

      <p>19 Aug 2020
       Igal Shilman (<a href="https://twitter.com/IgalShilman">@IgalShilman</a>)</p>

      <p><p>In this blog post, we’ll take a look at a class of use cases that is a natural fit for <a href="https://flink.apache.org/stateful-functions.html">Flink Stateful Functions</a>: monitoring and controlling networks of connected devices (often called the “Internet of Things” (IoT)).</p>

<p>IoT networks are composed of many individual, but interconnected components, which makes getting some kind of high-level insight into the status, problems, or optimization opportunities in these networks not trivial. Each individual device “sees” only its own state, which means that the status of groups of devices, or even the network as a whole, is often a complex aggregation of the individual devices’ state. Diagnosing, controlling, or optimizing these groups of devices thus requires distributed logic that analyzes the “bigger picture” and then acts upon it.</p>

<p>A powerful approach to implement this is using <em><a href="https://en.wikipedia.org/wiki/Digital_twin">digital twins</a></em>: each device has a corresponding virtual entity (i.e. the digital twin), which also captures their relationships and interactions. The digital twins track the status of their corresponding devices and send updates to other twins, representing groups (such as geographical regions) of devices. Those, in turn, handle the logic to obtain the network’s aggregated view, or this “bigger picture” we mentioned before.</p>

<h1 id="our-scenario-datacenter-monitoring-and-alerting">Our Scenario: Datacenter Monitoring and Alerting</h1>

<figure style="float:right;padding-left:1px;padding-top: 20px">
  <img src="/img/blog/2020-08-18-statefun/rack.png" width="350px" />
  <figcaption style="padding-top: 10px;text-align:center"><b>Fig.1</b> An oversimplified view of a data center.</figcaption>
</figure>

<p>There are many examples of the digital twins approach in the real world, such as <a href="https://www.infoq.com/presentations/tesla-vpp/">smart grids of batteries</a>, <a href="https://www.alibabacloud.com/solutions/intelligence-brain/city">smart cities</a>, or <a href="https://www.youtube.com/watch?v=9y27FJgz5-M">monitoring infrastructure software clusters</a>. In this blogpost, we’ll use the example of data center monitoring and alert correlation implemented with Stateful Functions.</p>

<p>Consider a very simplified view of a data center, consisting of many thousands of commodity servers arranged in server racks. Each server rack typically contains up to 40 servers, with a ToR (Top of the Rack) network switch connected to each server. The switches from all the racks connect through a larger switch (<strong>Fig. 1</strong>).</p>

<p>In this datacenter, many things can go wrong: a disk in a server can stop working, network cards can start dropping packets, or ToR switches might cease to function. The entire data center might also be affected by power supply degradation, causing servers to operate at reduced capacity. On-site engineers must be able to identify these incidents quickly and fix them promptly.</p>

<p>Diagnosing individual server failures is rather straightforward: take a recent history of metric reports from that particular server, analyse it and pinpoint the anomaly. On the other hand, other incidents only make sense “together”, because they share a common root cause. Diagnosing or predicting causes of networking degradation at a rack or datacenter level requires an aggregate view of metrics (such as package drop rates) from the individual machines and racks, and possibly some prediction model or diagnosis code that runs under certain conditions.</p>

<h2 id="monitoring-a-virtual-datacenter-via-digital-twins">Monitoring a Virtual Datacenter via Digital Twins</h2>

<p>For the sake of this blog post, our oversimplified data center has some servers and racks, each with a unique ID. Each server has a metrics-collecting daemon that publishes metrics to a message queue, and there is a provisioning service that operators will use to ask for server commission- and decommissioning.</p>

<div style="line-height:60%;">
    <br />
</div>

<center>
<img src="/img/blog/2020-08-18-statefun/1.png" width="550px" alt="" />
</center>

<div style="line-height:60%;">
    <br />
</div>

<p>Our application will consume these server metrics and commission/decommission events, and produce server/rack/datacenter alerts. There will also be an operator consuming any alerts triggered by the monitoring system. In the next section, we’ll show how this use case can be naturally modeled with Stateful Functions (StateFun).</p>

<h2 id="implementing-the-use-case-with-flink-statefun">Implementing the use case with Flink StateFun</h2>

<div class="alert alert-info">
  <p>You can find the code for this example at: <a href="https://github.com/igalshilman/iot-statefun-blogpost">https://github.com/igalshilman/iot-statefun-blogpost</a></p>
</div>

<p>The basic building block for modeling a StateFun application is a <a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.1/concepts/application-building-blocks.html#stateful-functions"><em>stateful function</em></a>, which has the following properties:</p>

<ul>
  <li>
    <p>It has a logical unique address; and persisted, fault tolerant state, scoped to that address.</p>
  </li>
  <li>
    <p>It can <em>react</em> to messages, both internal (or, sent from other stateful functions) and external (e.g. a message from Kafka).</p>
  </li>
  <li>
    <p>Invocations of a specific function are serializable, so messages sent to a specific address are <strong>not</strong> executed concurrently.</p>
  </li>
  <li>
    <p>There can be many billions of function instances in a single StateFun cluster.</p>
  </li>
</ul>

<p>To model our use case, we’ll define three functions: <strong>ServerFun</strong>, <strong>RackFun</strong> and <strong>DataCenterFun</strong>.</p>

<p><strong>ServerFun</strong></p>

<p>Each physical server is represented with its <em>digital twin</em> stateful function. This function is responsible for:</p>

<ol>
  <li>
    <p>Maintaining a sliding window of incoming metrics.</p>
  </li>
  <li>
    <p>Applying a model that decides whether or not to trigger an alert.</p>
  </li>
  <li>
    <p>Alerting if metrics are missing for too long.</p>
  </li>
  <li>
    <p>Notifying its containing <strong>RackFun</strong> about any open incidents.</p>
  </li>
</ol>

<p><strong>RackFun</strong></p>

<p>While the <em>ServerFun</em> is responsible for identifying server-local incidents, we need a function that correlates incidents happening on the different servers deployed in the same rack and:</p>

<ol>
  <li>
    <p>Collects open incidents reported by the <strong>ServerFun</strong> functions.</p>
  </li>
  <li>
    <p>Maintains an histogram of currently opened incidents on this rack.</p>
  </li>
  <li>
    <p>Applies a correlation model to the individual incidents sent by the <strong>ServerFun</strong>, and reports high-level, related incidents as a single incident to the <strong>DataCenterFun</strong>.</p>
  </li>
</ol>

<p><strong>DataCenterFun</strong></p>

<p>This function maintains a view of incidents across different racks in our datacenter.</p>

<div style="line-height:60%;">
    <br />
</div>

<center>
<img src="/img/blog/2020-08-18-statefun/2.png" width="600px" alt="" />
</center>

<div style="line-height:60%;">
    <br />
</div>

<p>To summarize our plan:</p>

<ul>
  <li>
    <p>Leaf functions ingest raw metric data (<span style="color:blue">blue</span> lines), and apply localized logic to trigger an alert.</p>
  </li>
  <li>
    <p>Intermediate functions operate on already summarized events (<span style="color:orange">orange</span> lines) and correlate them into high-level events.</p>
  </li>
  <li>
    <p>A root function correlates the high-level events across the intermediate functions and into a single <em>healthy/not healthy</em> value.</p>
  </li>
</ul>

<h2 id="how-does-it-really-look">How does it really look?</h2>

<h3 id="serverfun">ServerFun</h3>

<center>
<img src="/img/blog/2020-08-18-statefun/3_1.png" width="600px" alt="" />
</center>

<div style="line-height:60%;">
    <br />
</div>

<ol>
  <li>This section associates a behaviour for every message that the function expects to be invoked with.</li>
  <li>The <code>metricsHistory</code> buffer is our sliding window of the last 15 minutes worth of <code>ServerMetricReports</code>. Note that this buffer is configured to expire entries 15 minutes after they were written.</li>
  <li><code>serverHealthState</code> represents the current physical server state, open incidents and so on.</li>
</ol>

<p>Let’s take a look at what happens when a <code>ServerMetricReport</code> message arrives:</p>

<center>
<img src="/img/blog/2020-08-18-statefun/3_2.png" width="600px" alt="" />
</center>

<div style="line-height:60%;">
    <br />
</div>

<ol>
  <li>Retrieve the previously computed <code>serverHealthState</code> that is kept in state.</li>
  <li>Evaluate a model on the sliding window of the previous metric reports + the current metric reported + the previously computed server state to obtain an assessment of the current server health.</li>
  <li>If the server is not believed to be healthy, emit an alert via an alerts topic, and also send a message to our containing rack with all the open incidents that this server currently has.</li>
</ol>

<div class="alert alert-warning">
  <p>We’ll omit the other handlers for brevity, but it’s important to mention that <b>onTimer</b> makes sure that metric reports are coming in periodically, otherwise it’d trigger an alert stating that we didn’t hear from that server for a long time.</p>
</div>

<h3 id="rackfun">RackFun</h3>

<center>
<img src="/img/blog/2020-08-18-statefun/5.png" width="650px" alt="" />
</center>

<div style="line-height:60%;">
    <br />
</div>

<ol>
  <li>This function keeps a mapping between a <code>ServerId</code> and a set of open incidents on that server.</li>
  <li>When new alerts are received, this function tries to correlate the alert with any other open alerts on that rack. If a correlated rack alert is present, this function notifies the <strong>DataCenterFun</strong> about it.</li>
</ol>

<h3 id="datacenterfun">DataCenterFun</h3>

<center>
<img src="/img/blog/2020-08-18-statefun/6.png" width="650px" alt="" />
</center>

<div style="line-height:60%;">
    <br />
</div>

<ol>
  <li>A persisted mapping between a <code>RackId</code> and the latest alert that rack reported.</li>
  <li>Throughout the usage of ingress/egress pairs, this function can report back its current view of the world of what racks are currently known to be unhealthy.</li>
  <li>An operator (via a front-end) can send a <code>GetUnhealthyRacks</code> message addressed to that <strong>DataCenterFun</strong>, and wait for the corresponding response <code>message(UnhealthyRacks)</code>. Whenever a rack reports <em>OK</em>, it’ll be removed from the unhealthy racks map.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>This pattern — where each layer of functions performs a stateful aggregation of events sent from the previous layer (or the input) — is useful for a whole class of problems. And, although we used connected devices to motivate this use case, it’s not limited to the IoT domain.</p>

<center>
<img src="/img/blog/2020-08-18-statefun/7.png" width="500px" alt="" />
</center>

<div style="line-height:60%;">
    <br />
</div>

<p>Stateful Functions provides the building blocks necessary for building complex distributed applications (here the digital twins that support analysis and interactions of the physical entities), while removing common complexities of distributed systems like service discovery, retires, circuit breakers, state management, scalability and similar challenges. If you’d like to learn more about Stateful Functions, head over to the official <a href="https://ci.apache.org/projects/flink/flink-statefun-docs-master/">documentation</a>, where you can also find more hands-on tutorials to try out yourself!</p>
</p>

      <p><a href="/2020/08/19/statefun.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2020/08/06/external-resource.html">Accelerating your workload with GPU and other external resources</a></h2>

      <p>06 Aug 2020
       Yangze Guo </p>

      <p>This post introduces the new External Resource Framework in Flink 1.11 and take GPU as an example to show how to accelerate your workload with external resources.</p>

      <p><a href="/news/2020/08/06/external-resource.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2020/08/04/pyflink-pandas-udf-support-flink.html">PyFlink: The integration of Pandas into PyFlink</a></h2>

      <p>04 Aug 2020
       Jincheng Sun (<a href="https://twitter.com/sunjincheng121">@sunjincheng121</a>) &amp; Markos Sfikas (<a href="https://twitter.com/MarkSfik">@MarkSfik</a>)</p>

      <p>The Apache Flink community put some great effort into integrating Pandas with PyFlink in the latest Flink version 1.11. Some of the added features include support for Pandas UDF and the conversion between Pandas DataFrame and Table. In this article, we will introduce how these functionalities work and how to use them with a step-by-step example.</p>

      <p><a href="/2020/08/04/pyflink-pandas-udf-support-flink.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2020/07/30/demo-fraud-detection-3.html">Advanced Flink Application Patterns Vol.3: Custom Window Processing</a></h2>

      <p>30 Jul 2020
       Alexander Fedulov (<a href="https://twitter.com/alex_fedulov">@alex_fedulov</a>)</p>

      <p>In this series of blog posts you will learn about powerful Flink patterns for building streaming applications.</p>

      <p><a href="/news/2020/07/30/demo-fraud-detection-3.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2020/07/28/flink-sql-demo-building-e2e-streaming-application.html">Flink SQL Demo: Building an End-to-End Streaming Application</a></h2>

      <p>28 Jul 2020
       Jark Wu (<a href="https://twitter.com/JarkWu">@JarkWu</a>)</p>

      <p>Apache Flink 1.11 has released many exciting new features, including many developments in Flink SQL which is evolving at a fast pace. This article takes a closer look at how to quickly build streaming applications with Flink SQL from a practical point of view.</p>

      <p><a href="/2020/07/28/flink-sql-demo-building-e2e-streaming-application.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2020/07/27/community-update.html">Flink Community Update - July'20</a></h2>

      <p>27 Jul 2020
       Marta Paes (<a href="https://twitter.com/morsapaes">@morsapaes</a>)</p>

      <p>As July draws to an end, we look back at a monthful of activity in the Flink community, including two releases (!) and some work around improving the first-time contribution experience in the project. Also, events are starting to pick up again, so we've put together a list of some great events you can (virtually) attend in August!</p>

      <p><a href="/news/2020/07/27/community-update.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/2020/07/23/catalogs.html">Sharing is caring - Catalogs in Flink SQL</a></h2>

      <p>23 Jul 2020
       Dawid Wysakowicz (<a href="https://twitter.com/dwysakowicz">@dwysakowicz</a>)</p>

      <p><p>With an ever-growing number of people working with data, it’s a common practice for companies to build self-service platforms with the goal of democratizing their access across different teams and — especially — to enable users from any background to be independent in their data needs. In such environments, metadata management becomes a crucial aspect. Without it, users often work blindly, spending too much time searching for datasets and their location, figuring out data formats and similar cumbersome tasks.</p>

<p>In this blog post, we want to give you a high level overview of catalogs in Flink. We’ll describe why you should consider using them and what you can achieve with one in place. To round it up, we’ll also showcase how simple it is to combine catalogs and Flink, in the form of an end-to-end example that you can try out yourself.</p>

<h2 id="why-do-i-need-a-catalog">Why do I need a catalog?</h2>

<p>Frequently, companies start building a data platform with a metastore, catalog, or schema registry of some sort already in place. Those let you clearly separate making the data available from consuming it. That separation has a few benefits:</p>

<ul>
  <li><strong>Improved productivity</strong> - The most obvious one. Making data reusable and shifting the focus on building new models/pipelines rather than data cleansing and discovery.</li>
  <li><strong>Security</strong> - You can control the access to certain features of the data. For example, you can make the schema of the dataset publicly available, but limit the actual access to the underlying data only to particular teams.</li>
  <li><strong>Compliance</strong> - If you have all the metadata in a central entity, it’s much easier to ensure compliance with GDPR and similar regulations and legal requirements.</li>
</ul>

<h2 id="what-is-stored-in-a-catalog">What is stored in a catalog?</h2>

<p>Almost all data sets can be described by certain properties that must be known in order to consume them. Those include:</p>

<ul>
  <li>
    <p><strong>Schema</strong> - It describes the actual contents of the data, what columns it has, what are the constraints (e.g. keys) on which the updates should be performed, which fields can act as time attributes, what are the rules for watermark generation and so on.</p>
  </li>
  <li>
    <p><strong>Location</strong> - Does the data come from Kafka or a file in a filesystem? How do you connect to the external system? Which topic or file name do you use?</p>
  </li>
  <li>
    <p><strong>Format</strong> - Is the data serialized as JSON, CSV, or maybe Avro records?</p>
  </li>
  <li>
    <p><strong>Statistics</strong> - You can also store additional information that can be useful when creating an execution plan of your query. For example, you can choose the best join algorithm, based on the number of rows in joined datasets.</p>
  </li>
</ul>

<p>Catalogs don’t have to be limited to the metadata of datasets. You can usually store other objects that can be reused in different scenarios, such as:</p>

<ul>
  <li>
    <p><strong>Functions</strong> - It’s very common to have domain specific functions that can be helpful in different use cases. Instead of having to create them in each place separately, you can just create them once and share them with others.</p>
  </li>
  <li>
    <p><strong>Queries</strong> - Those can be useful when you don’t want to persist a data set, but want to provide a recipe for creating it from other sources instead.</p>
  </li>
</ul>

<h2 id="catalogs-support-in-flink-sql">Catalogs support in Flink SQL</h2>
<p>Starting from version 1.9, Flink has a set of Catalog APIs that allows to integrate Flink with various catalog implementations. With the help of those APIs, you can query tables in Flink that were created in your external catalogs (e.g. Hive Metastore). Additionally, depending on the catalog implementation, you can create new objects such as tables or views from Flink, reuse them across different jobs, and possibly even use them in other tools compatible with that catalog. In other words, you can see catalogs as having a two-fold purpose:</p>

<ul>
  <li>
    <p>Provide an out-of-the box integration with ecosystems such as RDBMSs or Hive that allows you to query external objects like tables, views, or functions with no additional connector configuration. The connector properties are automatically derived from the catalog itself.</p>
  </li>
  <li>
    <p>Act as a persistent store for Flink-specific metadata. In this mode, we additionally store connector properties alongside the logical metadata (e.g. schema, object name). That approach enables you to, for example, store a full definition of a Kafka-backed table with records serialized with Avro in Hive that can be later on used by Flink. However, as it incorporates Flink-specific properties, it can not be used by other tools that leverage Hive Metastore.</p>
  </li>
</ul>

<p>As of Flink 1.11, there are two catalog implementations supported by the community:</p>

<ol>
  <li>
    <p>A comprehensive Hive catalog</p>
  </li>
  <li>
    <p>A Postgres catalog (preview, read-only, for now)</p>
  </li>
</ol>

<div class="alert alert-info">
  <p><span class="label label-info" style="display: inline-block"><span class="glyphicon glyphicon-info-sign" aria-hidden="true"></span> Note</span>
Flink does not store data at rest; it is a compute engine and requires other systems to consume input from and write its output. This means that Flink does not own the lifecycle of the data. Integration with Catalogs does not change that. Flink uses catalogs for metadata management only.</p>
</div>

<p>All you need to do to start querying your tables defined in either of these metastores is to create the corresponding catalogs with connection parameters. Once this is done, you can use them the way you would in any relational database management system.</p>

<div class="highlight"><pre><code class="language-sql"><span class="c1">-- create a catalog which gives access to the backing Postgres installation</span>
<span class="k">CREATE</span> <span class="k">CATALOG</span> <span class="n">postgres</span> <span class="k">WITH</span> <span class="p">(</span>
    <span class="s1">&#39;type&#39;</span><span class="o">=</span><span class="s1">&#39;jdbc&#39;</span><span class="p">,</span>
    <span class="s1">&#39;property-version&#39;</span><span class="o">=</span><span class="s1">&#39;1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;base-url&#39;</span><span class="o">=</span><span class="s1">&#39;jdbc:postgresql://postgres:5432/&#39;</span><span class="p">,</span>
    <span class="s1">&#39;default-database&#39;</span><span class="o">=</span><span class="s1">&#39;postgres&#39;</span><span class="p">,</span>
    <span class="s1">&#39;username&#39;</span><span class="o">=</span><span class="s1">&#39;postgres&#39;</span><span class="p">,</span>
    <span class="s1">&#39;password&#39;</span><span class="o">=</span><span class="s1">&#39;example&#39;</span>
<span class="p">);</span>

<span class="c1">-- create a catalog which gives access to the backing Hive installation</span>
<span class="k">CREATE</span> <span class="k">CATALOG</span> <span class="n">hive</span> <span class="k">WITH</span> <span class="p">(</span>
    <span class="s1">&#39;type&#39;</span><span class="o">=</span><span class="s1">&#39;hive&#39;</span><span class="p">,</span>
    <span class="s1">&#39;property-version&#39;</span><span class="o">=</span><span class="s1">&#39;1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hive-version&#39;</span><span class="o">=</span><span class="s1">&#39;2.3.6&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hive-conf-dir&#39;</span><span class="o">=</span><span class="s1">&#39;/opt/hive-conf&#39;</span>
<span class="p">);</span></code></pre></div>

<p>After creating the catalogs, you can confirm that they are available to Flink and also list the databases or tables in each of these catalogs:</p>

<div class="highlight"><pre><code class="language-sql"><span class="o">&gt;</span> <span class="k">show</span> <span class="n">catalogs</span><span class="p">;</span>
<span class="n">default_catalog</span>
<span class="n">hive</span>
<span class="n">postgres</span>

<span class="c1">-- switch the default catalog to Hive</span>
<span class="o">&gt;</span> <span class="n">use</span> <span class="k">catalog</span> <span class="n">hive</span><span class="p">;</span>
<span class="o">&gt;</span> <span class="k">show</span> <span class="n">databases</span><span class="p">;</span>
<span class="k">default</span> <span class="c1">-- hive&#39;s default database</span>

<span class="o">&gt;</span> <span class="k">show</span> <span class="n">tables</span><span class="p">;</span>
<span class="n">dev_orders</span>

<span class="o">&gt;</span> <span class="n">use</span> <span class="k">catalog</span> <span class="n">postgres</span><span class="p">;</span>
<span class="o">&gt;</span> <span class="k">show</span> <span class="n">tables</span><span class="p">;</span>
<span class="n">prod_customer</span>
<span class="n">prod_nation</span>
<span class="n">prod_rates</span>
<span class="n">prod_region</span>
<span class="n">region_stats</span>

<span class="c1">-- desribe a schema of a table in Postgres, the Postgres types are automatically mapped to</span>
<span class="c1">-- Flink&#39;s type system</span>
<span class="o">&gt;</span> <span class="k">describe</span> <span class="n">prod_customer</span>
<span class="n">root</span>
 <span class="o">|</span><span class="c1">-- c_custkey: INT NOT NULL</span>
 <span class="o">|</span><span class="c1">-- c_name: VARCHAR(25) NOT NULL</span>
 <span class="o">|</span><span class="c1">-- c_address: VARCHAR(40) NOT NULL</span>
 <span class="o">|</span><span class="c1">-- c_nationkey: INT NOT NULL</span>
 <span class="o">|</span><span class="c1">-- c_phone: CHAR(15) NOT NULL</span>
 <span class="o">|</span><span class="c1">-- c_acctbal: DOUBLE NOT NULL</span>
 <span class="o">|</span><span class="c1">-- c_mktsegment: CHAR(10) NOT NULL</span>
 <span class="o">|</span><span class="c1">-- c_comment: VARCHAR(117) NOT NULL</span></code></pre></div>

<p>Now that you know which tables are available, you can write your first query.
In this scenario, we keep customer orders in Hive (<code>dev_orders</code>) because of their volume, and reference customer data in Postgres (<code>prod_customer</code>) to be able to easily update it. Let’s write a query that shows customers and their orders by region and order priority for a specific day.</p>

<div class="highlight"><pre><code class="language-sql"><span class="n">USE</span> <span class="k">CATALOG</span> <span class="n">postgres</span><span class="p">;</span>
<span class="k">SELECT</span>
  <span class="n">r_name</span> <span class="k">AS</span> <span class="o">`</span><span class="n">region</span><span class="o">`</span><span class="p">,</span>
  <span class="n">o_orderpriority</span> <span class="k">AS</span> <span class="o">`</span><span class="n">priority</span><span class="o">`</span><span class="p">,</span>
  <span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span> <span class="n">c_custkey</span><span class="p">)</span> <span class="k">AS</span> <span class="o">`</span><span class="n">number_of_customers</span><span class="o">`</span><span class="p">,</span>
  <span class="k">COUNT</span><span class="p">(</span><span class="n">o_orderkey</span><span class="p">)</span> <span class="k">AS</span> <span class="o">`</span><span class="n">number_of_orders</span><span class="o">`</span>
<span class="k">FROM</span> <span class="o">`</span><span class="n">hive</span><span class="o">`</span><span class="p">.</span><span class="o">`</span><span class="k">default</span><span class="o">`</span><span class="p">.</span><span class="n">dev_orders</span> <span class="c1">-- we need to fully qualify the table in hive because we set the</span>
                                 <span class="c1">-- current catalog to Postgres</span>
<span class="k">JOIN</span> <span class="n">prod_customer</span> <span class="k">ON</span> <span class="n">o_custkey</span> <span class="o">=</span> <span class="n">c_custkey</span>
<span class="k">JOIN</span> <span class="n">prod_nation</span> <span class="k">ON</span> <span class="n">c_nationkey</span> <span class="o">=</span> <span class="n">n_nationkey</span>
<span class="k">JOIN</span> <span class="n">prod_region</span> <span class="k">ON</span> <span class="n">n_regionkey</span> <span class="o">=</span> <span class="n">r_regionkey</span>
<span class="k">WHERE</span>
  <span class="n">FLOOR</span><span class="p">(</span><span class="n">o_ordertime</span> <span class="k">TO</span> <span class="k">DAY</span><span class="p">)</span> <span class="o">=</span> <span class="k">TIMESTAMP</span> <span class="s1">&#39;2020-04-01 0:00:00.000&#39;</span>
  <span class="k">AND</span> <span class="k">NOT</span> <span class="n">o_orderpriority</span> <span class="o">=</span> <span class="s1">&#39;4-NOT SPECIFIED&#39;</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">r_name</span><span class="p">,</span> <span class="n">o_orderpriority</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">r_name</span><span class="p">,</span> <span class="n">o_orderpriority</span><span class="p">;</span></code></pre></div>

<p>Flink’s catalog support also covers storing Flink-specific objects in external catalogs that might not be fully usable by the corresponding external tools. The most notable use case for this is, for example, storing a table that describes a Kafka topic in a Hive catalog. Take the following DDL statement, that contains a watermark declaration as well as a set of connector properties that are not recognizable by Hive. You won’t be able to query the table with Hive, but it will be persisted and can be reused by different Flink jobs.</p>

<div class="highlight"><pre><code class="language-sql"><span class="n">USE</span> <span class="k">CATALOG</span> <span class="n">hive</span><span class="p">;</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">prod_lineitem</span> <span class="p">(</span>
  <span class="n">l_orderkey</span> <span class="nb">INTEGER</span><span class="p">,</span>
  <span class="n">l_partkey</span> <span class="nb">INTEGER</span><span class="p">,</span>
  <span class="n">l_suppkey</span> <span class="nb">INTEGER</span><span class="p">,</span>
  <span class="n">l_linenumber</span> <span class="nb">INTEGER</span><span class="p">,</span>
  <span class="n">l_quantity</span> <span class="n">DOUBLE</span><span class="p">,</span>
  <span class="n">l_extendedprice</span> <span class="n">DOUBLE</span><span class="p">,</span>
  <span class="n">l_discount</span> <span class="n">DOUBLE</span><span class="p">,</span>
  <span class="n">l_tax</span> <span class="n">DOUBLE</span><span class="p">,</span>
  <span class="n">l_currency</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">l_returnflag</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">l_linestatus</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">l_ordertime</span> <span class="k">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
  <span class="n">l_shipinstruct</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">l_shipmode</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">l_comment</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">l_proctime</span> <span class="k">AS</span> <span class="n">PROCTIME</span><span class="p">(),</span>
  <span class="n">WATERMARK</span> <span class="k">FOR</span> <span class="n">l_ordertime</span> <span class="k">AS</span> <span class="n">l_ordertime</span> <span class="o">-</span> <span class="nb">INTERVAL</span> <span class="s1">&#39;5&#39;</span> <span class="n">SECONDS</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">&#39;connector&#39;</span><span class="o">=</span><span class="s1">&#39;kafka&#39;</span><span class="p">,</span>
  <span class="s1">&#39;topic&#39;</span><span class="o">=</span><span class="s1">&#39;lineitem&#39;</span><span class="p">,</span>
  <span class="s1">&#39;scan.startup.mode&#39;</span><span class="o">=</span><span class="s1">&#39;earliest-offset&#39;</span><span class="p">,</span>
  <span class="s1">&#39;properties.bootstrap.servers&#39;</span><span class="o">=</span><span class="s1">&#39;kafka:9092&#39;</span><span class="p">,</span>
  <span class="s1">&#39;properties.group.id&#39;</span><span class="o">=</span><span class="s1">&#39;testGroup&#39;</span><span class="p">,</span>
  <span class="s1">&#39;format&#39;</span><span class="o">=</span><span class="s1">&#39;csv&#39;</span><span class="p">,</span>
  <span class="s1">&#39;csv.field-delimiter&#39;</span><span class="o">=</span><span class="s1">&#39;|&#39;</span>
<span class="p">);</span></code></pre></div>

<p>With <code>prod_lineitem</code> stored in Hive, you can now write a query that will enrich the incoming stream with static data kept in Postgres. To illustrate how this works, let’s calculate the item prices based on the current currency rates:</p>

<div class="highlight"><pre><code class="language-sql"><span class="n">USE</span> <span class="k">CATALOG</span> <span class="n">postgres</span><span class="p">;</span>
<span class="k">SELECT</span>
  <span class="n">l_proctime</span> <span class="k">AS</span> <span class="o">`</span><span class="n">querytime</span><span class="o">`</span><span class="p">,</span>
  <span class="n">l_orderkey</span> <span class="k">AS</span> <span class="o">`</span><span class="k">order</span><span class="o">`</span><span class="p">,</span>
  <span class="n">l_linenumber</span> <span class="k">AS</span> <span class="o">`</span><span class="n">linenumber</span><span class="o">`</span><span class="p">,</span>
  <span class="n">l_currency</span> <span class="k">AS</span> <span class="o">`</span><span class="n">currency</span><span class="o">`</span><span class="p">,</span>
  <span class="n">rs_rate</span> <span class="k">AS</span> <span class="o">`</span><span class="n">cur_rate</span><span class="o">`</span><span class="p">,</span>
  <span class="p">(</span><span class="n">l_extendedprice</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l_discount</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">l_tax</span><span class="p">))</span> <span class="o">/</span> <span class="n">rs_rate</span> <span class="k">AS</span> <span class="o">`</span><span class="n">open_in_euro</span><span class="o">`</span>
<span class="k">FROM</span> <span class="n">hive</span><span class="p">.</span><span class="o">`</span><span class="k">default</span><span class="o">`</span><span class="p">.</span><span class="n">prod_lineitem</span>
<span class="k">JOIN</span> <span class="n">prod_rates</span> <span class="k">FOR</span> <span class="n">SYSTEM_TIME</span> <span class="k">AS</span> <span class="k">OF</span> <span class="n">l_proctime</span> <span class="k">ON</span> <span class="n">rs_symbol</span> <span class="o">=</span> <span class="n">l_currency</span>
<span class="k">WHERE</span>
  <span class="n">l_linestatus</span> <span class="o">=</span> <span class="s1">&#39;O&#39;</span><span class="p">;</span></code></pre></div>

<p>The query above uses a <code>SYSTEM AS OF</code> <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table">clause</a> for executing a temporal join. If you’d like to learn more about the different kind of joins you can do in Flink I highly encourage you to check <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#joins">this documentation page</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Catalogs can be extremely powerful when building data platforms aimed at reusing the work of different teams in an organization. Centralizing the metadata is a common practice for improving productivity, security, and compliance when working with data.</p>

<p>Flink provides flexible metadata management capabilities, that aim at reducing the cumbersome, repetitive work needed before querying the data such as defining schemas, connection properties etc. As of version 1.11, Flink provides a native, comprehensive integration with Hive Metastore and a read-only version for Postgres catalogs.</p>

<p>You can get started with Flink and catalogs by reading <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html">the docs</a>. If you want to play around with Flink SQL (e.g. try out how catalogs work in Flink yourself), you can check <a href="https://github.com/fhueske/flink-sql-demo">this demo</a> prepared by our colleagues Fabian and Timo — it runs in a dockerized environment, and we used it for the examples in this blog post.</p>
</p>

      <p><a href="/2020/07/23/catalogs.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2020/07/21/release-1.11.1.html">Apache Flink 1.11.1 Released</a></h2>

      <p>21 Jul 2020
       Dian Fu (<a href="https://twitter.com/DianFu11">@DianFu11</a>)</p>

      <p><p>The Apache Flink community released the first bugfix version of the Apache Flink 1.11 series.</p>

<p>This release includes 44 fixes and minor improvements for Flink 1.11.0. The list below includes a detailed list of all fixes and improvements.</p>

<p>We highly recommend all users to upgrade to Flink 1.11.1.</p>

<p>Updated Maven dependencies:</p>

<div class="highlight"><pre><code class="language-xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-java<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.1<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-streaming-java_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.1<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-clients_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.1<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></div>

<p>You can find the binaries on the updated <a href="/downloads.html">Downloads page</a>.</p>

<p>List of resolved issues:</p>

<h2>        Sub-task
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-15794">FLINK-15794</a>] -         Rethink default value of kubernetes.container.image
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18324">FLINK-18324</a>] -         Translate updated data type and function page into Chinese
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18387">FLINK-18387</a>] -         Translate &quot;BlackHole SQL Connector&quot; page into Chinese
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18388">FLINK-18388</a>] -         Translate &quot;CSV Format&quot; page into Chinese
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18391">FLINK-18391</a>] -         Translate &quot;Avro Format&quot; page into Chinese
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18395">FLINK-18395</a>] -         Translate &quot;ORC Format&quot; page into Chinese
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18469">FLINK-18469</a>] -         Add Application Mode to release notes.
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18524">FLINK-18524</a>] -         Scala varargs cause exception for new inference
</li>
</ul>

<h2>        Bug
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-15414">FLINK-15414</a>] -         KafkaITCase#prepare failed in travis
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-16181">FLINK-16181</a>] -         IfCallGen will throw NPE for primitive types in blink
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-16572">FLINK-16572</a>] -         CheckPubSubEmulatorTest is flaky on Azure
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-17543">FLINK-17543</a>] -         Rerunning failed azure jobs fails when uploading logs
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-17636">FLINK-17636</a>] -         SingleInputGateTest.testConcurrentReadStateAndProcessAndClose: Trying to read from released RecoveredInputChannel
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18097">FLINK-18097</a>] -         History server doesn&#39;t clean all job json files
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18419">FLINK-18419</a>] -         Can not create a catalog from user jar
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18434">FLINK-18434</a>] -         Can not select fields with JdbcCatalog
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18440">FLINK-18440</a>] -         ROW_NUMBER function: ROW/RANGE not allowed with RANK, DENSE_RANK or ROW_NUMBER functions
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18461">FLINK-18461</a>] -         Changelog source can&#39;t be insert into upsert sink
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18470">FLINK-18470</a>] -         Tests RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte &amp; RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort fail locally
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18471">FLINK-18471</a>] -         flink-runtime lists &quot;org.uncommons.maths:uncommons-maths:1.2.2a&quot; as a bundled dependency, but it isn&#39;t
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18477">FLINK-18477</a>] -         ChangelogSocketExample does not work
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18478">FLINK-18478</a>] -         AvroDeserializationSchema does not work with types generated by avrohugger
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18485">FLINK-18485</a>] -         Kerberized YARN per-job on Docker test failed during unzip jce_policy-8.zip
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18519">FLINK-18519</a>] -         Propagate exception to client when execution fails for REST submission
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18520">FLINK-18520</a>] -         New Table Function type inference fails
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18529">FLINK-18529</a>] -         Query Hive table and filter by timestamp partition can fail
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18539">FLINK-18539</a>] -         StreamExecutionEnvironment#addSource(SourceFunction, TypeInformation) doesn&#39;t use the user defined type information
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18573">FLINK-18573</a>] -         InfluxDB reporter cannot be loaded as plugin
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18583">FLINK-18583</a>] -         The _id field is incorrectly set to index in Elasticsearch6 DynamicTableSink
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18585">FLINK-18585</a>] -         Dynamic index can not work in new DynamicTableSink
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18591">FLINK-18591</a>] -         Fix the format issue for metrics web page
</li>
</ul>

<h2>        Improvement
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18186">FLINK-18186</a>] -         Various updates on Kubernetes standalone document
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18422">FLINK-18422</a>] -         Update Prefer tag in documentation &#39;Fault Tolerance training lesson&#39;
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18457">FLINK-18457</a>] -         Fix invalid links in &quot;Detecting Patterns&quot; page of &quot;Streaming Concepts&quot;
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18472">FLINK-18472</a>] -         Local Installation Getting Started Guide
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18484">FLINK-18484</a>] -         RowSerializer arity error does not provide specific information about the mismatch
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18501">FLINK-18501</a>] -         Mapping of Pluggable Filesystems to scheme is not properly logged
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18526">FLINK-18526</a>] -         Add the configuration of Python UDF using Managed Memory in the doc of Pyflink
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18532">FLINK-18532</a>] -         Remove Beta tag from MATCH_RECOGNIZE docs
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18561">FLINK-18561</a>] -         Build manylinux1 with better compatibility instead of manylinux2014 Python Wheel Packages
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18593">FLINK-18593</a>] -         Hive bundle jar URLs are broken
</li>
</ul>

<h2>        Test
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18534">FLINK-18534</a>] -         KafkaTableITCase.testKafkaDebeziumChangelogSource failed with &quot;Topic &#39;changelog_topic&#39; already exists&quot;
</li>
</ul>

<h2>        Task
</h2>
<ul>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18502">FLINK-18502</a>] -         Add the page &#39;legacySourceSinks.zh.md&#39;  into the directory &#39;docs/dev/table&#39; 
</li>
<li>[<a href="https://issues.apache.org/jira/browse/FLINK-18505">FLINK-18505</a>] -          Correct the content of &#39;sourceSinks.zh.md&#39; 
</li>
</ul>
</p>

      <p><a href="/news/2020/07/21/release-1.11.1.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2020/07/14/application-mode.html">Application Deployment in Flink: Current State and the new Application Mode</a></h2>

      <p>14 Jul 2020
       Kostas Kloudas (<a href="https://twitter.com/kkloudas">@kkloudas</a>)</p>

      <p><p>With the rise of stream processing and real-time analytics as a critical tool for modern 
businesses, an increasing number of organizations build platforms with Apache Flink at their
core and offer it internally as a service. Many talks with related topics from companies 
like <a href="https://www.youtube.com/watch?v=VX3S9POGAdU">Uber</a>, <a href="https://www.youtube.com/watch?v=VX3S9POGAdU">Netflix</a>
and <a href="https://www.youtube.com/watch?v=cH9UdK0yYjc">Alibaba</a> in the latest editions of Flink Forward further 
illustrate this trend.</p>

<p>These platforms aim at simplifying application submission internally by lifting all the 
operational burden from the end user. To submit Flink applications, these platforms 
usually expose only a centralized or low-parallelism endpoint (<em>e.g.</em> a Web frontend) 
for application submission that we will call the <em>Deployer</em>.</p>

<p>One of the roadblocks that platform developers and maintainers often mention is that the 
Deployer can be a heavy resource consumer that is difficult to provision for. Provisioning 
for average load can lead to the Deployer service being overwhelmed with deployment 
requests (in the worst case, for all production applications in a short period of time), 
while planning based on top load leads to unnecessary costs. Building on this observation, 
Flink 1.11 introduces the <a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/#application-mode">Application Mode</a> 
as a deployment option, which allows for a lightweight, more scalable application 
submission process that manages to spread more evenly the application deployment load 
across the nodes in the cluster.</p>

<p>In order to understand the problem and how the Application Mode solves it, we start by 
describing briefly the current status of application execution in Flink, before 
describing the architectural changes introduced by the new deployment mode and how to 
leverage them.</p>

<h1 id="application-execution-in-flink">Application Execution in Flink</h1>

<p>The execution of an application in Flink mainly involves three entities: the <em>Client</em>, 
the <em>JobManager</em> and the <em>TaskManagers</em>. The Client is responsible for submitting the application to the 
cluster, the JobManager is responsible for the necessary bookkeeping during execution, 
and the TaskManagers are the ones doing the actual computation. For more details please 
refer to <a href="https://ci.apache.org/projects/flink/flink-docs-stable/concepts/flink-architecture.html">Flink’s Architecture</a>
documentation page.</p>

<h2 id="current-deployment-modes">Current Deployment Modes</h2>

<p>Before the introduction of the Application Mode in version 1.11, Flink allowed users to execute an application either on a 
<em>Session</em> or a <em>Per-Job Cluster</em>. The differences between the two have to do with the cluster 
lifecycle and the resource isolation guarantees they provide.</p>

<h3 id="session-mode">Session Mode</h3>

<p>Session Mode assumes an already running cluster and uses the resources of that cluster to 
execute any submitted application. Applications executed in the same (session) cluster use,
and consequently compete for, the same resources. This has the advantage that you do not 
pay the resource overhead of spinning up a full cluster for every submitted job. But, if 
one of the jobs misbehaves or brings down a TaskManager, then all jobs running on that 
TaskManager will be affected by the failure. Apart from a negative impact on the job that 
caused the failure, this implies a potential massive recovery process with all the 
restarting jobs accessing the file system concurrently and making it unavailable to other 
services. Additionally, having a single cluster running multiple jobs implies more load 
for the JobManager, which is responsible for the bookkeeping of all the jobs in the 
cluster. This mode is ideal for short jobs where startup latency is of high importance, 
<em>e.g.</em> interactive queries.</p>

<h3 id="per-job-mode">Per-Job Mode</h3>

<p>In Per-Job Mode, the available cluster manager framework (<em>e.g.</em> YARN or Kubernetes) is 
used to spin up a Flink cluster for each submitted job, which is available to that job 
only. When the job finishes, the cluster is shut down and any lingering resources 
(<em>e.g.</em> files) are cleaned up. This mode allows for better resource isolation, as a 
misbehaving job cannot affect any other job. In addition, it spreads the load of 
bookkeeping across multiple entities, as each application has its own JobManager. 
Given the aforementioned resource isolation concerns of the Session Mode, users often 
opt for the Per-Job Mode for long-running jobs which are willing to accept some increase 
in startup latency in favor of resilience.</p>

<p>To summarize, in Session Mode, the cluster lifecycle is independent of any job running on 
the cluster and all jobs running on the cluster share its resources. The per-job mode 
chooses to pay the price of spinning up a cluster for every submitted job, in order to 
provide better resource isolation guarantees as the resources are not shared across jobs. 
In this case, the lifecycle of the cluster is bound to that of the job.</p>

<h2 id="application-submission">Application Submission</h2>

<p>Flink application execution consists of two stages: <em>pre-flight</em>, when the users’ <code>main()</code>
method is called; and <em>runtime</em>, which is triggered as soon as the user code calls <code>execute()</code>.
The <code>main()</code> method constructs the user program using one of Flink’s APIs 
(DataStream API, Table API, DataSet API). When the <code>main()</code> method calls <code>env.execute()</code>, 
the user-defined pipeline is translated into a form that Flink’s runtime can understand, 
called the <em>job graph</em>, and it is shipped to the cluster.</p>

<p>Despite their differences, both session and per-job modes execute the application’s <code>main()</code> 
method, <em>i.e.</em> the <em>pre-flight</em> phase, on the client side.<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>

<p>This is usually not a problem for individual users who already have all the dependencies
of their jobs locally, and then submit their applications through a client running on
their machine. But in the case of submission through a remote entity like the Deployer,
this process includes:</p>

<ul>
  <li>
    <p>downloading the application’s dependencies locally,</p>
  </li>
  <li>
    <p>executing the main()method to extract the job graph,</p>
  </li>
  <li>
    <p>ship the job graph and its dependencies to the cluster for execution and,</p>
  </li>
  <li>
    <p>potentially, wait for the result.</p>
  </li>
</ul>

<p>This makes the Client a heavy resource consumer as it may need substantial network
bandwidth to download dependencies and ship binaries to the cluster, and CPU cycles to
execute the <code>main()</code> method. This problem is even more pronounced as more users share
the same Client.</p>

<div style="line-height:60%;">
    <br />
</div>

<center>
<img src="/img/blog/2020-07-14-application-mode/session-per-job.png" width="75%" alt="Session and Per-Job Mode" />
</center>

<div style="line-height:150%;">
    <br />
</div>

<p>The figure above illustrates the two deployment modes using 3 applications depicted in
<span style="color:red">red</span>, <span style="color:blue">blue</span> and <span style="color:green">green</span>. 
Each one has a parallelism of 3. The black rectangles represent 
different processes: TaskManagers, JobManagers and the Deployer; and we assume a single 
Deployer process in all scenarios. The colored triangles represent the load of the 
submission process, while the colored rectangles represent the load of the TaskManager 
and JobManager processes. As shown in the figure, the Deployer in both per-job and 
session mode share the same load. Their difference lies in the distribution of the 
tasks and the JobManager load. In the Session Mode, there is a single JobManager for 
all the jobs in the cluster while in the per-job mode, there is one for each job. In 
addition, tasks in Session Mode are assigned randomly to TaskManagers while in Per-Job 
Mode, each TaskManager can only have tasks of a single job.</p>

<h1 id="application-mode">Application Mode</h1>

<p><img style="float: right;margin-left:10px;margin-right: 15px;" src="/img/blog/2020-07-14-application-mode/application.png" width="320px" alt="Application Mode" /></p>

<p>The Application Mode builds on the above observations and tries to combine the resource
isolation of the per-job mode with a lightweight and scalable application submission 
process. To achieve this, it creates a cluster <em>per submitted application</em>, but this 
time, the <code>main()</code> method of the application is executed on the JobManager.</p>

<p>Creating a cluster per application can be seen as creating a session cluster shared 
only among the jobs of a particular application and torn down when the application 
finishes. With this architecture, the Application Mode provides the same resource 
isolation and load balancing guarantees as the Per-Job Mode, but at the granularity of 
a whole application. This makes sense, as jobs belonging to the same application are 
expected to be correlated and treated as a unit.</p>

<p>Executing the <code>main()</code> method on the JobManager allows saving the CPU cycles required 
for extracting the job graph, but also the bandwidth required on the client for 
downloading the dependencies locally and shipping the job graph and its dependencies 
to the cluster. Furthermore, it spreads the network load more evenly, as there is one 
JobManager per application. This is illustrated in the figure above, where we have the 
same scenario as in the session and per-job deployment mode section, but this time 
the client load has shifted to the JobManager of each application.</p>

<div class="alert alert-info">
  <p><span class="label label-info" style="display: inline-block"><span class="glyphicon glyphicon-info-sign" aria-hidden="true"></span> Note</span>
In the Application Mode, the main() method is executed on the cluster and not on the Client, as in the other modes. 
This may have implications for your code as, for example, any paths you register in your 
environment using the registerCachedFile() must be accessible by the JobManager of 
your application.</p>
</div>

<p>Compared to the Per-Job Mode, the Application Mode allows the submission of applications
consisting of multiple jobs. The order of job execution is not affected by the deployment
mode but by the call used to launch the job. Using the blocking <code>execute()</code> method 
establishes an order and will lead to the execution of the “next” job being postponed 
until “this” job finishes. In contrast, the non-blocking <code>executeAsync()</code> method will 
immediately continue to submit the “next” job as soon as the current job is submitted.</p>

<h2 id="reducing-network-requirements">Reducing Network Requirements</h2>

<p>As described above, by executing the application’s <code>main()</code> method on the JobManager, 
the Application Mode manages to save a lot of the resources previously required during 
job submission. But there is still room for improvement.</p>

<p>Focusing on YARN, which already supports all the optimizations mentioned here<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>, and
even with the Application Mode in place, the Client is still required to send the user 
jar to the JobManager. In addition, <em>for each application</em>, the Client has to ship to 
the cluster the “flink-dist” directory which contains the binaries of the framework 
itself, including the <code>flink-dist.jar</code>, <code>lib/</code> and <code>plugin/</code> directories. These two can 
account for a substantial amount of bandwidth on the client side. Furthermore, shipping 
the same flink-dist binaries on every submission is both a waste of bandwidth but also 
of storage space which can be alleviated by simply allowing applications to share the 
same binaries.</p>

<p>In Flink 1.11, we introduce options that allow the user to:</p>

<ol>
  <li>
    <p>Specify a remote path to a directory where YARN can find the Flink distribution binaries, and</p>
  </li>
  <li>
    <p>Specify a remote path where YARN can find the user jar.</p>
  </li>
</ol>

<p>For 1., we leverage YARN’s distributed cache and allow applications to share these 
binaries. So, if an application happens to find copies of Flink on the local storage 
of its TaskManager due to a previous application that was executed on the same 
TaskManager, it will not even have to download it internally.</p>

<div class="alert alert-info">
  <p><span class="label label-info" style="display: inline-block"><span class="glyphicon glyphicon-info-sign" aria-hidden="true"></span> Note</span>
Both optimizations are available to all deployment modes on YARN, and not only the Application Mode.</p>
</div>

<h1 id="example-application-mode-on-yarn">Example: Application Mode on Yarn</h1>

<p>For a full description, please refer to the official Flink documentation and more 
specifically to the page that refers to your cluster management framework, <em>e.g.</em> 
<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/yarn_setup.html#run-an-application-in-application-mode">YARN</a> 
or <a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/native_kubernetes.html#flink-kubernetes-application">Kubernetes</a>.
Here we will give some examples around YARN, where all the above features are available.</p>

<p>To launch an application in Application Mode, you can use:</p>

<pre><code><b>./bin/flink run-application -t yarn-application</b> ./MyApplication.jar</code></pre>

<p>With this command, all configuration parameters, such as the path to a savepoint to 
be used to bootstrap the application’s state or the required JobManager/TaskManager 
memory sizes, can be specified by their configuration option, prefixed by <code>-D</code>. For 
a catalog of the available configuration options, please refer to Flink’s 
<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html">configuration page</a>.</p>

<p>As an example, the command to specify the memory sizes of the JobManager and the 
TaskManager would look like:</p>

<pre><code>./bin/flink run-application -t yarn-application \
    <b>-Djobmanager.memory.process.size=2048m</b> \
    <b>-Dtaskmanager.memory.process.size=4096m</b> \
    ./MyApplication.jar
</code></pre>

<p>As discussed earlier, the above will make sure that your application’s <code>main()</code> method 
will be executed on the JobManager.</p>

<p>To further save the bandwidth of shipping the Flink distribution to the cluster, consider
pre-uploading the Flink distribution to a location accessible by YARN and using the 
<code>yarn.provided.lib.dirs</code> configuration option, as shown below:</p>

<pre><code>./bin/flink run-application -t yarn-application \
    -Djobmanager.memory.process.size=2048m \
    -Dtaskmanager.memory.process.size=4096m \
    <b>-Dyarn.provided.lib.dirs="hdfs://myhdfs/remote-flink-dist-dir"</b> \
    ./MyApplication.jar
</code></pre>

<p>Finally, in order to further save the bandwidth required to submit your application jar,
you can pre-upload it to HDFS, and specify the remote path that points to 
<code>./MyApplication.jar</code>, as shown below:</p>

<pre><code>./bin/flink run-application -t yarn-application \
    -Djobmanager.memory.process.size=2048m \
    -Dtaskmanager.memory.process.size=4096m \
    -Dyarn.provided.lib.dirs="hdfs://myhdfs/remote-flink-dist-dir" \
    <b>hdfs://myhdfs/jars/MyApplication.jar</b>
</code></pre>

<p>This will make the job submission extra lightweight as the needed Flink jars and the 
application jar are going to be picked up from the specified remote locations rather 
than be shipped to the cluster by the Client. The only thing the Client will ship to 
the cluster is the configuration of your application which includes all the 
aforementioned paths.</p>

<h1 id="conclusion">Conclusion</h1>

<p>We hope that this discussion helped you understand the differences between the various 
deployment modes offered by Flink and will help you to make informed decisions about 
which one is suitable in your own setup. Feel free to play around with them and report 
any issues you may find. If you have any questions or requests, do not hesitate to post 
them in the <a href="https://wints.github.io/flink-web//community.html#mailing-lists">mailing lists</a>
and, hopefully, see you (virtually) at one of our conferences or meetups soon!</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The only exceptions are the Web Submission and the Standalone per-job implementation. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Support for Kubernetes will come soon. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</p>

      <p><a href="/news/2020/07/14/application-mode.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    

    <!-- Pagination links -->
    
    <ul class="pager">
      <li>
      
        <a href="/blog/page2" class="previous">Previous</a>
      
      </li>
      <li>
        <span class="page_number ">Page: 3 of 15</span>
      </li>
      <li>
      
        <a href="/blog/page4" class="next">Next</a>
      
      </li>
    </ul>
    
  </div>

  <div class="col-sm-4" markdown="1">
    <!-- Blog posts by YEAR -->
    
      
      

      
    <h2>2021</h2>

    <ul id="markdown-toc">
      
      <li><a href="/2021/03/11/batch-execution-mode.html">A Rundown of Batch Execution Mode in the DataStream API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/03/03/release-1.12.2.html">Apache Flink 1.12.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/02/10/native-k8s-with-ha.html">How to natively deploy Flink on Kubernetes with High-Availability (HA)</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/29/release-1.10.3.html">Apache Flink 1.10.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/19/release-1.12.1.html">Apache Flink 1.12.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/01/18/rocksdb.html">Using RocksDB State Backend in Apache Flink: When and How</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/11/batch-fine-grained-fault-tolerance.html">Exploring fine-grained recovery of bounded data sets on Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/01/07/pulsar-flink-connector-270.html">What's New in the Pulsar Flink Connector 2.7.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/02/release-statefun-2.2.2.html">Stateful Functions 2.2.2 Release Announcement</a></li>

      
        
    </ul>
        <hr>
        <h2>2020</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2020/12/18/release-1.11.3.html">Apache Flink 1.11.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/12/15/pipelined-region-sheduling.html">Improvements in task scheduling for batch workloads in Apache Flink 1.12</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/12/10/release-1.12.0.html">Apache Flink 1.12.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/11/11/release-statefun-2.2.1.html">Stateful Functions 2.2.1 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1.html">From Aligned to Unaligned Checkpoints - Part 1: Checkpoints, Alignment, and Backpressure</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/10/13/stateful-serverless-internals.html">Stateful Functions Internals: Behind the scenes of Stateful Serverless</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/28/release-statefun-2.2.0.html">Stateful Functions 2.2.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/17/release-1.11.2.html">Apache Flink 1.11.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/04/community-update.html">Flink Community Update - August'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/09/01/flink-1.11-memory-management-improvements.html">Memory Management improvements for Flink’s JobManager in Apache Flink 1.11</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/25/release-1.10.2.html">Apache Flink 1.10.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/20/flink-docker.html">The State of Flink on Docker</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/08/19/statefun.html">Monitoring and Controlling Networks of IoT Devices with Flink Stateful Functions</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/06/external-resource.html">Accelerating your workload with GPU and other external resources</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/08/04/pyflink-pandas-udf-support-flink.html">PyFlink: The integration of Pandas into PyFlink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/30/demo-fraud-detection-3.html">Advanced Flink Application Patterns Vol.3: Custom Window Processing</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/07/28/flink-sql-demo-building-e2e-streaming-application.html">Flink SQL Demo: Building an End-to-End Streaming Application</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/27/community-update.html">Flink Community Update - July'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/07/23/catalogs.html">Sharing is caring - Catalogs in Flink SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/21/release-1.11.1.html">Apache Flink 1.11.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/14/application-mode.html">Application Deployment in Flink: Current State and the new Application Mode</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/06/release-1.11.0.html">Apache Flink 1.11.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/ecosystem/2020/06/23/flink-on-zeppelin-part2.html">Flink on Zeppelin Notebooks for Interactive Data Analysis - Part 2</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/15/flink-on-zeppelin-part1.html">Flink on Zeppelin Notebooks for Interactive Data Analysis - Part 1</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/11/community-update.html">Flink Community Update - June'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/09/release-statefun-2.1.0.html">Stateful Functions 2.1.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/12/release-1.10.1.html">Apache Flink 1.10.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/07/community-update.html">Flink Community Update - May'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/04/season-of-docs.html">Applying to Google Season of Docs 2020</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/24/release-1.9.3.html">Apache Flink 1.9.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/21/memory-management-improvements-flink-1.10.html">Memory Management Improvements with Apache Flink 1.10</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/15/flink-serialization-tuning-vol-1.html">Flink Serialization Tuning Vol. 1: Choosing your Serializer — if you can</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/04/09/pyflink-udf-support-flink.html">PyFlink: Introducing Python Support for UDFs in Flink's Table API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/07/release-statefun-2.0.0.html">Stateful Functions 2.0 - An Event-driven Database on Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/01/community-update.html">Flink Community Update - April'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2020/03/27/flink-for-data-warehouse.html">Flink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/03/24/demo-fraud-detection-2.html">Advanced Flink Application Patterns Vol.2: Dynamic Updates of Application Logic</a></li>

      
        
      
    
      
      

      
      <li><a href="/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html">Apache Beam: How Beam Runs on Top of Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/20/ddl.html">No Java Required: Configuring Sources and Sinks in SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/11/release-1.10.0.html">Apache Flink 1.10.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/07/a-guide-for-unit-testing-in-apache-flink.html">A Guide for Unit Testing in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/30/release-1.9.2.html">Apache Flink 1.9.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/29/state-unlocked-interacting-with-state-in-apache-flink.html">State Unlocked: Interacting with State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/15/demo-fraud-detection.html">Advanced Flink Application Patterns Vol.1: Case Study of a Fraud Detection System</a></li>

      
        
    </ul>
        <hr>
        <h2>2019</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2019/12/11/release-1.8.3.html">Apache Flink 1.8.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/12/09/flink-kubernetes-kudo.html">Running Apache Flink on Kubernetes with KUDO</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/11/25/query-pulsar-streams-using-apache-flink.html">How to query Pulsar Streams using Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/10/18/release-1.9.1.html">Apache Flink 1.9.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/feature/2019/09/13/state-processor-api.html">The State Processor API: How to Read, write and modify the state of Flink applications</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/09/11/release-1.8.2.html">Apache Flink 1.8.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/09/10/community-update.html">Flink Community Update - September'19</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/08/22/release-1.9.0.html">Apache Flink 1.9.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/07/23/flink-network-stack-2.html">Flink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/07/02/release-1.8.1.html">Apache Flink 1.8.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/06/26/broadcast-state.html">A Practical Guide to Broadcast State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/06/05/flink-network-stack.html">A Deep-Dive into Flink's Network Stack</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/19/state-ttl.html">State TTL in Flink 1.8.0: How to Automatically Cleanup Application State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/14/temporal-tables.html">Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/03/pulsar-flink.html">When Flink & Pulsar Come Together</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/04/17/sod.html">Apache Flink's Application to Season of Docs</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/04/09/release-1.8.0.html">Apache Flink 1.8.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2019/03/11/prometheus-monitoring.html">Flink and Prometheus: Cloud-native monitoring of streaming applications</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/03/06/ffsf-preview.html">What to expect from Flink Forward San Francisco 2019</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/25/monitoring-best-practices.html">Monitoring Apache Flink Applications 101</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/25/release-1.6.4.html">Apache Flink 1.6.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/15/release-1.7.2.html">Apache Flink 1.7.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/13/unified-batch-streaming-blink.html">Batch as a Special Case of Streaming and Alibaba's contribution of Blink</a></li>

      
        
    </ul>
        <hr>
        <h2>2018</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2018/12/26/release-1.5.6.html">Apache Flink 1.5.6 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/12/22/release-1.6.3.html">Apache Flink 1.6.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/12/21/release-1.7.1.html">Apache Flink 1.7.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/11/30/release-1.7.0.html">Apache Flink 1.7.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/10/29/release-1.6.2.html">Apache Flink 1.6.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/10/29/release-1.5.5.html">Apache Flink 1.5.5 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/09/20/release-1.6.1.html">Apache Flink 1.6.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/09/20/release-1.5.4.html">Apache Flink 1.5.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/08/21/release-1.5.3.html">Apache Flink 1.5.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/08/09/release-1.6.0.html">Apache Flink 1.6.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/07/31/release-1.5.2.html">Apache Flink 1.5.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/07/12/release-1.5.1.html">Apache Flink 1.5.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/05/25/release-1.5.0.html">Apache Flink 1.5.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/03/15/release-1.3.3.html">Apache Flink 1.3.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/03/08/release-1.4.2.html">Apache Flink 1.4.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2018/03/01/end-to-end-exactly-once-apache-flink.html">An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/02/15/release-1.4.1.html">Apache Flink 1.4.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2018/01/30/incremental-checkpointing.html">Managing Large State in Apache Flink: An Intro to Incremental Checkpointing</a></li>

      
        
    </ul>
        <hr>
        <h2>2017</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2017/12/21/2017-year-in-review.html">Apache Flink in 2017: Year in Review</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/12/12/release-1.4.0.html">Apache Flink 1.4.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/11/22/release-1.4-and-1.5-timeline.html">Looking Ahead to Apache Flink 1.4.0 and 1.5.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/08/05/release-1.3.2.html">Apache Flink 1.3.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2017/07/04/flink-rescalable-state.html">A Deep Dive into Rescalable State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/06/23/release-1.3.1.html">Apache Flink 1.3.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/06/01/release-1.3.0.html">Apache Flink 1.3.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/05/16/official-docker-image.html">Introducing Docker Images for Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/04/26/release-1.2.1.html">Apache Flink 1.2.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/04/04/dynamic-tables.html">Continuous Queries on Dynamic Tables</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/03/29/table-sql-api-update.html">From Streams to Tables and Back Again: An Update on Flink's Table & SQL API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/03/23/release-1.1.5.html">Apache Flink 1.1.5 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/02/06/release-1.2.0.html">Announcing Apache Flink 1.2.0</a></li>

      
        
    </ul>
        <hr>
        <h2>2016</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2016/12/21/release-1.1.4.html">Apache Flink 1.1.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/12/19/2016-year-in-review.html">Apache Flink in 2016: Year in Review</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/10/12/release-1.1.3.html">Apache Flink 1.1.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/09/05/release-1.1.2.html">Apache Flink 1.1.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/24/ff16-keynotes-panels.html">Flink Forward 2016: Announcing Schedule, Keynotes, and Panel Discussion</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/11/release-1.1.1.html">Flink 1.1.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/08/release-1.1.0.html">Announcing Apache Flink 1.1.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/05/24/stream-sql.html">Stream Processing for Everyone with SQL and Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/05/11/release-1.0.3.html">Flink 1.0.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/22/release-1.0.2.html">Flink 1.0.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/14/flink-forward-announce.html">Flink Forward 2016 Call for Submissions Is Now Open</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/06/cep-monitoring.html">Introducing Complex Event Processing (CEP) with Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/06/release-1.0.1.html">Flink 1.0.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/03/08/release-1.0.0.html">Announcing Apache Flink 1.0.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/02/11/release-0.10.2.html">Flink 0.10.2 Released</a></li>

      
        
    </ul>
        <hr>
        <h2>2015</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2015/12/18/a-year-in-review.html">Flink 2015: A year in review, and a lookout to 2016</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/12/11/storm-compatibility.html">Storm Compatibility in Apache Flink: How to run existing Storm topologies on Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/12/04/Introducing-windows.html">Introducing Stream Windows in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/11/27/release-0.10.1.html">Flink 0.10.1 released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/11/16/release-0.10.0.html">Announcing Apache Flink 0.10.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/16/off-heap-memory.html">Off-heap Memory in Apache Flink and the curious JIT compiler</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/03/flink-forward.html">Announcing Flink Forward 2015</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/01/release-0.9.1.html">Apache Flink 0.9.1 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/08/24/introducing-flink-gelly.html">Introducing Gelly: Graph Processing with Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/06/24/announcing-apache-flink-0.9.0-release.html">Announcing Apache Flink 0.9.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/05/14/Community-update-April.html">April 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/05/11/Juggling-with-Bits-and-Bytes.html">Juggling with Bits and Bytes</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/04/13/release-0.9.0-milestone1.html">Announcing Flink 0.9.0-milestone1 preview release</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/04/07/march-in-flink.html">March 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html">Peeking into Apache Flink's Engine Room</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/03/02/february-2015-in-flink.html">February 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/02/09/streaming-example.html">Introducing Flink Streaming</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/02/04/january-in-flink.html">January 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/01/21/release-0.8.html">Apache Flink 0.8.0 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/01/06/december-in-flink.html">December 2014 in the Flink community</a></li>

      
        
    </ul>
        <hr>
        <h2>2014</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2014/11/18/hadoop-compatibility.html">Hadoop Compatibility in Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/11/04/release-0.7.0.html">Apache Flink 0.7.0 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/10/03/upcoming_events.html">Upcoming Events</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/09/26/release-0.6.1.html">Apache Flink 0.6.1 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/08/26/release-0.6.html">Apache Flink 0.6 available</a></li>

      
    </ul>
      
    
  </div>
</div>
      </div>
    </div>

    <hr />

    <div class="row">
      <div class="footer text-center col-sm-12">
        <p>Copyright © 2014-2021 <a href="http://apache.org">The Apache Software Foundation</a>. All Rights Reserved.</p>
        <p>Apache Flink, Flink®, Apache®, the squirrel logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation.</p>
        <p><a href="/privacy-policy.html">Privacy Policy</a> &middot; <a href="/blog/feed.xml">RSS feed</a></p>
      </div>
    </div>
    </div><!-- /.container -->

    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/js/jquery.matchHeight-min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/codetabs.js"></script>
    <script src="/js/stickysidebar.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>
  </body>
</html>
