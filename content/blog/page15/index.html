<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink: Blog</title>
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/flink.css">
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Blog RSS feed -->
    <link href="/blog/feed.xml" rel="alternate" type="application/rss+xml" title="Apache Flink Blog: RSS feed" />

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!-- We need to load Jquery in the header for custom google analytics event tracking-->
    <script src="/js/jquery.min.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>  
    

    <!-- Main content. -->
    <div class="container">
    <div class="row">

      
     <div id="sidebar" class="col-sm-3">
        

<!-- Top navbar. -->
    <nav class="navbar navbar-default">
        <!-- The logo. -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="navbar-logo">
            <a href="/">
              <img alt="Apache Flink" src="/img/flink-header-logo.svg" width="147px" height="73px">
            </a>
          </div>
        </div><!-- /.navbar-header -->

        <!-- The navigation links. -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav navbar-main">

            <!-- First menu section explains visitors what Flink is -->

            <!-- What is Stream Processing? -->
            <!--
            <li><a href="/streamprocessing1.html">What is Stream Processing?</a></li>
            -->

            <!-- What is Flink? -->
            <li><a href="/flink-architecture.html">What is Apache Flink?</a></li>

            

            <!-- What is Stateful Functions? -->

            <li><a href="/stateful-functions.html">What is Stateful Functions?</a></li>

            <!-- Use cases -->
            <li><a href="/usecases.html">Use Cases</a></li>

            <!-- Powered by -->
            <li><a href="/poweredby.html">Powered By</a></li>


            &nbsp;
            <!-- Second menu section aims to support Flink users -->

            <!-- Downloads -->
            <li><a href="/downloads.html">Downloads</a></li>

            <!-- Getting Started -->
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Getting Started<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/index.html" target="_blank">With Flink <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/getting-started/project-setup.html" target="_blank">With Flink Stateful Functions <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="/training.html">Training Course</a></li>
              </ul>
            </li>

            <!-- Documentation -->
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Documentation<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12" target="_blank">Flink 1.12 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-docs-master" target="_blank">Flink Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2" target="_blank">Flink Stateful Functions 2.2 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-master" target="_blank">Flink Stateful Functions Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
              </ul>
            </li>

            <!-- getting help -->
            <li><a href="/gettinghelp.html">Getting Help</a></li>

            <!-- Blog -->
            <li class="active"><a href="/blog/"><b>Flink Blog</b></a></li>


            <!-- Flink-packages -->
            <li>
              <a href="https://flink-packages.org" target="_blank">flink-packages.org <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>
            &nbsp;

            <!-- Third menu section aim to support community and contributors -->

            <!-- Community -->
            <li><a href="/community.html">Community &amp; Project Info</a></li>

            <!-- Roadmap -->
            <li><a href="/roadmap.html">Roadmap</a></li>

            <!-- Contribute -->
            <li><a href="/contributing/how-to-contribute.html">How to Contribute</a></li>
            

            <!-- GitHub -->
            <li>
              <a href="https://github.com/apache/flink" target="_blank">Flink on GitHub <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>

            &nbsp;

            <!-- Language Switcher -->
            <li>
              
                
                  <!-- link to the Chinese home page when current is blog page -->
                  <a href="/zh">中文版</a>
                
              
            </li>

          </ul>

          <style>
            .smalllinks:link {
              display: inline-block !important; background: none; padding-top: 0px; padding-bottom: 0px; padding-right: 0px; min-width: 75px;
            }
          </style>

          <ul class="nav navbar-nav navbar-bottom">
          <hr />

            <!-- Twitter -->
            <li><a href="https://twitter.com/apacheflink" target="_blank">@ApacheFlink <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <!-- Visualizer -->
            <li class=" hidden-md hidden-sm"><a href="/visualizer/" target="_blank">Plan Visualizer <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <li >
                  <a href="/security.html">Flink Security</a>
            </li>

          <hr />

            <li><a href="https://apache.org" target="_blank">Apache Software Foundation <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <li>

              <a class="smalllinks" href="https://www.apache.org/licenses/" target="_blank">License</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/security/" target="_blank">Security</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/foundation/sponsorship.html" target="_blank">Donate</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/foundation/thanks.html" target="_blank">Thanks</a> <small><span class="glyphicon glyphicon-new-window"></span></small>
            </li>

          </ul>
        </div><!-- /.navbar-collapse -->
    </nav>

      </div>
      <div class="col-sm-9">
      <h1>Blog</h1>
<hr />

<div class="row">
  <div class="col-sm-8">
    <!-- Blog posts -->
    
    <article>
      <h2 class="blog-title"><a href="/news/2015/01/21/release-0.8.html">Apache Flink 0.8.0 available</a></h2>

      <p>21 Jan 2015
      </p>

      <p><p>We are pleased to announce the availability of Flink 0.8.0. This release includes new user-facing features as well as performance and bug fixes, extends the support for filesystems and introduces the Scala API and flexible windowing semantics for Flink Streaming. A total of 33 people have contributed to this release, a big thanks to all of them!</p>

<p><a href="http://www.apache.org/dyn/closer.cgi/flink/flink-0.8.0/flink-0.8.0-bin-hadoop2.tgz">Download Flink 0.8.0</a></p>

<p><a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12328699">See the release changelog</a></p>

<h2 id="overview-of-major-new-features">Overview of major new features</h2>

<ul>
  <li>
    <p><strong>Extended filesystem support</strong>: The former <code>DistributedFileSystem</code> interface has been generalized to <code>HadoopFileSystem</code> now supporting all sub classes of <code>org.apache.hadoop.fs.FileSystem</code>. This allows users to use all file systems supported by Hadoop with Apache Flink.
<a href="http://ci.apache.org/projects/flink/flink-docs-release-0.8/example_connectors.html">See connecting to other systems</a></p>
  </li>
  <li>
    <p><strong>Streaming Scala API</strong>: As an alternative to the existing Java API Streaming is now also programmable in Scala. The Java and Scala APIs have now the same syntax and transformations and will be kept from now on in sync in every future release.</p>
  </li>
  <li>
    <p><strong>Streaming windowing semantics</strong>: The new windowing api offers an expressive way to define custom logic for triggering the execution of a stream window and removing elements. The new features include out-of-the-box support for windows based in logical or physical time and data-driven properties on the events themselves among others. <a href="http://ci.apache.org/projects/flink/flink-docs-release-0.8/streaming_guide.html#window-operators">Read more here</a></p>
  </li>
  <li>
    <p><strong>Mutable and immutable objects in runtime</strong> All Flink versions before 0.8.0 were always passing the same objects to functions written by users. This is a common performance optimization, also used in other systems such as Hadoop.
 However, this is error-prone for new users because one has to carefully check that references to the object aren’t kept in the user function. Starting from 0.8.0, Flink allows to configure a mode which is disabling that mechanism.</p>
  </li>
  <li><strong>Performance and usability improvements</strong>: The new Apache Flink 0.8.0 release brings several new features which will significantly improve the performance and the usability of the system. Amongst others, these features include:
    <ul>
      <li>Improved input split assignment which maximizes computation locality</li>
      <li>Smart broadcasting mechanism which minimizes network I/O</li>
      <li>Custom partitioners which let the user control how the data is partitioned within the cluster. This helps to prevent data skewness and allows to implement highly efficient algorithms.</li>
      <li>coGroup operator now supports group sorting for its inputs</li>
    </ul>
  </li>
  <li>
    <p><strong>Kryo is the new fallback serializer</strong>: Apache Flink has a sophisticated type analysis and serialization framework that is able to handle commonly used types very efficiently.
 In addition to that, there is a fallback serializer for types which are not supported. Older versions of Flink used the reflective <a href="http://avro.apache.org/">Avro</a> serializer for that purpose. With this release, Flink is using the powerful <a href="https://github.com/EsotericSoftware/kryo">Kryo</a> and twitter-chill library for support of types such as Java Collections and Scala specifc types.</p>
  </li>
  <li>
    <p><strong>Hadoop 2.2.0+ is now the default Hadoop dependency</strong>: With Flink 0.8.0 we made the “hadoop2” build profile the default build for Flink. This means that all users using Hadoop 1 (0.2X or 1.2.X versions) have to specify  version “0.8.0-hadoop1” in their pom files.</p>
  </li>
  <li><strong>HBase module updated</strong> The HBase version has been updated to 0.98.6.1. Also, Hbase is now available to the Hadoop1 and Hadoop2 profile of Flink.</li>
</ul>

<h2 id="contributors">Contributors</h2>

<ul>
  <li>Marton Balassi</li>
  <li>Daniel Bali</li>
  <li>Carsten Brandt</li>
  <li>Moritz Borgmann</li>
  <li>Stefan Bunk</li>
  <li>Paris Carbone</li>
  <li>Ufuk Celebi</li>
  <li>Nils Engelbach</li>
  <li>Stephan Ewen</li>
  <li>Gyula Fora</li>
  <li>Gabor Hermann</li>
  <li>Fabian Hueske</li>
  <li>Vasiliki Kalavri</li>
  <li>Johannes Kirschnick</li>
  <li>Aljoscha Krettek</li>
  <li>Suneel Marthi</li>
  <li>Robert Metzger</li>
  <li>Felix Neutatz</li>
  <li>Chiwan Park</li>
  <li>Flavio Pompermaier</li>
  <li>Mingliang Qi</li>
  <li>Shiva Teja Reddy</li>
  <li>Till Rohrmann</li>
  <li>Henry Saputra</li>
  <li>Kousuke Saruta</li>
  <li>Chesney Schepler</li>
  <li>Erich Schubert</li>
  <li>Peter Szabo</li>
  <li>Jonas Traub</li>
  <li>Kostas Tzoumas</li>
  <li>Timo Walther</li>
  <li>Daniel Warneke</li>
  <li>Chen Xu</li>
</ul>
</p>

      <p><a href="/news/2015/01/21/release-0.8.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2015/01/06/december-in-flink.html">December 2014 in the Flink community</a></h2>

      <p>06 Jan 2015
      </p>

      <p><p>This is the first blog post of a “newsletter” like series where we give a summary of the monthly activity in the Flink community. As the Flink project grows, this can serve as a “tl;dr” for people that are not following the Flink dev and user mailing lists, or those that are simply overwhelmed by the traffic.</p>

<h3 id="flink-graduation">Flink graduation</h3>

<p>The biggest news is that the Apache board approved Flink as a top-level Apache project! The Flink team is working closely with the Apache press team for an official announcement, so stay tuned for details!</p>

<h3 id="new-flink-website">New Flink website</h3>

<p>The <a href="http://flink.apache.org">Flink website</a> got a total make-over, both in terms of appearance and content.</p>

<h3 id="flink-irc-channel">Flink IRC channel</h3>

<p>A new IRC channel called #flink was created at irc.freenode.org. An easy way to access the IRC channel is through the <a href="http://webchat.freenode.net/">web client</a>.  Feel free to stop by to ask anything or share your ideas about Apache Flink!</p>

<h3 id="meetups-and-talks">Meetups and Talks</h3>

<p>Apache Flink was presented in the <a href="http://www.meetup.com/Netherlands-Hadoop-User-Group/events/218635152">Amsterdam Hadoop User Group</a></p>

<h2 id="notable-code-contributions">Notable code contributions</h2>

<p><strong>Note:</strong> Code contributions listed here may not be part of a release or even the current snapshot yet.</p>

<h3 id="streaming-scala-apihttpsgithubcomapacheincubator-flinkpull275"><a href="https://github.com/apache/incubator-flink/pull/275">Streaming Scala API</a></h3>

<p>The Flink Streaming Java API recently got its Scala counterpart. Once merged, Flink Streaming users can use both Scala and Java for their development. The Flink Streaming Scala API is built as a thin layer on top of the Java API, making sure that the APIs are kept easily in sync.</p>

<h3 id="intermediate-datasetshttpsgithubcomapacheincubator-flinkpull254"><a href="https://github.com/apache/incubator-flink/pull/254">Intermediate datasets</a></h3>

<p>This pull request introduces a major change in the Flink runtime. Currently, the Flink runtime is based on the notion of operators that exchange data through channels. With the PR, intermediate data sets that are produced by operators become first-class citizens in the runtime. While this does not have any user-facing impact yet, it lays the groundwork for a slew of future features such as blocking execution, fine-grained fault-tolerance, and more efficient data sharing between cluster and client.</p>

<h3 id="configurable-execution-modehttpsgithubcomapacheincubator-flinkpull259"><a href="https://github.com/apache/incubator-flink/pull/259">Configurable execution mode</a></h3>

<p>This pull request allows the user to change the object-reuse behaviour. Before this pull request, some operations would reuse objects passed to the user function while others would always create new objects. This introduces a system wide switch and changes all operators to either reuse objects or don’t reuse objects.</p>

<h3 id="distributed-coordination-via-akkahttpsgithubcomapacheincubator-flinkpull149"><a href="https://github.com/apache/incubator-flink/pull/149">Distributed Coordination via Akka</a></h3>

<p>Another major change is a complete rewrite of the JobManager / TaskManager components in Scala. In addition to that, the old RPC service was replaced by Actors, using the Akka framework.</p>

<h3 id="sorting-of-very-large-recordshttpsgithubcomapacheincubator-flinkpull249-"><a href="https://github.com/apache/incubator-flink/pull/249">Sorting of very large records</a></h3>

<p>Flink’s internal sort-algorithms were improved to better handle large records (multiple 100s of megabytes or larger). Previously, the system did in some cases hold instances of multiple large records, resulting in high memory consumption and JVM heap thrashing. Through this fix, large records are streamed through the operators, reducing the memory consumption and GC pressure. The system now requires much less memory to support algorithms that work on such large records.</p>

<h3 id="kryo-serialization-as-the-new-default-fallbackhttpsgithubcomapacheincubator-flinkpull271"><a href="https://github.com/apache/incubator-flink/pull/271">Kryo Serialization as the new default fallback</a></h3>

<p>Flink’s build-in type serialization framework is handles all common types very efficiently. Prior versions uses Avro to serialize types that the built-in framework could not handle.
Flink serialization system improved a lot over time and by now surpasses the capabilities of Avro in many cases. Kryo now serves as the default fallback serialization framework, supporting a much broader range of types.</p>

<h3 id="hadoop-filesystem-supporthttpsgithubcomapacheincubator-flinkpull268"><a href="https://github.com/apache/incubator-flink/pull/268">Hadoop FileSystem support</a></h3>

<p>This change permits users to use all file systems supported by Hadoop with Flink. In practice this means that users can use Flink with Tachyon, Google Cloud Storage (also out of the box Flink YARN support on Google Compute Cloud), FTP and all the other file system implementations for Hadoop.</p>

<h2 id="heading-to-the-080-release">Heading to the 0.8.0 release</h2>

<p>The community is working hard together with the Apache infra team to migrate the Flink infrastructure to a top-level project. At the same time, the Flink community is working on the Flink 0.8.0 release which should be out very soon.</p>
</p>

      <p><a href="/news/2015/01/06/december-in-flink.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2014/11/18/hadoop-compatibility.html">Hadoop Compatibility in Flink</a></h2>

      <p>18 Nov 2014 by Fabian Hüske (<a href="https://twitter.com/">@fhueske</a>)
      </p>

      <p><p><a href="http://hadoop.apache.org">Apache Hadoop</a> is an industry standard for scalable analytical data processing. Many data analysis applications have been implemented as Hadoop MapReduce jobs and run in clusters around the world. Apache Flink can be an alternative to MapReduce and improves it in many dimensions. Among other features, Flink provides much better performance and offers APIs in Java and Scala, which are very easy to use. Similar to Hadoop, Flink’s APIs provide interfaces for Mapper and Reducer functions, as well as Input- and OutputFormats along with many more operators. While being conceptually equivalent, Hadoop’s MapReduce and Flink’s interfaces for these functions are unfortunately not source compatible.</p>

<h2 id="flinks-hadoop-compatibility-package">Flink’s Hadoop Compatibility Package</h2>

<center>
<img src="/img/blog/hcompat-logos.png" style="width:30%;margin:15px" />
</center>

<p>To close this gap, Flink provides a Hadoop Compatibility package to wrap functions implemented against Hadoop’s MapReduce interfaces and embed them in Flink programs. This package was developed as part of a <a href="https://developers.google.com/open-source/soc/">Google Summer of Code</a> 2014 project.</p>

<p>With the Hadoop Compatibility package, you can reuse all your Hadoop</p>

<ul>
  <li><code>InputFormats</code> (mapred and mapreduce APIs)</li>
  <li><code>OutputFormats</code> (mapred and mapreduce APIs)</li>
  <li><code>Mappers</code> (mapred API)</li>
  <li><code>Reducers</code> (mapred API)</li>
</ul>

<p>in Flink programs without changing a line of code. Moreover, Flink also natively supports all Hadoop data types (<code>Writables</code> and <code>WritableComparable</code>).</p>

<p>The following code snippet shows a simple Flink WordCount program that solely uses Hadoop data types, InputFormat, OutputFormat, Mapper, and Reducer functions.</p>

<div class="highlight"><pre><code class="language-java"><span class="c1">// Definition of Hadoop Mapper function</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">Tokenizer</span> <span class="kd">implements</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
<span class="c1">// Definition of Hadoop Reducer function</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">Counter</span> <span class="kd">implements</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
  <span class="kd">final</span> <span class="n">String</span> <span class="n">inputPath</span> <span class="o">=</span> <span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">];</span>
  <span class="kd">final</span> <span class="n">String</span> <span class="n">outputPath</span> <span class="o">=</span> <span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">];</span>

  <span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
        
  <span class="c1">// Setup Hadoop’s TextInputFormat</span>
  <span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="n">hadoopInputFormat</span> <span class="o">=</span> 
      <span class="k">new</span> <span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;(</span>
        <span class="k">new</span> <span class="nf">TextInputFormat</span><span class="o">(),</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="k">new</span> <span class="nf">JobConf</span><span class="o">());</span>
  <span class="n">TextInputFormat</span><span class="o">.</span><span class="na">addInputPath</span><span class="o">(</span><span class="n">hadoopInputFormat</span><span class="o">.</span><span class="na">getJobConf</span><span class="o">(),</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">inputPath</span><span class="o">));</span>
  
  <span class="c1">// Read a DataSet with the Hadoop InputFormat</span>
  <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">createInput</span><span class="o">(</span><span class="n">hadoopInputFormat</span><span class="o">);</span>
  <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="n">text</span>
    <span class="c1">// Wrap Tokenizer Mapper function</span>
    <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopMapFunction</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span><span class="k">new</span> <span class="nf">Tokenizer</span><span class="o">()))</span>
    <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
    <span class="c1">// Wrap Counter Reducer function (used as Reducer and Combiner)</span>
    <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopReduceCombineFunction</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
      <span class="k">new</span> <span class="nf">Counter</span><span class="o">(),</span> <span class="k">new</span> <span class="nf">Counter</span><span class="o">()));</span>
        
  <span class="c1">// Setup Hadoop’s TextOutputFormat</span>
  <span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span> <span class="n">hadoopOutputFormat</span> <span class="o">=</span> 
    <span class="k">new</span> <span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
      <span class="k">new</span> <span class="n">TextOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(),</span> <span class="k">new</span> <span class="nf">JobConf</span><span class="o">());</span>
  <span class="n">hadoopOutputFormat</span><span class="o">.</span><span class="na">getJobConf</span><span class="o">().</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;mapred.textoutputformat.separator&quot;</span><span class="o">,</span> <span class="s">&quot; &quot;</span><span class="o">);</span>
  <span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">setOutputPath</span><span class="o">(</span><span class="n">hadoopOutputFormat</span><span class="o">.</span><span class="na">getJobConf</span><span class="o">(),</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">));</span>
        
  <span class="c1">// Output &amp; Execute</span>
  <span class="n">words</span><span class="o">.</span><span class="na">output</span><span class="o">(</span><span class="n">hadoopOutputFormat</span><span class="o">);</span>
  <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="s">&quot;Hadoop Compat WordCount&quot;</span><span class="o">);</span>
<span class="o">}</span></code></pre></div>

<p>As you can see, Flink represents Hadoop key-value pairs as <code>Tuple2&lt;key, value&gt;</code> tuples. Note, that the program uses Flink’s <code>groupBy()</code> transformation to group data on the key field (field 0 of the <code>Tuple2&lt;key, value&gt;</code>) before it is given to the Reducer function. At the moment, the compatibility package does not evaluate custom Hadoop partitioners, sorting comparators, or grouping comparators.</p>

<p>Hadoop functions can be used at any position within a Flink program and of course also be mixed with native Flink functions. This means that instead of assembling a workflow of Hadoop jobs in an external driver method or using a workflow scheduler such as <a href="http://oozie.apache.org">Apache Oozie</a>, you can implement an arbitrary complex Flink program consisting of multiple Hadoop Input- and OutputFormats, Mapper and Reducer functions. When executing such a Flink program, data will be pipelined between your Hadoop functions and will not be written to HDFS just for the purpose of data exchange.</p>

<center>
<img src="/img/blog/hcompat-flow.png" style="width:100%;margin:15px" />
</center>

<h2 id="what-comes-next">What comes next?</h2>

<p>While the Hadoop compatibility package is already very useful, we are currently working on a dedicated Hadoop Job operation to embed and execute Hadoop jobs as a whole in Flink programs, including their custom partitioning, sorting, and grouping code. With this feature, you will be able to chain multiple Hadoop jobs, mix them with Flink functions, and other operations such as <a href="http://ci.apache.org/projects/flink/flink-docs-release-0.7/spargel_guide.html">Spargel</a> operations (Pregel/Giraph-style jobs).</p>

<h2 id="summary">Summary</h2>

<p>Flink lets you reuse a lot of the code you wrote for Hadoop MapReduce, including all data types, all Input- and OutputFormats, and Mapper and Reducers of the mapred-API. Hadoop functions can be used within Flink programs and mixed with all other Flink functions. Due to Flink’s pipelined execution, Hadoop functions can arbitrarily be assembled without data exchange via HDFS. Moreover, the Flink community is currently working on a dedicated Hadoop Job operation to supporting the execution of Hadoop jobs as a whole.</p>

<p>If you want to use Flink’s Hadoop compatibility package checkout our <a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/batch/hadoop_compatibility.html">documentation</a>.</p>
</p>

      <p><a href="/news/2014/11/18/hadoop-compatibility.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2014/11/04/release-0.7.0.html">Apache Flink 0.7.0 available</a></h2>

      <p>04 Nov 2014
      </p>

      <p><p>We are pleased to announce the availability of Flink 0.7.0. This release includes new user-facing features as well as performance and bug fixes, brings the Scala and Java APIs in sync, and introduces Flink Streaming. A total of 34 people have contributed to this release, a big thanks to all of them!</p>

<p>Download Flink 0.7.0 <a href="http://flink.incubator.apache.org/downloads.html">here</a></p>

<p>See the release changelog <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12327648">here</a></p>

<h2 id="overview-of-major-new-features">Overview of major new features</h2>

<p><strong>Flink Streaming:</strong> The gem of the 0.7.0 release is undoubtedly Flink Streaming. Available currently in alpha, Flink Streaming provides a Java API on top of Apache Flink that can consume streaming data sources (e.g., from Apache Kafka, Apache Flume, and others) and process them in real time. A dedicated blog post on Flink Streaming and its performance is coming up here soon. You can check out the Streaming programming guide <a href="http://ci.apache.org/projects/flink/flink-docs-release-0.7/streaming_guide.html">here</a>.</p>

<p><strong>New Scala API:</strong> The Scala API has been completely rewritten. The Java and Scala APIs have now the same syntax and transformations and will be kept from now on in sync in every future release. See the new Scala API <a href="http://ci.apache.org/projects/flink/flink-docs-release-0.7/programming_guide.html">here</a>.</p>

<p><strong>Logical key expressions:</strong> You can now specify grouping and joining keys with logical names for member variables of POJO data types. For example, you can join two data sets as <code>persons.join(cities).where(“zip”).equalTo(“zipcode”)</code>. Read more <a href="http://ci.apache.org/projects/flink/flink-docs-release-0.7/programming_guide.html#specifying-keys">here</a>.</p>

<p><strong>Hadoop MapReduce compatibility:</strong> You can run unmodified Hadoop Mappers and Reducers (mapred API) in Flink, use all Hadoop data types, and read data with all Hadoop InputFormats.</p>

<p><strong>Collection-based execution backend:</strong> The collection-based execution backend enables you to execute a Flink job as a simple Java collections program, bypassing completely the Flink runtime and optimizer. This feature is extremely useful for prototyping, and embedding Flink jobs in projects in a very lightweight manner.</p>

<p><strong>Record API deprecated:</strong> The (old) Stratosphere Record API has been marked as deprecated and is planned for removal in the 0.9.0 release.</p>

<p><strong>BLOB service:</strong> This release contains a new service to distribute jar files and other binary data among the JobManager, TaskManagers and the client.</p>

<p><strong>Intermediate data sets:</strong> A major rewrite of the system internals introduces intermediate data sets as first class citizens. The internal state machine that tracks the distributed tasks has also been completely rewritten for scalability. While this is not visible as a user-facing feature yet, it is the foundation for several upcoming exciting features.</p>

<p><strong>Note:</strong> Currently, there is limited support for Java 8 lambdas when compiling and running from an IDE. The problem is due to type erasure and whether Java compilers retain type information. We are currently working with the Eclipse and OpenJDK communities to resolve this.</p>

<h2 id="contributors">Contributors</h2>

<ul>
  <li>Tamas Ambrus</li>
  <li>Mariem Ayadi</li>
  <li>Marton Balassi</li>
  <li>Daniel Bali</li>
  <li>Ufuk Celebi</li>
  <li>Hung Chang</li>
  <li>David Eszes</li>
  <li>Stephan Ewen</li>
  <li>Judit Feher</li>
  <li>Gyula Fora</li>
  <li>Gabor Hermann</li>
  <li>Fabian Hueske</li>
  <li>Vasiliki Kalavri</li>
  <li>Kristof Kovacs</li>
  <li>Aljoscha Krettek</li>
  <li>Sebastian Kruse</li>
  <li>Sebastian Kunert</li>
  <li>Matyas Manninger</li>
  <li>Robert Metzger</li>
  <li>Mingliang Qi</li>
  <li>Till Rohrmann</li>
  <li>Henry Saputra</li>
  <li>Chesnay Schelper</li>
  <li>Moritz Schubotz</li>
  <li>Hung Sendoh Chang</li>
  <li>Peter Szabo</li>
  <li>Jonas Traub</li>
  <li>Fabian Tschirschnitz</li>
  <li>Artem Tsikiridis</li>
  <li>Kostas Tzoumas</li>
  <li>Timo Walther</li>
  <li>Daniel Warneke</li>
  <li>Tobias Wiens</li>
  <li>Yingjun Wu</li>
</ul>
</p>

      <p><a href="/news/2014/11/04/release-0.7.0.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2014/10/03/upcoming_events.html">Upcoming Events</a></h2>

      <p>03 Oct 2014
      </p>

      <p><p>We are happy to announce several upcoming Flink events both in Europe and the US. Starting with a <strong>Flink hackathon in Stockholm</strong> (Oct 8-9) and a talk about Flink at the <strong>Stockholm Hadoop User Group</strong> (Oct 8). This is followed by the very first <strong>Flink Meetup in Berlin</strong> (Oct 15). In the US, there will be two Flink Meetup talks: the first one at the <strong>Pasadena Big Data User Group</strong> (Oct 29) and the second one at <strong>Silicon Valley Hands On Programming Events</strong> (Nov 4).</p>

<p>We are looking forward to seeing you at any of these events. The following is an overview of each event and links to the respective Meetup pages.</p>

<h3 id="flink-hackathon-stockholm-oct-8-9">Flink Hackathon, Stockholm (Oct 8-9)</h3>

<p>The hackathon will take place at KTH/SICS from Oct 8th-9th. You can sign up here: https://docs.google.com/spreadsheet/viewform?formkey=dDZnMlRtZHJ3Z0hVTlFZVjU2MWtoX0E6MA.</p>

<p>Here is a rough agenda and a list of topics to work upon or look into. Suggestions and more topics are welcome.</p>

<h4 id="wednesday-8th">Wednesday (8th)</h4>

<p>9:00 - 10:00  Introduction to Apache Flink, System overview, and Dev
environment (by Stephan)</p>

<p>10:15 - 11:00 Introduction to the topics (Streaming API and system by Gyula
&amp; Marton), (Graphs by Vasia / Martin / Stephan)</p>

<p>11:00 - 12:30 Happy hacking (part 1)</p>

<p>12:30 - Lunch (Food will be provided by KTH / SICS. A big thank you to them
and also to Paris, for organizing that)</p>

<p>13:xx - Happy hacking (part 2)</p>

<h4 id="thursday-9th">Thursday (9th)</h4>

<p>Happy hacking (continued)</p>

<h4 id="suggestions-for-topics">Suggestions for topics</h4>

<h5 id="streaming">Streaming</h5>

<ul>
  <li>
    <p>Sample streaming applications (e.g. continuous heavy hitters and topics
on the twitter stream)</p>
  </li>
  <li>
    <p>Implement a simple SQL to Streaming program parser. Possibly using
Apache Calcite (http://optiq.incubator.apache.org/)</p>
  </li>
  <li>
    <p>Implement different windowing methods (count-based, time-based, …)</p>
  </li>
  <li>
    <p>Implement different windowed operations (windowed-stream-join,
windowed-stream-co-group)</p>
  </li>
  <li>
    <p>Streaming state, and interaction with other programs (that access state
of a stream program)</p>
  </li>
</ul>

<h5 id="graph-analysis">Graph Analysis</h5>

<ul>
  <li>
    <p>Prototype a Graph DSL (simple graph building, filters, graph
properties, some algorithms)</p>
  </li>
  <li>
    <p>Prototype abstractions different Graph processing paradigms
(vertex-centric, partition-centric).</p>
  </li>
  <li>
    <p>Generalize the delta iterations, allow flexible state access.</p>
  </li>
</ul>

<h3 id="meetup-hadoop-user-group-talk-stockholm-oct-8">Meetup: Hadoop User Group Talk, Stockholm (Oct 8)</h3>

<p>Hosted by Spotify, opens at 6 PM.</p>

<p>http://www.meetup.com/stockholm-hug/events/207323222/</p>

<h3 id="st-flink-meetup-berlin-oct-15">1st Flink Meetup, Berlin (Oct 15)</h3>

<p>We are happy to announce the first Flink meetup in Berlin. You are very welcome to to sign up and attend. The event will be held in Betahaus Cafe.</p>

<p>http://www.meetup.com/Apache-Flink-Meetup/events/208227422/</p>

<h3 id="meetup-pasadena-big-data-user-group-oct-29">Meetup: Pasadena Big Data User Group (Oct 29)</h3>

<p>http://www.meetup.com/Pasadena-Big-Data-Users-Group/</p>

<h3 id="meetup-silicon-valley-hands-on-programming-events-nov-4">Meetup: Silicon Valley Hands On Programming Events (Nov 4)</h3>

<p>http://www.meetup.com/HandsOnProgrammingEvents/events/210504392/</p>

</p>

      <p><a href="/news/2014/10/03/upcoming_events.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2014/09/26/release-0.6.1.html">Apache Flink 0.6.1 available</a></h2>

      <p>26 Sep 2014
      </p>

      <p><p>We are happy to announce the availability of Flink 0.6.1.</p>

<p>0.6.1 is a maintenance release, which includes minor fixes across several parts
of the system. We suggest all users of Flink to work with this newest version.</p>

<p><a href="/downloads.html">Download</a> the release today.</p>
</p>

      <p><a href="/news/2014/09/26/release-0.6.1.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    
    <article>
      <h2 class="blog-title"><a href="/news/2014/08/26/release-0.6.html">Apache Flink 0.6 available</a></h2>

      <p>26 Aug 2014
      </p>

      <p><p>We are happy to announce the availability of Flink 0.6. This is the
first release of the system inside the Apache Incubator and under the
name Flink. Releases up to 0.5 were under the name Stratosphere, the
academic and open source project that Flink originates from.</p>

<h2 id="what-is-flink">What is Flink?</h2>

<p>Apache Flink is a general-purpose data processing engine for
clusters. It runs on YARN clusters on top of data stored in Hadoop, as
well as stand-alone. Flink currently has programming APIs in Java and
Scala. Jobs are executed via Flink’s own runtime engine. Flink
features:</p>

<p><strong>Robust in-memory and out-of-core processing:</strong> once read, data stays
  in memory as much as possible, and is gracefully de-staged to disk in
  the presence of memory pressure from limited memory or other
  applications. The runtime is designed to perform very well both in
  setups with abundant memory and in setups where memory is scarce.</p>

<p><strong>POJO-based APIs:</strong> when programming, you do not have to pack your
  data into key-value pairs or some other framework-specific data
  model. Rather, you can use arbitrary Java and Scala types to model
  your data.</p>

<p><strong>Efficient iterative processing:</strong> Flink contains explicit “iterate” operators
  that enable very efficient loops over data sets, e.g., for machine
  learning and graph applications.</p>

<p><strong>A modular system stack:</strong> Flink is not a direct implementation of its
  APIs but a layered system. All programming APIs are translated to an
  intermediate program representation that is compiled and optimized
  via a cost-based optimizer. Lower-level layers of Flink also expose
  programming APIs for extending the system.</p>

<p><strong>Data pipelining/streaming:</strong> Flink’s runtime is designed as a
  pipelined data processing engine rather than a batch processing
  engine. Operators do not wait for their predecessors to finish in
  order to start processing data. This results to very efficient
  handling of large data sets.</p>

<h2 id="release-06">Release 0.6</h2>

<p>Flink 0.6 builds on the latest Stratosphere 0.5 release. It includes
many bug fixes and improvements that make the system more stable and
robust, as well as breaking API changes.</p>

<p>The full release notes are available <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12327101">here</a>.</p>

<p>Download the release <a href="http://flink.incubator.apache.org/downloads.html">here</a>.</p>

<h2 id="contributors">Contributors</h2>

<ul>
  <li>Wilson Cao</li>
  <li>Ufuk Celebi</li>
  <li>Stephan Ewen</li>
  <li>Jonathan Hasenburg</li>
  <li>Markus Holzemer</li>
  <li>Fabian Hueske</li>
  <li>Sebastian Kunert</li>
  <li>Vikhyat Korrapati</li>
  <li>Aljoscha Krettek</li>
  <li>Sebastian Kruse</li>
  <li>Raymond Liu</li>
  <li>Robert Metzger</li>
  <li>Mingliang Qi</li>
  <li>Till Rohrmann</li>
  <li>Henry Saputra</li>
  <li>Chesnay Schepler</li>
  <li>Kostas Tzoumas</li>
  <li>Robert Waury</li>
  <li>Timo Walther</li>
  <li>Daniel Warneke</li>
  <li>Tobias Wiens</li>
</ul>
</p>

      <p><a href="/news/2014/08/26/release-0.6.html">Continue reading &raquo;</a></p>
    </article>

    <hr>
    

    <!-- Pagination links -->
    
    <ul class="pager">
      <li>
      
        <a href="/blog/page14" class="previous">Previous</a>
      
      </li>
      <li>
        <span class="page_number ">Page: 15 of 15</span>
      </li>
      <li>
      
        <span>Next</span>
      
      </li>
    </ul>
    
  </div>

  <div class="col-sm-4" markdown="1">
    <!-- Blog posts by YEAR -->
    
      
      

      
    <h2>2021</h2>

    <ul id="markdown-toc">
      
      <li><a href="/2021/03/11/batch-execution-mode.html">A Rundown of Batch Execution Mode in the DataStream API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/03/03/release-1.12.2.html">Apache Flink 1.12.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/02/10/native-k8s-with-ha.html">How to natively deploy Flink on Kubernetes with High-Availability (HA)</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/29/release-1.10.3.html">Apache Flink 1.10.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/19/release-1.12.1.html">Apache Flink 1.12.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/01/18/rocksdb.html">Using RocksDB State Backend in Apache Flink: When and How</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/11/batch-fine-grained-fault-tolerance.html">Exploring fine-grained recovery of bounded data sets on Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2021/01/07/pulsar-flink-connector-270.html">What's New in the Pulsar Flink Connector 2.7.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2021/01/02/release-statefun-2.2.2.html">Stateful Functions 2.2.2 Release Announcement</a></li>

      
        
    </ul>
        <hr>
        <h2>2020</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2020/12/18/release-1.11.3.html">Apache Flink 1.11.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/12/15/pipelined-region-sheduling.html">Improvements in task scheduling for batch workloads in Apache Flink 1.12</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/12/10/release-1.12.0.html">Apache Flink 1.12.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/11/11/release-statefun-2.2.1.html">Stateful Functions 2.2.1 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/10/15/from-aligned-to-unaligned-checkpoints-part-1.html">From Aligned to Unaligned Checkpoints - Part 1: Checkpoints, Alignment, and Backpressure</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/10/13/stateful-serverless-internals.html">Stateful Functions Internals: Behind the scenes of Stateful Serverless</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/28/release-statefun-2.2.0.html">Stateful Functions 2.2.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/17/release-1.11.2.html">Apache Flink 1.11.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/09/04/community-update.html">Flink Community Update - August'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/09/01/flink-1.11-memory-management-improvements.html">Memory Management improvements for Flink’s JobManager in Apache Flink 1.11</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/25/release-1.10.2.html">Apache Flink 1.10.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/20/flink-docker.html">The State of Flink on Docker</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/08/19/statefun.html">Monitoring and Controlling Networks of IoT Devices with Flink Stateful Functions</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/08/06/external-resource.html">Accelerating your workload with GPU and other external resources</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/08/04/pyflink-pandas-udf-support-flink.html">PyFlink: The integration of Pandas into PyFlink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/30/demo-fraud-detection-3.html">Advanced Flink Application Patterns Vol.3: Custom Window Processing</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/07/28/flink-sql-demo-building-e2e-streaming-application.html">Flink SQL Demo: Building an End-to-End Streaming Application</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/27/community-update.html">Flink Community Update - July'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/07/23/catalogs.html">Sharing is caring - Catalogs in Flink SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/21/release-1.11.1.html">Apache Flink 1.11.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/14/application-mode.html">Application Deployment in Flink: Current State and the new Application Mode</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/07/06/release-1.11.0.html">Apache Flink 1.11.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/ecosystem/2020/06/23/flink-on-zeppelin-part2.html">Flink on Zeppelin Notebooks for Interactive Data Analysis - Part 2</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/15/flink-on-zeppelin-part1.html">Flink on Zeppelin Notebooks for Interactive Data Analysis - Part 1</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/11/community-update.html">Flink Community Update - June'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/06/09/release-statefun-2.1.0.html">Stateful Functions 2.1.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/12/release-1.10.1.html">Apache Flink 1.10.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/07/community-update.html">Flink Community Update - May'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/05/04/season-of-docs.html">Applying to Google Season of Docs 2020</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/24/release-1.9.3.html">Apache Flink 1.9.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/21/memory-management-improvements-flink-1.10.html">Memory Management Improvements with Apache Flink 1.10</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/15/flink-serialization-tuning-vol-1.html">Flink Serialization Tuning Vol. 1: Choosing your Serializer — if you can</a></li>

      
        
      
    
      
      

      
      <li><a href="/2020/04/09/pyflink-udf-support-flink.html">PyFlink: Introducing Python Support for UDFs in Flink's Table API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/07/release-statefun-2.0.0.html">Stateful Functions 2.0 - An Event-driven Database on Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/04/01/community-update.html">Flink Community Update - April'20</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2020/03/27/flink-for-data-warehouse.html">Flink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/03/24/demo-fraud-detection-2.html">Advanced Flink Application Patterns Vol.2: Dynamic Updates of Application Logic</a></li>

      
        
      
    
      
      

      
      <li><a href="/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html">Apache Beam: How Beam Runs on Top of Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/20/ddl.html">No Java Required: Configuring Sources and Sinks in SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/11/release-1.10.0.html">Apache Flink 1.10.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/02/07/a-guide-for-unit-testing-in-apache-flink.html">A Guide for Unit Testing in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/30/release-1.9.2.html">Apache Flink 1.9.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/29/state-unlocked-interacting-with-state-in-apache-flink.html">State Unlocked: Interacting with State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2020/01/15/demo-fraud-detection.html">Advanced Flink Application Patterns Vol.1: Case Study of a Fraud Detection System</a></li>

      
        
    </ul>
        <hr>
        <h2>2019</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2019/12/11/release-1.8.3.html">Apache Flink 1.8.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/12/09/flink-kubernetes-kudo.html">Running Apache Flink on Kubernetes with KUDO</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/11/25/query-pulsar-streams-using-apache-flink.html">How to query Pulsar Streams using Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/10/18/release-1.9.1.html">Apache Flink 1.9.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/feature/2019/09/13/state-processor-api.html">The State Processor API: How to Read, write and modify the state of Flink applications</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/09/11/release-1.8.2.html">Apache Flink 1.8.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/09/10/community-update.html">Flink Community Update - September'19</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/08/22/release-1.9.0.html">Apache Flink 1.9.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/07/23/flink-network-stack-2.html">Flink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/07/02/release-1.8.1.html">Apache Flink 1.8.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/06/26/broadcast-state.html">A Practical Guide to Broadcast State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/06/05/flink-network-stack.html">A Deep-Dive into Flink's Network Stack</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/19/state-ttl.html">State TTL in Flink 1.8.0: How to Automatically Cleanup Application State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/14/temporal-tables.html">Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL</a></li>

      
        
      
    
      
      

      
      <li><a href="/2019/05/03/pulsar-flink.html">When Flink & Pulsar Come Together</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/04/17/sod.html">Apache Flink's Application to Season of Docs</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/04/09/release-1.8.0.html">Apache Flink 1.8.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2019/03/11/prometheus-monitoring.html">Flink and Prometheus: Cloud-native monitoring of streaming applications</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/03/06/ffsf-preview.html">What to expect from Flink Forward San Francisco 2019</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/25/monitoring-best-practices.html">Monitoring Apache Flink Applications 101</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/25/release-1.6.4.html">Apache Flink 1.6.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/15/release-1.7.2.html">Apache Flink 1.7.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2019/02/13/unified-batch-streaming-blink.html">Batch as a Special Case of Streaming and Alibaba's contribution of Blink</a></li>

      
        
    </ul>
        <hr>
        <h2>2018</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2018/12/26/release-1.5.6.html">Apache Flink 1.5.6 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/12/22/release-1.6.3.html">Apache Flink 1.6.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/12/21/release-1.7.1.html">Apache Flink 1.7.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/11/30/release-1.7.0.html">Apache Flink 1.7.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/10/29/release-1.6.2.html">Apache Flink 1.6.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/10/29/release-1.5.5.html">Apache Flink 1.5.5 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/09/20/release-1.6.1.html">Apache Flink 1.6.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/09/20/release-1.5.4.html">Apache Flink 1.5.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/08/21/release-1.5.3.html">Apache Flink 1.5.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/08/09/release-1.6.0.html">Apache Flink 1.6.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/07/31/release-1.5.2.html">Apache Flink 1.5.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/07/12/release-1.5.1.html">Apache Flink 1.5.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/05/25/release-1.5.0.html">Apache Flink 1.5.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/03/15/release-1.3.3.html">Apache Flink 1.3.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/03/08/release-1.4.2.html">Apache Flink 1.4.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2018/03/01/end-to-end-exactly-once-apache-flink.html">An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2018/02/15/release-1.4.1.html">Apache Flink 1.4.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2018/01/30/incremental-checkpointing.html">Managing Large State in Apache Flink: An Intro to Incremental Checkpointing</a></li>

      
        
    </ul>
        <hr>
        <h2>2017</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2017/12/21/2017-year-in-review.html">Apache Flink in 2017: Year in Review</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/12/12/release-1.4.0.html">Apache Flink 1.4.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/11/22/release-1.4-and-1.5-timeline.html">Looking Ahead to Apache Flink 1.4.0 and 1.5.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/08/05/release-1.3.2.html">Apache Flink 1.3.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/features/2017/07/04/flink-rescalable-state.html">A Deep Dive into Rescalable State in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/06/23/release-1.3.1.html">Apache Flink 1.3.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/06/01/release-1.3.0.html">Apache Flink 1.3.0 Release Announcement</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/05/16/official-docker-image.html">Introducing Docker Images for Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/04/26/release-1.2.1.html">Apache Flink 1.2.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/04/04/dynamic-tables.html">Continuous Queries on Dynamic Tables</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/03/29/table-sql-api-update.html">From Streams to Tables and Back Again: An Update on Flink's Table & SQL API</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/03/23/release-1.1.5.html">Apache Flink 1.1.5 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2017/02/06/release-1.2.0.html">Announcing Apache Flink 1.2.0</a></li>

      
        
    </ul>
        <hr>
        <h2>2016</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2016/12/21/release-1.1.4.html">Apache Flink 1.1.4 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/12/19/2016-year-in-review.html">Apache Flink in 2016: Year in Review</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/10/12/release-1.1.3.html">Apache Flink 1.1.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/09/05/release-1.1.2.html">Apache Flink 1.1.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/24/ff16-keynotes-panels.html">Flink Forward 2016: Announcing Schedule, Keynotes, and Panel Discussion</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/11/release-1.1.1.html">Flink 1.1.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/08/08/release-1.1.0.html">Announcing Apache Flink 1.1.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/05/24/stream-sql.html">Stream Processing for Everyone with SQL and Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/05/11/release-1.0.3.html">Flink 1.0.3 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/22/release-1.0.2.html">Flink 1.0.2 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/14/flink-forward-announce.html">Flink Forward 2016 Call for Submissions Is Now Open</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/06/cep-monitoring.html">Introducing Complex Event Processing (CEP) with Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/04/06/release-1.0.1.html">Flink 1.0.1 Released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/03/08/release-1.0.0.html">Announcing Apache Flink 1.0.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2016/02/11/release-0.10.2.html">Flink 0.10.2 Released</a></li>

      
        
    </ul>
        <hr>
        <h2>2015</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2015/12/18/a-year-in-review.html">Flink 2015: A year in review, and a lookout to 2016</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/12/11/storm-compatibility.html">Storm Compatibility in Apache Flink: How to run existing Storm topologies on Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/12/04/Introducing-windows.html">Introducing Stream Windows in Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/11/27/release-0.10.1.html">Flink 0.10.1 released</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/11/16/release-0.10.0.html">Announcing Apache Flink 0.10.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/16/off-heap-memory.html">Off-heap Memory in Apache Flink and the curious JIT compiler</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/03/flink-forward.html">Announcing Flink Forward 2015</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/09/01/release-0.9.1.html">Apache Flink 0.9.1 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/08/24/introducing-flink-gelly.html">Introducing Gelly: Graph Processing with Apache Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/06/24/announcing-apache-flink-0.9.0-release.html">Announcing Apache Flink 0.9.0</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/05/14/Community-update-April.html">April 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/05/11/Juggling-with-Bits-and-Bytes.html">Juggling with Bits and Bytes</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/04/13/release-0.9.0-milestone1.html">Announcing Flink 0.9.0-milestone1 preview release</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/04/07/march-in-flink.html">March 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html">Peeking into Apache Flink's Engine Room</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/03/02/february-2015-in-flink.html">February 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/02/09/streaming-example.html">Introducing Flink Streaming</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/02/04/january-in-flink.html">January 2015 in the Flink community</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/01/21/release-0.8.html">Apache Flink 0.8.0 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2015/01/06/december-in-flink.html">December 2014 in the Flink community</a></li>

      
        
    </ul>
        <hr>
        <h2>2014</h2>
    <ul id="markdown-toc">
        
      
    
      
      

      
      <li><a href="/news/2014/11/18/hadoop-compatibility.html">Hadoop Compatibility in Flink</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/11/04/release-0.7.0.html">Apache Flink 0.7.0 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/10/03/upcoming_events.html">Upcoming Events</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/09/26/release-0.6.1.html">Apache Flink 0.6.1 available</a></li>

      
        
      
    
      
      

      
      <li><a href="/news/2014/08/26/release-0.6.html">Apache Flink 0.6 available</a></li>

      
    </ul>
      
    
  </div>
</div>
      </div>
    </div>

    <hr />

    <div class="row">
      <div class="footer text-center col-sm-12">
        <p>Copyright © 2014-2019 <a href="http://apache.org">The Apache Software Foundation</a>. All Rights Reserved.</p>
        <p>Apache Flink, Flink®, Apache®, the squirrel logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation.</p>
        <p><a href="/privacy-policy.html">Privacy Policy</a> &middot; <a href="/blog/feed.xml">RSS feed</a></p>
      </div>
    </div>
    </div><!-- /.container -->

    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/js/jquery.matchHeight-min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/codetabs.js"></script>
    <script src="/js/stickysidebar.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>
  </body>
</html>
