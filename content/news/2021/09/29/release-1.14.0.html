<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink: Apache Flink 1.14.0 Release Announcement</title>
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/flink.css">
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Blog RSS feed -->
    <link href="/blog/feed.xml" rel="alternate" type="application/rss+xml" title="Apache Flink Blog: RSS feed" />

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!-- We need to load Jquery in the header for custom google analytics event tracking-->
    <script src="/js/jquery.min.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Matomo -->
    <script>
      var _paq = window._paq = window._paq || [];
      /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
      /* We explicitly disable cookie tracking to avoid privacy issues */
      _paq.push(['disableCookies']);
      /* Measure a visit to flink.apache.org and nightlies.apache.org/flink as the same visit */
      _paq.push(["setDomains", ["*.flink.apache.org","*.nightlies.apache.org/flink"]]);
      _paq.push(['trackPageView']);
      _paq.push(['enableLinkTracking']);
      (function() {
        var u="//matomo.privacy.apache.org/";
        _paq.push(['setTrackerUrl', u+'matomo.php']);
        _paq.push(['setSiteId', '1']);
        var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
        g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
      })();
    </script>
    <!-- End Matomo Code -->
  </head>
  <body>  
    

    <!-- Main content. -->
    <div class="container">
    <div class="row">

      
     <div id="sidebar" class="col-sm-3">
        

<!-- Top navbar. -->
    <nav class="navbar navbar-default">
        <!-- The logo. -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="navbar-logo">
            <a href="/">
              <img alt="Apache Flink" src="/img/flink-header-logo.svg" width="147px" height="73px">
            </a>
          </div>
        </div><!-- /.navbar-header -->

        <!-- The navigation links. -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav navbar-main">

            <!-- First menu section explains visitors what Flink is -->

            <!-- What is Stream Processing? -->
            <!--
            <li><a href="/streamprocessing1.html">What is Stream Processing?</a></li>
            -->

            <!-- What is Flink? -->
            <li><a href="/flink-architecture.html">What is Apache Flink?</a></li>

            

            <!-- Stateful Functions? -->

            <li><a href="https://nightlies.apache.org/flink/flink-statefun-docs-stable/">What is Stateful Functions?</a></li>

            <!-- Flink ML? -->

            <li><a href="https://nightlies.apache.org/flink/flink-ml-docs-stable/">What is Flink ML?</a></li>

            <!-- Use cases -->
            <li><a href="/usecases.html">Use Cases</a></li>

            <!-- Powered by -->
            <li><a href="/poweredby.html">Powered By</a></li>


            &nbsp;
            <!-- Second menu section aims to support Flink users -->

            <!-- Downloads -->
            <li><a href="/downloads.html">Downloads</a></li>

            <!-- Getting Started -->
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Getting Started<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="https://nightlies.apache.org/flink/flink-docs-release-1.14//docs/try-flink/local_installation/" target="_blank">With Flink <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-statefun-docs-release-3.2/getting-started/project-setup.html" target="_blank">With Flink Stateful Functions <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-ml-docs-release-2.0/try-flink-ml/quick-start.html" target="_blank">With Flink ML <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/try-flink-kubernetes-operator/quick-start.html" target="_blank">With Flink Kubernetes Operator <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="/training.html">Training Course</a></li>
              </ul>
            </li>

            <!-- Documentation -->
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#">Documentation<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="https://nightlies.apache.org/flink/flink-docs-release-1.14" target="_blank">Flink 1.14 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-docs-master" target="_blank">Flink Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-statefun-docs-release-3.2" target="_blank">Flink Stateful Functions 3.2 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-statefun-docs-master" target="_blank">Flink Stateful Functions Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-ml-docs-release-2.0" target="_blank">Flink ML 2.0 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-ml-docs-master" target="_blank">Flink ML Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main" target="_blank">Flink Kubernetes Operator 1.0 (Latest stable release) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
                <li><a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main" target="_blank">Flink Kubernetes Operator Master (Latest Snapshot) <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>
              </ul>
            </li>

            <!-- getting help -->
            <li><a href="/gettinghelp.html">Getting Help</a></li>

            <!-- Blog -->
            <li class="active"><a href="/blog/"><b>Flink Blog</b></a></li>


            <!-- Flink-packages -->
            <li>
              <a href="https://flink-packages.org" target="_blank">flink-packages.org <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>
            &nbsp;

            <!-- Third menu section aim to support community and contributors -->

            <!-- Community -->
            <li><a href="/community.html">Community &amp; Project Info</a></li>

            <!-- Roadmap -->
            <li><a href="/roadmap.html">Roadmap</a></li>

            <!-- Contribute -->
            <li><a href="/contributing/how-to-contribute.html">How to Contribute</a></li>
            

            <!-- GitHub -->
            <li>
              <a href="https://github.com/apache/flink" target="_blank">Flink on GitHub <small><span class="glyphicon glyphicon-new-window"></span></small></a>
            </li>

            &nbsp;

            <!-- Language Switcher -->
            <li>
              
                
                  <!-- link to the Chinese home page when current is blog page -->
                  <a href="/zh">中文版</a>
                
              
            </li>

          </ul>

          <style>
            .smalllinks:link {
              display: inline-block !important; background: none; padding-top: 0px; padding-bottom: 0px; padding-right: 0px; min-width: 75px;
            }
          </style>

          <ul class="nav navbar-nav navbar-bottom">
          <hr />

            <!-- Twitter -->
            <li><a href="https://twitter.com/apacheflink" target="_blank">@ApacheFlink <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <!-- Visualizer -->
            <li class=" hidden-md hidden-sm"><a href="/visualizer/" target="_blank">Plan Visualizer <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <li >
                  <a href="/security.html">Flink Security</a>
            </li>

          <hr />

            <li><a href="https://apache.org" target="_blank">Apache Software Foundation <small><span class="glyphicon glyphicon-new-window"></span></small></a></li>

            <li>

              <a class="smalllinks" href="https://www.apache.org/licenses/" target="_blank">License</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/security/" target="_blank">Security</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/foundation/sponsorship.html" target="_blank">Donate</a> <small><span class="glyphicon glyphicon-new-window"></span></small>

              <a class="smalllinks" href="https://www.apache.org/foundation/thanks.html" target="_blank">Thanks</a> <small><span class="glyphicon glyphicon-new-window"></span></small>
            </li>

          </ul>
        </div><!-- /.navbar-collapse -->
    </nav>

      </div>
      <div class="col-sm-9">
      <div class="row-fluid">
  <div class="col-sm-12">
    <div class="row">
      <h1>Apache Flink 1.14.0 Release Announcement</h1>
      <p><i></i></p>

      <article>
        <p>29 Sep 2021 Stephan Ewen (<a href="https://twitter.com/StephanEwen">@StephanEwen</a>) &amp; Johannes Moser (<a href="https://twitter.com/joemoeAT">@joemoeAT</a>)</p>

<p>The Apache Software Foundation recently released its annual report and Apache Flink once again made
it on the list of the top 5 most active projects! This remarkable
activity also shows in the new 1.14.0 release. Once again, more than 200 contributors worked on
over 1,000 issues. We are proud of how this community is consistently moving the project forward.</p>

<p>This release brings many new features and improvements in areas such as the SQL API, more connector support, checkpointing, and PyFlink.
A major area of changes in this release is the integrated streaming &amp; batch experience. We believe
that, in practice, unbounded stream processing goes hand-in-hand with bounded- and batch processing tasks,
because many use cases require processing historic data from various sources alongside streaming data.
Examples are data exploration when developing new applications, bootstrapping state for new applications, training
models to be applied in a streaming application, or re-processing data after fixes/upgrades.</p>

<p>In Flink 1.14, we finally made it possible to <strong>mix bounded and unbounded streams in an application</strong>:
Flink now supports taking checkpoints of applications that are partially running and partially finished (some
operators reached the end of the bounded inputs). Additionally, <strong>bounded streams now take a final checkpoint</strong>
when reaching their end to ensure smooth committing of results in sinks.</p>

<p>The <strong>batch execution mode now supports programs that use a mixture of the DataStream API and the SQL/Table API</strong>
(previously only pure Table/SQL or DataStream programs).</p>

<p>The unified Source and Sink APIs have gotten an update, and we started <strong>consolidating the connector ecosystem around the unified APIs</strong>. We added a new <strong>hybrid source</strong> that can bridge between multiple storage systems.
You can now do things like read old data from Amazon S3 and then switch over to Apache Kafka.</p>

<p>In addition, this release furthers our initiative in making Flink more self-tuning and
easier to operate, without necessarily requiring a lot of Stream-Processor-specific knowledge.
We started this initiative in the previous release with <a href="/news/2021/05/03/release-1.13.0.html#reactive-scaling">reactive scaling</a>
and are now adding <strong>automatic network memory tuning</strong> (<em>a.k.a. Buffer Debloating</em>).
This feature speeds up checkpoints under high load while maintaining high throughput and without
increasing checkpoint size. The mechanism continuously adjusts the network buffers to ensure the best
throughput while having minimal in-flight data. See the <a href="#buffer-debloating">Buffer Debloating section</a>
for more details.</p>

<p>There are many more improvements and new additions throughout various components, as we discuss below.
We also had to say goodbye to some features that have been superceded by newer ones in recent releases,
most prominently we are <strong>removing the old SQL execution engine</strong> and are
<strong>removing the active integration with Apache Mesos</strong>.</p>

<p>We hope you like the new release and we’d be eager to learn about your experience with it, which yet
unsolved problems it solves, what new use-cases it unlocks for you.</p>

<div class="page-toc">
<ul id="markdown-toc">
  <li><a href="#the-unified-batch-and-stream-processing-experience" id="markdown-toc-the-unified-batch-and-stream-processing-experience">The Unified Batch and Stream Processing Experience</a>    <ul>
      <li><a href="#checkpointing-and-bounded-streams" id="markdown-toc-checkpointing-and-bounded-streams">Checkpointing and Bounded Streams</a></li>
      <li><a href="#batch-execution-for-mixed-datastream-and-tablesql-applications" id="markdown-toc-batch-execution-for-mixed-datastream-and-tablesql-applications">Batch Execution for mixed DataStream and Table/SQL Applications</a></li>
      <li><a href="#hybrid-source" id="markdown-toc-hybrid-source">Hybrid Source</a></li>
      <li><a href="#consolidating-sources-and-sink" id="markdown-toc-consolidating-sources-and-sink">Consolidating Sources and Sink</a></li>
    </ul>
  </li>
  <li><a href="#improvements-to-operations" id="markdown-toc-improvements-to-operations">Improvements to Operations</a>    <ul>
      <li><a href="#buffer-debloating" id="markdown-toc-buffer-debloating">Buffer debloating</a></li>
      <li><a href="#fine-grained-resource-management" id="markdown-toc-fine-grained-resource-management">Fine-grained Resource Management</a></li>
    </ul>
  </li>
  <li><a href="#connectors" id="markdown-toc-connectors">Connectors</a>    <ul>
      <li><a href="#connector-metrics" id="markdown-toc-connector-metrics">Connector Metrics</a></li>
      <li><a href="#pulsar-connector" id="markdown-toc-pulsar-connector">Pulsar Connector</a></li>
    </ul>
  </li>
  <li><a href="#pyflink" id="markdown-toc-pyflink">PyFlink</a>    <ul>
      <li><a href="#performance-improvements-through-chaining" id="markdown-toc-performance-improvements-through-chaining">Performance Improvements through Chaining</a></li>
      <li><a href="#loopback-mode-for-debugging" id="markdown-toc-loopback-mode-for-debugging">Loopback Mode for Debugging</a></li>
      <li><a href="#miscellaneous-improvements" id="markdown-toc-miscellaneous-improvements">Miscellaneous Improvements</a></li>
    </ul>
  </li>
  <li><a href="#goodbye-legacy-sql-engine-and-mesos-support" id="markdown-toc-goodbye-legacy-sql-engine-and-mesos-support">Goodbye Legacy SQL Engine and Mesos Support</a></li>
  <li><a href="#upgrade-notes" id="markdown-toc-upgrade-notes">Upgrade Notes</a></li>
  <li><a href="#list-of-contributors" id="markdown-toc-list-of-contributors">List of Contributors</a></li>
</ul>

</div>

<h1 id="the-unified-batch-and-stream-processing-experience">The Unified Batch and Stream Processing Experience</h1>

<p>One of Flink’s unique characteristics is how it integrates stream- and batch processing,
using unified APIs and a runtime that supports multiple execution paradigms.</p>

<p>As motivated in the introduction, we believe that stream- and batch processing always go hand in hand. This quote from
a <a href="https://research.fb.com/wp-content/uploads/2016/11/realtime_data_processing_at_facebook.pdf">report on Facebook’s streaming infrastructure</a>
echos this sentiment nicely.</p>

<blockquote>
  <p>Streaming versus batch processing is not an either/or decision. Originally, all data warehouse
processing at Facebook was batch processing. We began developing Puma and Swift about five years
ago. As we showed in Section […], using a mix of streaming and batch processing can speed up
long pipelines by hours.</p>
</blockquote>

<p>Having both the real-time and the historic computations in the same engine also ensures consistency
between semantics and makes results well comparable. Here is an <a href="https://www.ververica.com/blog/apache-flinks-stream-batch-unification-powers-alibabas-11.11-in-2020">article by Alibaba</a>
about unifying business reporting with Apache Flink and getting consistent reports that way.</p>

<p>While unified streaming &amp; batch are already possible in earlier versions, this release brings
some features that unlock new use cases, as well as a series of quality-of-life improvements.</p>

<h2 id="checkpointing-and-bounded-streams">Checkpointing and Bounded Streams</h2>

<p>Flink’s checkpointing mechanism could originally only create checkpoints when all tasks in an application’s
DAG were running. This meant that applications using both bounded and unbounded data sources were not really possible.
In addition, applications on bounded inputs that were executed in a streaming way (not in a batch way)
stopped checkpointing towards the end of the processing, when some tasks finished. Without checkpoints, the
latest output data was not committed, resulting in lingering data for exactly-once sinks.</p>

<p>With <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-147%3A+Support+Checkpoints+After+Tasks+Finished">FLIP-147</a>
Flink now supports checkpoints after tasks are finished, and takes a final checkpoint at the end of a
bounded stream, ensuring that all sink data is committed before the job ends (similar to how
<em>stop-with-savepoint</em> behaves).</p>

<p>To activate this feature, add <code>execution.checkpointing.checkpoints-after-tasks-finish.enabled: true</code>
to your configuration. Keeping with the opt-in tradition for big and new features,
this is not activated by default in Flink 1.14. We expect it to become the default mode in the next release.</p>

<p>Background: While the batch execution mode is often the preferrable way to run applications over bounded streams,
there are various reasons to use streaming execution mode over bounded streams. For example, the sink being used
might only support streaming execution (i.e. Kafka sink) or you may want to exploit the streaming-inherent
quasi-ordering-by-time in your application, such as motivated by the <a href="https://youtu.be/4qSlsYogALo?t=666">Kappa+ Architecture</a>.</p>

<h2 id="batch-execution-for-mixed-datastream-and-tablesql-applications">Batch Execution for mixed DataStream and Table/SQL Applications</h2>

<p>SQL and the Table API are becoming the default starting points for new projects. The declarative
nature and richness of built-in types and operations make it easy to develop applications fast.
It is not uncommon, however, for developers to eventually hit the limit of SQL’s expressiveness for
certain types of event-driven business logic (or hit the point when it becomes grotesque to express
that logic in SQL).</p>

<p>At that point, the natural step is to blend in a piece of stateful DataStream API logic, before
switching back to SQL again.</p>

<p>In Flink 1.14, bounded batch-executed SQL/Table programs can convert their intermediate
Tables to a DataStream, apply some DataSteam API operations, and convert it back to a Table.
Under the hood, Flink builds a dataflow DAG mixing declarative optimized SQL execution with batch-executed DataStream logic.
Check out the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/table/data_stream_api/#converting-between-datastream-and-table">documentation</a> for details.</p>

<h2 id="hybrid-source">Hybrid Source</h2>

<p>The new <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/datastream/hybridsource/">Hybrid Source</a>
produces a combined stream from multiple sources, by reading those sources one after the other,
seamlessly switching over from one source to the other.</p>

<p>The motivating use case for the Hybrid Source was to read streams from tiered storage setups as if there was one
stream that spans all tiers. For example, new data may land in Kafka and is eventually
migrated to S3 (typically in compressed columnar format, for cost efficiency and performance).
The Hybrid Source can read this as one contiguous logical stream, starting with the historic data on S3
and transitioning over to the more recent data in Kafka.</p>

<figure style="align-content: center">
  <img src="/img/blog/2021-09-25-release-1.14.0/hybrid_source.png" style="display: block; margin-left: auto; margin-right: auto; width: 600px" />
</figure>

<p>We believe that this is an exciting step in realizing the full promise of logs and the <em>Kappa Architecture.</em>
Even if older parts of an event log are physically migrated to different storage
(for reasons such as cost, better compression, faster reads) you can still treat and process it as one
contiguous log.</p>

<p>Flink 1.14 adds the core functionality of the Hybrid Source. Over the next releases, we expect to add more
utilities and patterns for typical switching strategies.</p>

<h2 id="consolidating-sources-and-sink">Consolidating Sources and Sink</h2>

<p>With the new unified (streaming/batch) source and sink APIs now being stable, we started the
big effort to consolidate all connectors around those APIs. At the same time, we are
better aligning connectors between DataStream and SQL/Table API. First are the <em>Kafka</em> and
<em>File</em> Soures and Sinks for the DataStream API.</p>

<p>The result of this effort (that we expect to span at least 1-2 futher releases) will be a much
smoother and more consistent experience for Flink users when connecting to external systems.</p>

<h1 id="improvements-to-operations">Improvements to Operations</h1>

<h2 id="buffer-debloating">Buffer debloating</h2>

<p><em>Buffer Debloating</em> is a new technology in Flink that minimizes checkpoint latency and cost.
It does so by automatically tuning the usage of network memory to ensure high throughput,
while minimizing the amount of in-flight data.</p>

<p>Apache Flink buffers a certain amount of data in its network stack to be able to utilize the
bandwidth of fast networks. A Flink application running with high throughput uses some (or
all) of that memory. Aligned checkpoints flow with the data through the network buffers in milliseconds.</p>

<p>During (temporary) backpressure from a resource bottleneck such as an external system, data skew, or (temporarily)
increased load, Flink was buffering a lot more data inside its network buffers than necessary to utilize
enough network bandwidth for the application’s current – backpressured – throughput. This actually has an adverse
effect because more buffered data means that the checkpoints need to do more work. Aligned checkpoint barriers
need to wait for more data to be processed, unaligned checkpoints need to persist more in-flight data.</p>

<p>This is where <em>Buffer Debloating</em> comes into play: It changes the network stack from keeping up to X bytes of data
to keeping data that is worth X milliseconds of receiver computing time. With the default setting
of 1000 milliseconds, that means the network stack will buffer as much data as the receiving task can
process in 1000 milliseconds. These values are constantly measured and adjusted, so the system keeps
this characteristic even under varying conditions. As a result, Flink can now provide
stable and predictable alignment times for aligned checkpoints under backpressure, and can vastly
reduce the amount of in-flight data stored in unaliged checkpoints under backpressure.</p>

<figure style="align-content: center">
  <img src="/img/blog/2021-09-25-release-1.14.0/buffer_debloating.svg" style="display: block; margin-left: auto; margin-right: auto; width: 600px" />
</figure>

<p>Buffer Debloating acts as a complementary feature, or even alternative, to unaligned checkpoints.
Checkout the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/deployment/memory/network_mem_tuning/#the-buffer-debloating-mechanism">documentation</a>
to see how to activate this feature.</p>

<h2 id="fine-grained-resource-management">Fine-grained Resource Management</h2>

<p><em>Fine-grained resource management</em> is an advanced new feature that increases the resource
utilization of large shared clusters.</p>

<p>Flink clusters execute various data processing workloads. Different data processing steps typically need
different resources such as compute resources and memory. For example, most <code>map()</code> functions are fairly
lightweight, but large windows with long retention can benefit from lots of memory.
By default, Flink manages resources in coarse-grained units called <em>slots</em>, which are slices
of a TaskManager’s resources. Streaming pipelines fill a slot with one parallel
subtask of each operator, so each slot holds a pipeline of subtasks.
Through <em>‘slot sharing groups’</em>, users can influence how subtasks are assigned to slots.</p>

<p>With fine-grained resource management, TaskManager slots can now be dynamically sized.
Transformations and operators specify what resource profiles they would like (CPU size,
memory pools, disk space) and Flink’s Resource Manager and TaskManagers slice off that specific
part of a TaskManager’s total resources. You can think of it as a minimal lightweight resource orchestration
layer within Flink. The figure below illustrates the difference between the current default mode of shared
fixed-size slots and the new fine-grained resource management feature.</p>

<figure style="align-content: center">
  <img src="/img/blog/2021-09-25-release-1.14.0/fine_grained_resource_management.svg" style="display: block; margin-left: auto; margin-right: auto; width: 600px" />
</figure>

<p>You may be wondering why we implement such a feature in Flink, when we also integrate with full-fledged
resource orchestration frameworks like Kubernetes or YARN. There are several situations where the additional
resource management layer within Flink significantly increases the resource utilization:</p>

<ul>
  <li>For many small slots, the overhead of dedicated TaskManagers is very high (JVM overhead, Flink control data structures).
Slot-sharing implicitly works around this by sharing the slots between all operator types, which means
sharing resources between lightweight operators (which need small slots) and heavyweight operators (which need large slots).
However, this only works well when all operators share the same parallelism, which is not aways optimal.
Furthermore, certain operators work better when run in isolation (for example ML training operators
that need dedicated GPU resources).</li>
  <li>Kubernetes and YARN often take quite some time to fulfill requests, especially on loaded clusters.
For many batch jobs, efficiency gets lost while waiting for the requests to be fulfilled.</li>
</ul>

<p>So when should you use this feature? For most streaming and batch jobs the default resource management mechanism
are perfectly suitable. Fine-grained resourced management can help you increase resource efficiency if you have either long-running
streaming jobs, or fast batch jobs, where different stages have different resource requirements, and you may
have already tuned the parallelism of different operators to different values.</p>

<p>Alibaba’s internal Flink-based platform has used this mechanism for some time now and the resource utilization
of the cluster has improved significantly.</p>

<p>Please refer to the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/deployment/finegrained_resource/">Fine-grained Resource Management documentation</a>
for details on how to use this feature.</p>

<h1 id="connectors">Connectors</h1>

<h2 id="connector-metrics">Connector Metrics</h2>

<p>Metrics for connectors have been standardized in this release (see <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-33%3A+Standardize+Connector+Metrics">FLIP-33</a>).
The community will gradually pull metrics through all connectors, as we rework them
onto the new unified APIs over the next releases. In Flink 1.14, we cover the Kafka connector
and (partially) the FileSystem connectors.</p>

<p>Connectors are the entry and exit points for data in a Flink job. If a job is not running as
expected, the connector telemetry is among the first parts to be checked. We believe this will become
a nice improvement when operating Flink applications in production.</p>

<h2 id="pulsar-connector">Pulsar Connector</h2>

<p>In this release, Flink added the <a href="https://pulsar.apache.org/">Apache Pulsar</a> connector.
The Pulsar connector reads data from Pulsar topics and supports both streaming and batch execution modes.
With the support of the transaction functionality (introduced in Pulsar 2.8.0), the Pulsar connector provides
exactly-once delivery semantic to ensure that a message is delivered exactly once to a consumer,
even if a producer retries sending that message.</p>

<p>To support the different message-ordering and scaling requirements of different use cases, the Pulsar
source connector exposes four subscription types:
  - <a href="https://pulsar.apache.org/docs/en/concepts-messaging/#exclusive">Exclusive</a>
  - <a href="https://pulsar.apache.org/docs/en/concepts-messaging/#shared">Shared</a>
  - <a href="https://pulsar.apache.org/docs/en/concepts-messaging/#failover">Failover</a>
  - <a href="https://pulsar.apache.org/docs/en/concepts-messaging/#key_shared">Key-Shared</a></p>

<p>The connector currently supports the DataStream API. Table API/SQL bindings are expected to be
contributed in a future release. For details about how to use the Pulsar connector, see
<a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/datastream/pulsar/#apache-pulsar-connector">Apache Pulsar Connector</a>.</p>

<h1 id="pyflink">PyFlink</h1>

<h2 id="performance-improvements-through-chaining">Performance Improvements through Chaining</h2>

<p>Similar to how the Java APIs chain transformation functions/operators within a task to avoid
serialization overhead, PyFlink now chains Python functions. In PyFlink’s case, the
chaining not only eliminates serialization overhead, but also reduces RPC round trips
between the Java and Python processes. This provides a significant
boost to PyFlink’s overall performance.</p>

<p>Python function chaining was already available for Python UDFs used in the Table API &amp; SQL.
In Flink 1.14, chaining is also exploited for the cPython functions in Python DataStream API.</p>

<h2 id="loopback-mode-for-debugging">Loopback Mode for Debugging</h2>

<p>Python functions are normally executed in a separate Python process next to Flink’s JVM.
This architecture makes it difficult to debug Python code.</p>

<p>PyFlink 1.14 introduces a <em>loopback mode</em>, which is activated by default for local deployments.
In this mode, user-defined Python functions will be executed in the Python process of the client,
which is the entry point process that starts the PyFlink program and contains the DataStream API and
Table API code that builds the dataflow DAG. Users can now easily debug their Python functions
by setting breakpoints in their IDEs when launching a PyFlink job locally.</p>

<h2 id="miscellaneous-improvements">Miscellaneous Improvements</h2>

<p>There are also many other improvements to PyFlink, such as support for executing
jobs in YARN application mode and support for compressed tgz files as Python archives.
Check out the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/python/overview/">Python API documentation</a> 
for more details.</p>

<h1 id="goodbye-legacy-sql-engine-and-mesos-support">Goodbye Legacy SQL Engine and Mesos Support</h1>

<p>Maintaining an open source project also means sometimes saying good-bye to some beloved features.</p>

<p>When we added the Blink SQL Engine to Flink more than two years ago, it was clear that it would
eventually replace the previous SQL engine. Blink was faster and more feature-complete.
For a year now, Blink has been the default SQL engine. With Flink 1.14 we finally remove all
code from the previous SQL engine. This allowed us to drop many outdated interfaces and reduce
confusion for users about which interfaces to use when implementing custom connectors or functions.
It will also help us in the future to make faster changes to the SQL engine.</p>

<p>The active integration with Apache Mesos was also removed, because we saw little interest by
users in this feature and we could not gather enough contributors willing to help maintaining this
part of the system. Flink 1.14 can no longer run on Mesos without the help of projects like Marathon,
and the Flink Resource Manager can no longer request and release resources from Mesos for workloads
with changing resource requirements.</p>

<h1 id="upgrade-notes">Upgrade Notes</h1>

<p>While we aim to make upgrades as smooth as possible, some of the changes require users
to adjust some parts of the program when upgrading to Apache Flink 1.14.
Please take a look at the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/release-notes/flink-1.14/">release notes</a>
for a list of adjustments to make and issues to check during upgrades.</p>

<h1 id="list-of-contributors">List of Contributors</h1>

<p>The Apache Flink community would like to thank each one of the contributors that have made this
release possible:</p>

<p>adavis9592, Ada Wong, aidenma, Aitozi, Ankush Khanna, anton, Anton Kalashnikov, Arvid Heise, Ashwin
Kolhatkar, Authuir, bgeng777, Brian Zhou, camile.sing, caoyingjie, Cemre Mengu, chennuo, Chesnay
Schepler, chuixue, CodeCooker17, comsir, Daisy T, Danny Cranmer, David Anderson, David Moravek,
Dawid Wysakowicz, dbgp2021, Dian Fu, Dong Lin, Edmondsky, Elphas Toringepi, Emre Kartoglu, ericliuk,
Eron Wright, est08zw, Etienne Chauchot, Fabian Paul, fangliang, fangyue1, fengli, Francesco
Guardiani, FuyaoLi2017, fuyli, Gabor Somogyi, gaoyajun02, Gen Luo, gentlewangyu, GitHub, godfrey he,
godfreyhe, gongzhongqiang, Guokuai Huang, GuoWei Ma, Gyula Fora, hackergin, hameizi, Hang Ruan, Han
Wei, hapihu, hehuiyuan, hstdream, Huachao Mao, HuangXiao, huangxingbo, huxixiang, Ingo Bürk,
Jacklee, Jan Brusch, Jane, Jane Chan, Jark Wu, JasonLee, Jiajie Zhong, Jiangjie (Becket) Qin,
Jianzhang Chen, Jiayi Liao, Jing, Jingsong Lee, JingsongLi, Jing Zhang, jinxing64, junfan.zhang, Jun
Qin, Jun Zhang, kanata163, Kevin Bohinski, kevin.cyj, Kevin Fan, Kurt Young, kylewang, Lars
Bachmann, lbb, LB Yu, LB-Yu, LeeJiangchuan, Leeviiii, leiyanfei, Leonard Xu, LightGHLi, Lijie Wang,
liliwei, lincoln lee, Linyu, liuyanpunk, lixiaobao14, luoyuxia, Lyn Zhang, lys0716, MaChengLong,
mans2singh, Marios Trivyzas, martijnvisser, Matthias Pohl, Mayi, mayue.fight, Michael Li, Michal
Ciesielczyk, Mika, Mika Naylor, MikuSugar, movesan, Mulan, Nico Kruber, Nicolas Raga, Nicolaus
Weidner, paul8263, Paul Lin, pierre xiong, Piotr Nowojski, Qingsheng Ren, Rainie Li, Robert Metzger,
Roc Marshal, Roman, Roman Khachatryan, Rui Li, sammieliu, sasukerui, Senbin Lin, Senhong Liu, Serhat
Soydan, Seth Wiesman, sharkdtu, Shengkai, Shen Zhu, shizhengchao, Shuo Cheng, shuo.cs, simenliuxing,
sjwiesman, Srinivasulu Punuru, Stefan Gloutnikov, SteNicholas, Stephan Ewen, sujun, sv3ndk, Svend
Vanderveken, syhily, Tartarus0zm, Terry Wang, Thesharing, Thomas Weise, tiegen, Till Rohrmann, Timo
Walther, tison, Tony Wei, trushev, tsreaper, TsReaper, Tzu-Li (Gordon) Tai, wangfeifan, wangwei1025,
wangxianghu, wangyang0918, weizheng92, Wenhao Ji, Wenlong Lyu, wenqiao, WilliamSong11, wuren,
wysstartgo, Xintong Song, yanchenyun, yangminghua, yangqu, Yang Wang, Yangyang ZHANG, Yangze Guo,
Yao Zhang, yfhanfei, yiksanchan, Yik San Chan, Yi Tang, yljee, Youngwoo Kim, Yuan Mei, Yubin Li,
Yufan Sheng, yulei0824, Yun Gao, Yun Tang, yuxia Luo, Zakelly, zhang chaoming, zhangjunfan,
zhangmang, zhangzhengqi3, zhao_wei_nan, zhaown, zhaoxing, ZhiJie Yang, Zhilong Hong, Zhiwen Sun, Zhu
Zhu, zlzhang0122, zoran, Zor X. LIU, zoucao, Zsombor Chikan, 子扬, 莫辞</p>

      </article>
    </div>

    <div class="row">
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'stratosphere-eu'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
             (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
    </div>
  </div>
</div>
      </div>
    </div>

    <hr />

    <div class="row">
      <div class="footer text-center col-sm-12">
        <p>Copyright © 2014-2022 <a href="http://apache.org">The Apache Software Foundation</a>. All Rights Reserved.</p>
        <p>Apache Flink, Flink®, Apache®, the squirrel logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation.</p>
        <p><a href="/privacy-policy.html">Privacy Policy</a> &middot; <a href="/blog/feed.xml">RSS feed</a></p>
      </div>
    </div>
    </div><!-- /.container -->

    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/js/jquery.matchHeight-min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/codetabs.js"></script>
    <script src="/js/stickysidebar.js"></script>
  </body>
</html>
