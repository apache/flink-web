<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Flink Documentation on Apache Flink</title>
    <link>https://flink.apache.org/zh/</link>
    <description>Recent content in Apache Flink Documentation on Apache Flink</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language><atom:link href="https://flink.apache.org/zh/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flink $FlinkStableShortVersion (stable)</title>
      <link>https://flink.apache.org/zh/documentation/flink-stable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/flink-stable/</guid>
      <description> Flink documentation (latest stable release) # You can find the Flink documentation for the latest stable release here. </description>
    </item>
    
    <item>
      <title>With Flink</title>
      <link>https://flink.apache.org/zh/getting-started/with-flink/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/getting-started/with-flink/</guid>
      <description> Getting Started with Flink # Read how you can get started with Flink here. </description>
    </item>
    
    <item>
      <title>架构</title>
      <link>https://flink.apache.org/zh/what-is-flink/flink-architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink/flink-architecture/</guid>
      <description>Apache Flink 是什么？ # Apache Flink 是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。
接下来，我们来介绍一下 Flink 架构中的重要方面。
处理无界和有界数据 # 任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。
数据可以被作为 无界 或者 有界 流来处理。
无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。
有界流 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理
Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。
通过探索 Flink 之上构建的 用例 来加深理解。
部署应用到任意地方 # Apache Flink 是一个分布式系统，它需要计算资源来执行应用程序。Flink 集成了所有常见的集群资源管理器，例如 Hadoop YARN、 Apache Mesos 和 Kubernetes，但同时也可以作为独立集群运行。
Flink 被设计为能够很好地工作在上述每个资源管理器中，这是通过资源管理器特定(resource-manager-specific)的部署模式实现的。Flink 可以采用与当前资源管理器相适应的方式进行交互。
部署 Flink 应用程序时，Flink 会根据应用程序配置的并行性自动标识所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink 通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成。
运行任意规模应用 # Flink 旨在任意规模上运行有状态流式应用。因此，应用程序被并行化为可能数千个任务，这些任务分布在集群中并发执行。所以应用程序能够充分利用无尽的 CPU、内存、磁盘和网络 IO。而且 Flink 很容易维护非常大的应用程序状态。其异步和增量的检查点算法对处理延迟产生最小的影响，同时保证精确一次状态的一致性。
Flink 用户报告了其生产环境中一些令人印象深刻的扩展性数字
处理每天处理数万亿的事件, 应用维护几TB大小的状态, 和 应用在数千个内核上运行。 利用内存性能 # 有状态的 Flink 程序针对本地状态访问进行了优化。任务的状态始终保留在内存中，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。</description>
    </item>
    
    <item>
      <title>Flink Master (snapshot)</title>
      <link>https://flink.apache.org/zh/documentation/flink-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/flink-master/</guid>
      <description> Flink documentation (latest snapshot) # You can find the Flink documentation for the latest snapshot here. </description>
    </item>
    
    <item>
      <title>With Flink Stateful Functions</title>
      <link>https://flink.apache.org/zh/getting-started/with-flink-stateful-functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/getting-started/with-flink-stateful-functions/</guid>
      <description> Getting Started with Flink Stateful Functions # Read how you can get started with Flink Stateful Functions here. </description>
    </item>
    
    <item>
      <title>应用</title>
      <link>https://flink.apache.org/zh/what-is-flink/flink-applications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink/flink-applications/</guid>
      <description>Apache Flink 是什么？ # Apache Flink 是一个针对无界和有界数据流进行有状态计算的框架。Flink 自底向上在不同的抽象级别提供了多种 API，并且针对常见的使用场景开发了专用的扩展库。
在本章中，我们将介绍 Flink 所提供的这些简单易用、易于表达的 API 和库。
流处理应用的基本组件 # 可以由流处理框架构建和执行的应用程序类型是由框架对 流、状态、时间 的支持程度来决定的。在下文中，我们将对上述这些流处理应用的基本组件逐一进行描述，并对 Flink 处理它们的方法进行细致剖析。
流 # 显而易见，（数据）流是流处理的基本要素。然而，流也拥有着多种特征。这些特征决定了流如何以及何时被处理。Flink 是一个能够处理任何类型数据流的强大处理框架。
有界 和 无界 的数据流：流可以是无界的；也可以是有界的，例如固定大小的数据集。Flink 在无界的数据流处理上拥有诸多功能强大的特性，同时也针对有界的数据流开发了专用的高效算子。 实时 和 历史记录 的数据流：所有的数据都是以流的方式产生，但用户通常会使用两种截然不同的方法处理数据。或是在数据生成时进行实时的处理；亦或是先将数据流持久化到存储系统中——例如文件系统或对象存储，然后再进行批处理。Flink 的应用能够同时支持处理实时以及历史记录数据流。 状态 # 只有在每一个单独的事件上进行转换操作的应用才不需要状态，换言之，每一个具有一定复杂度的流处理应用都是有状态的。任何运行基本业务逻辑的流处理应用都需要在一定时间内存储所接收的事件或中间结果，以供后续的某个时间点（例如收到下一个事件或者经过一段特定时间）进行访问并进行后续处理。
应用状态是 Flink 中的一等公民，Flink 提供了许多状态管理相关的特性支持，其中包括：
多种状态基础类型：Flink 为多种不同的数据结构提供了相对应的状态基础类型，例如原子值（value），列表（list）以及映射（map）。开发者可以基于处理函数对状态的访问方式，选择最高效、最适合的状态基础类型。 插件化的State Backend：State Backend 负责管理应用程序状态，并在需要的时候进行 checkpoint。Flink 支持多种 state backend，可以将状态存在内存或者 RocksDB。RocksDB 是一种高效的嵌入式、持久化键值存储引擎。Flink 也支持插件式的自定义 state backend 进行状态存储。 精确一次语义：Flink 的 checkpoint 和故障恢复算法保证了故障发生后应用状态的一致性。因此，Flink 能够在应用程序发生故障时，对应用程序透明，不造成正确性的影响。 超大数据量状态：Flink 能够利用其异步以及增量式的 checkpoint 算法，存储数 TB 级别的应用状态。 可弹性伸缩的应用：Flink 能够通过在更多或更少的工作节点上对状态进行重新分布，支持有状态应用的分布式的横向伸缩。 时间 # 时间是流处理应用另一个重要的组成部分。因为事件总是在特定时间点发生，所以大多数的事件流都拥有事件本身所固有的时间语义。进一步而言，许多常见的流计算都基于时间语义，例如窗口聚合、会话计算、模式检测和基于时间的 join。流处理的一个重要方面是应用程序如何衡量时间，即区分事件时间（event-time）和处理时间（processing-time）。</description>
    </item>
    
    <item>
      <title>With Flink ML</title>
      <link>https://flink.apache.org/zh/getting-started/with-flink-ml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/getting-started/with-flink-ml/</guid>
      <description> Getting Started with Flink ML # Read how you can get started with Flink ML here. </description>
    </item>
    
    <item>
      <title>运维</title>
      <link>https://flink.apache.org/zh/what-is-flink/flink-operations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink/flink-operations/</guid>
      <description>Apache Flink 是什么？ # Apache Flink 是一个针对无界和有界数据流进行有状态计算的框架。由于许多流应用程序旨在以最短的停机时间连续运行，因此流处理器必须提供出色的故障恢复能力，以及在应用程序运行期间进行监控和维护的工具。
Apache Flink 非常注重流数据处理的可运维性。因此在这一小节中，我们将详细介绍 Flink 的故障恢复机制，并介绍其管理和监控应用的功能。
7 * 24小时稳定运行 # 在分布式系统中，服务故障是常有的事，为了保证服务能够7*24小时稳定运行，像Flink这样的流处理器故障恢复机制是必须要有的。显然这就意味着，它(这类流处理器)不仅要能在服务出现故障时候能够重启服务，而且还要当故障发生时，保证能够持久化服务内部各个组件的当前状态，只有这样才能保证在故障恢复时候，服务能够继续正常运行，好像故障就没有发生过一样。
Flink通过几下多种机制维护应用可持续运行及其一致性:
检查点的一致性: Flink的故障恢复机制是通过建立分布式应用服务状态一致性检查点实现的，当有故障产生时，应用服务会重启后，再重新加载上一次成功备份的状态检查点信息。结合可重放的数据源，该特性可保证*精确一次（exactly-once）*的状态一致性。 高效的检查点: 如果一个应用要维护一个TB级的状态信息，对此应用的状态建立检查点服务的资源开销是很高的，为了减小因检查点服务对应用的延迟性（SLAs服务等级协议）的影响，Flink采用异步及增量的方式构建检查点服务。 端到端的精确一次: Flink 为某些特定的存储支持了事务型输出的功能，及时在发生故障的情况下，也能够保证精确一次的输出。 集成多种集群管理服务: Flink已与多种集群管理服务紧密集成，如 Hadoop YARN, Mesos, 以及 Kubernetes。当集群中某个流程任务失败后，一个新的流程服务会自动启动并替代它继续执行。 内置高可用服务: Flink内置了为解决单点故障问题的高可用性服务模块，此模块是基于Apache ZooKeeper 技术实现的，Apache ZooKeeper是一种可靠的、交互式的、分布式协调服务组件。 Flink能够更方便地升级、迁移、暂停、恢复应用服务 # 驱动关键业务服务的流应用是经常需要维护的。比如需要修复系统漏洞，改进功能，或开发新功能。然而升级一个有状态的流应用并不是简单的事情，因为在我们为了升级一个改进后版本而简单停止当前流应用并重启时，我们还不能丢失掉当前流应用的所处于的状态信息。
而Flink的 Savepoint 服务就是为解决升级服务过程中记录流应用状态信息及其相关难题而产生的一种唯一的、强大的组件。一个 Savepoint，就是一个应用服务状态的一致性快照，因此其与checkpoint组件的很相似，但是与checkpoint相比，Savepoint 需要手动触发启动，而且当流应用服务停止时，它并不会自动删除。Savepoint 常被应用于启动一个已含有状态的流服务，并初始化其（备份时）状态。Savepoint 有以下特点：
便于升级应用服务版本: Savepoint 常在应用版本升级时使用，当前应用的新版本更新升级时，可以根据上一个版本程序记录的 Savepoint 内的服务状态信息来重启服务。它也可能会使用更早的 Savepoint 还原点来重启服务，以便于修复由于有缺陷的程序版本导致的不正确的程序运行结果。 方便集群服务移植: 通过使用 Savepoint，流服务应用可以自由的在不同集群中迁移部署。 方便Flink版本升级: 通过使用 Savepoint，可以使应用服务在升级Flink时，更加安全便捷。 增加应用并行服务的扩展性: Savepoint 也常在增加或减少应用服务集群的并行度时使用。 便于A/B测试及假设分析场景对比结果: 通过把同一应用在使用不同版本的应用程序，基于同一个 Savepoint 还原点启动服务时，可以测试对比2个或多个版本程序的性能及服务质量。 暂停和恢复服务: 一个应用服务可以在新建一个 Savepoint 后再停止服务，以便于后面任何时间点再根据这个实时刷新的 Savepoint 还原点进行恢复服务。 归档服务: Savepoint 还提供还原点的归档服务，以便于用户能够指定时间点的 Savepoint 的服务数据进行重置应用服务的状态，进行恢复服务。 监控和控制应用服务 # 如其它应用服务一样，持续运行的流应用服务也需要监控及集成到一些基础设施资源管理服务中，例如一个组件的监控服务及日志服务等。监控服务有助于预测问题并提前做出反应，日志服务提供日志记录能够帮助追踪、调查、分析故障发生的根本原因。最后，便捷易用的访问控制应用服务运行的接口也是Flink的一个重要的亮点特征。</description>
    </item>
    
    <item>
      <title>Stateful Functions $StateFunStableShortVersion (stable)</title>
      <link>https://flink.apache.org/zh/documentation/flink-stateful-functions-stable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/flink-stateful-functions-stable/</guid>
      <description> Flink documentation (latest stable release) # You can find the Flink documentation for the latest stable release here. </description>
    </item>
    
    <item>
      <title>Stateful Functions Master (snapshot)</title>
      <link>https://flink.apache.org/zh/documentation/flink-stateful-functions-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/flink-stateful-functions-master/</guid>
      <description> Flink Stateful Functions documentation (latest snapshot) # You can find the Flink Stateful Functions documentation for the latest snapshot here. </description>
    </item>
    
    <item>
      <title>With Flink Kubernetes Operator</title>
      <link>https://flink.apache.org/zh/getting-started/with-flink-kubernetes-operator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/getting-started/with-flink-kubernetes-operator/</guid>
      <description> Getting Started with Flink Kubernetes Operator # Read how you can get started with Flink Kubernetes Operator here. </description>
    </item>
    
    <item>
      <title>应用场景</title>
      <link>https://flink.apache.org/zh/what-is-flink/use-cases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink/use-cases/</guid>
      <description>应用场景 # Apache Flink 功能强大，支持开发和运行多种不同种类的应用程序。它的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。Flink 不仅可以运行在包括 YARN、 Mesos、Kubernetes 在内的多种资源管理框架上，还支持在裸机集群上独立部署。在启用高可用选项的情况下，它不存在单点失效问题。事实证明，Flink 已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。世界各地有很多要求严苛的流处理应用都运行在 Flink 之上。
接下来我们将介绍 Flink 常见的几类应用并给出相关实例链接。
事件驱动型应用 数据分析应用 数据管道应用 事件驱动型应用 # 什么是事件驱动型应用？ # 事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。
事件驱动型应用是在计算存储分离的传统应用基础上进化而来。在传统架构中，应用需要读写远程事务型数据库。
相反，事件驱动型应用是基于状态化流处理来完成。在该设计中，数据和计算不会分离，应用只需访问本地（内存或磁盘）即可获取数据。系统容错性的实现依赖于定期向远程持久化存储写入 checkpoint。下图描述了传统应用和事件驱动型应用架构的区别。
事件驱动型应用的优势？ # 事件驱动型应用无须查询远程数据库，本地数据访问使得它具有更高的吞吐和更低的延迟。而由于定期向远程持久化存储的 checkpoint 工作可以异步、增量式完成，因此对于正常事件处理的影响甚微。事件驱动型应用的优势不仅限于本地数据访问。传统分层架构下，通常多个应用会共享同一个数据库，因而任何对数据库自身的更改（例如：由应用更新或服务扩容导致数据布局发生改变）都需要谨慎协调。反观事件驱动型应用，由于只需考虑自身数据，因此在更改数据表示或服务扩容时所需的协调工作将大大减少。
Flink 如何支持事件驱动型应用？ # 事件驱动型应用会受制于底层流处理系统对时间和状态的把控能力，Flink 诸多优秀特质都是围绕这些方面来设计的。它提供了一系列丰富的状态操作原语，允许以精确一次的一致性语义合并海量规模（TB 级别）的状态数据。此外，Flink 还支持事件时间和自由度极高的定制化窗口逻辑，而且它内置的 ProcessFunction 支持细粒度时间控制，方便实现一些高级业务逻辑。同时，Flink 还拥有一个复杂事件处理（CEP）类库，可以用来检测数据流中的模式。
Flink 中针对事件驱动应用的明星特性当属 savepoint。Savepoint 是一个一致性的状态映像，它可以用来初始化任意状态兼容的应用。在完成一次 savepoint 后，即可放心对应用升级或扩容，还可以启动多个版本的应用来完成 A/B 测试。
典型的事件驱动型应用实例 # 反欺诈 异常检测 基于规则的报警 业务流程监控 （社交网络）Web 应用 数据分析应用 # 什么是数据分析应用？ # 数据分析任务需要从原始数据中提取有价值的信息和指标。传统的分析方式通常是利用批查询，或将事件记录下来并基于此有限数据集构建应用来完成。为了得到最新数据的分析结果，必须先将它们加入分析数据集并重新执行查询或运行应用，随后将结果写入存储系统或生成报告。
借助一些先进的流处理引擎，还可以实时地进行数据分析。和传统模式下读取有限数据集不同，流式查询或应用会接入实时事件流，并随着事件消费持续产生和更新结果。这些结果数据可能会写入外部数据库系统或以内部状态的形式维护。仪表展示应用可以相应地从外部数据库读取数据或直接查询应用的内部状态。
如下图所示，Apache Flink 同时支持流式及批量分析应用。
流式分析应用的优势？ # 和批量分析相比，由于流式分析省掉了周期性的数据导入和查询过程，因此从事件中获取指标的延迟更低。不仅如此，批量查询必须处理那些由定期导入和输入有界性导致的人工数据边界，而流式查询则无须考虑该问题。</description>
    </item>
    
    <item>
      <title>Downloads</title>
      <link>https://flink.apache.org/zh/downloads/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/downloads/</guid>
      <description>Apache Flink® Downloads # Apache Flink # Apache Flink® 1.18.0 是我们最新的稳定版本。
Apache Flink 1.18.1 # Apache Flink 1.18.1 (asc, sha512)
Apache Flink 1.18.1 Source Release (asc, sha512)
Release Notes # Please have a look at the Release Notes for Apache Flink 1.18.1 if you plan to upgrade your Flink setup from a previous version.
Apache Flink 1.17.2 # Apache Flink 1.17.2 (asc, sha512)
Apache Flink 1.17.2 Source Release (asc, sha512)</description>
    </item>
    
    <item>
      <title>Flink 用户</title>
      <link>https://flink.apache.org/zh/what-is-flink/powered-by/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink/powered-by/</guid>
      <description>Powered By Flink # Apache Flink 为全球许多公司和企业的关键业务提供支持。在这个页面上，我们展示了一些著名的 Flink 用户，他们在生产中运行着有意思的用例，并提供了展示更详细信息的链接。
在项目的 wiki 页面中有一个 谁在使用 Flink 的页面，展示了更多的 Flink 用户。请注意，该列表并不全面。我们只添加明确要求列出的用户。
如果你希望加入此页面，请通过 Flink 用户邮件列表 告诉我们。
全球最大的零售商阿里巴巴（Alibaba）使用 Flink 的分支版本 Blink 来优化实时搜索排名。 阅读更多有关 Flink 在阿里巴巴扮演角色的信息 Amazon Managed Service for Apache Flink 是一项完全托管的 Amazon 服务；可以让您能够使用Apache Flink来处理和分析流数据。 BetterCloud 是一个多 SaaS 管理平台，它使用 Flink 从 SaaS 应用程序活动中获取近乎实时的智能。 请参阅 BetterCloud 在 Flink Forward SF 2017 上的分享 Bouygues Telecom 正在运行由 Flink 提供支持的 30 个生产应用程序，每天处理 100 亿个原始事件。 请参阅 Bouygues Telecom 在 Flink Forward 2016 上的分享 财富 500 强金融服务公司 Capital One 使用 Flink 进行实时活动监控和报警。 了解 Capital One 的欺诈检测用例 康卡斯特（Comcast）是一家全球媒体和技术公司，它使用 Flink 来实现机器学习模型和近实时事件流处理。 了解 Flink 在康卡斯特的应用 Criteo 是开放互联网的广告平台，使用 Flink 进行实时收入监控和近实时事件处理。 了解 Criteo 的 Flink 用例 滴滴出行是全球卓越的移动出行平台，使用 Apache Flink支持了实时监控、实时特征抽取、实时ETL等业务。 了解滴滴如何使用 Flink 的。 Drivetribe是由前“Top Gear”主持人创建的数字社区，它使用 Flink 作为指标和内容推荐。 了解 Flink 在 Drivetribe stack 的应用 Ebay 的监控平台由 Flink 提供支持，可在指标和日志流上计算上千条自定义报警规则。 了解更多 Flink 在 Ebay 的信息 爱立信使用 Flink 构建了一个实时异常检测器，通过大型基础设施进行机器学习。 阅读关于O&amp;rsquo;Reilly想法的详细概述 Gojek 是一个超级 App: 拥有超过20种服务，并使用 Flink 为其自助平台提供支持，从而实现跨功能的数据驱动决策。 更多信息请访问 Gojek 工程师博客 华为是全球领先的 ICT 基础设施和智能设备供应商。华为云提供基于 Flink 的云服务。 了解Flink 如何为云服务提供动力 King，Candy Crush Saga的创建者，使用 Flink 为数据科学团队提供实时分析仪表板。 阅读 King 的 Flink 实现 Klaviyo使用 Apache Flink 扩展其实时分析系统，该系统每秒对超过一百万个事件进行重复数据删除和聚合。 阅读 Klaviyo 的实时分析 快手是中国领先的短视频分享 App，使用了 Apache Flink 搭建了一个实时监控平台，监控短视频和直播的质量。 阅读 Flink 在快手的应用实践 Lyft 使用 Flink 作为其流媒体平台的处理引擎，例如为机器学习持续生成特征。 阅读更多关于 Lyft 的流媒体 MediaMath 是一家程序化营销公司，它使用 Flink 为其实时报告基础架构提供支持。 请参阅 MediaMath 在 Flink Forward SF 2017 上的分享 Mux 是一家流媒体视频提供商的分析公司，它使用 Flink 进行实时异常检测和报警。 详细了解 Mux 如何使用 Flink OPPO, 作为中国最大的手机厂商之一，利用 Apache Flink 构建了实时数据仓库，用于即时分析运营活动效果及用户短期兴趣。 了解 OPPO 如何使用 Flink 全球第二大在线零售商奥托集团（Otto Group）使用 Flink 进行商业智能流处理。 请参阅 Otto 在 Flink Forward 2016 上的分享 OVH 使用 Flink 开发面向流的应用程序，比如实时商业智能系统或警报系统。 详细了解 OVH 如何使用 Flink Pinterest 使用基于 Apache Flink 的实时实验分析平台每天进行上千次的实验。 阅读更多在 Pinterest 有关实时实验分析的信息 Razorpay 是印度最大的支付门户网站之一，使用 Flink 构建了自己的内部平台 Mitra，用以扩展 AI 特征生成和实时模型服务。 阅读更多在 Razorpay 有关 Flink 数据分析的信息 ResearchGate 是科学家的社交网络，它使用 Flink 进行网络分析和近似重复检测。 请参阅 ResearchGate 在 Flink Forward 2016 上的分享 三星（SK telecom）是韩国最大的无线运营商。它在很多应用中使用了 Flink，包括智能工厂和移动应用程序。 了解其中一个 SK telecom 的使用案例。 Telefónica NEXT 的 TÜV 认证数据匿名平台由 Flink 提供支持。 了解更多关于 Telefónica NEXT 的信息 作为最大的互联网公司之一，腾讯利用 Apache Flink 构建了一个内部平台，以提高开发和操作实时应用程序的效率。 阅读有关腾讯平台的更多信息。 Uber 在 Apache Flink 上构建了基于 SQL 的开源流媒体分析平台 AthenaX。 更多信息请访问Uber工程博客 Vip，中国最大的品牌特卖网站之一，应用Flink实时的将数据流ETL到Hive中用于数据处理和分析.</description>
    </item>
    
    <item>
      <title>ML $FlinkMLStableShortVersion (stable)</title>
      <link>https://flink.apache.org/zh/documentation/flinkml-stable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/flinkml-stable/</guid>
      <description> Flink ML documentation (latest stable release) # You can find the Flink ML documentation for the latest stable release here. </description>
    </item>
    
    <item>
      <title>With Paimon(incubating) (formerly Flink Table Store)</title>
      <link>https://flink.apache.org/zh/getting-started/with-paimon-incubating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/getting-started/with-paimon-incubating/</guid>
      <description> Getting Started with Apache Paimon(incubating) (formerly Flink Table Store) # Read how you can get started with Apache Paimon(incubating) (formerly Flink Table Store) here. </description>
    </item>
    
    <item>
      <title>ML Master (snapshot)</title>
      <link>https://flink.apache.org/zh/documentation/flinkml-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/flinkml-master/</guid>
      <description> Flink ML documentation (latest snapshot) # You can find the Flink ML documentation for the latest snapshot here. </description>
    </item>
    
    <item>
      <title>Training Course</title>
      <link>https://flink.apache.org/zh/getting-started/training-course/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/getting-started/training-course/</guid>
      <description> Training Course # Read all about the Flink Training Course here. </description>
    </item>
    
    <item>
      <title>开发计划</title>
      <link>https://flink.apache.org/zh/what-is-flink/roadmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink/roadmap/</guid>
      <description>Roadmap # 导读： 此计划路线图旨在对Flink社区当前正在进行的项目进行总结摘要，并对这些项目根据工作内容进行分组。 鉴于Flink每个分组中现在都有非常多的工作正在进行，我们希望此计划书有助于用户和贡献者理解每个项目乃至于整个Flink的未来方向。 这个计划书既涵盖刚起步的项目，也包括接近完成的项目，这样可以使大家更好地了解各项目的发展方向以及当前的进展。
关于各个项目更多的细节讨论和其他较小的改动记录在 FLIPs 。
路线图会不断更新。一旦达成共识，新的特性和工作都会添加到路线图中。 这里的共识是指这些特性和工作将来确定会发生，以及这些工作对于用户来说大致是什么样的。
Last Update: 2023-09-01
功能图谱 # 功能图谱旨在为用户提供有关功能成熟度方面的引导，包括哪些功能正在积极推进，哪些功能即将寿终正寝。 如有任何疑问，请联系开发人员邮件列表：dev@flink.apache.org 。
功能阶段 # MVP: 可以了解一下这个功能，也许在将来对您有所帮助。 Beta: 您可以从中受益，但是您在使用之前应该仔细评估该功能。 Ready and Evolving: 生产可用，但是请注意，将来在升级Flink时，可能需要对您的应用和设置进行一些调整。 Stable: 可以在生产中稳定不受限制地使用。 Approaching End-of-Life: 仍然可以稳定使用，但请考虑替代方法。对于新的长期项目而言，不建议使用。 Deprecated: 不推荐使用，您需要开始寻找替代产品。 Scenarios We Focus On # Batch / Streaming Unification and Mixing # Flink is a streaming data system in its core, that executes “batch as a special case of streaming”. Efficient execution of batch jobs is powerful in its own right; but even more so, batch processing capabilities (efficient processing of bounded streams) open the way for a seamless unification of batch and streaming applications.</description>
    </item>
    
    <item>
      <title>Kubernetes Operator $FlinkKubernetesOperatorStableShortVersion (latest)</title>
      <link>https://flink.apache.org/zh/documentation/flink-kubernetes-operator-stable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/flink-kubernetes-operator-stable/</guid>
      <description> Flink Kubernetes Operator documentation (latest stable release) # You can find the Flink Kubernetes Operator documentation for the latest stable release here. </description>
    </item>
    
    <item>
      <title>社区 &amp; 项目信息</title>
      <link>https://flink.apache.org/zh/what-is-flink/community/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink/community/</guid>
      <description>社区 &amp;amp; 项目信息 # 如何从 Apache Flink 获得帮助？ # 我们可以通过多种方式从 Apache Flink 社区获得帮助。Flink committer 主要活跃在 邮件列表。对于用户支持和问题咨询，则可以通过 用户邮件列表 获得帮助。你还可以加入社区专属的 Slack。有些 Committer 同时会关注 Stack Overflow。请在提问的时候记得添加 apache-flink 的标签。问题反馈以及新特性的讨论则可以在 开发邮件列表 或者 Jira 上进行讨论。有兴趣对 Flink 进行贡献的人请查阅 贡献指南.
邮件列表 # 名字 订阅 摘要 退订 发送 归档 news@flink.apache.org
Flink 社区的新闻和公告 订阅 订阅 退订 只读邮件列表 归档 community@flink.apache.org
与会议，博客以及工作机会相关的更广泛的社区讨论	订阅 订阅 退订 发送 归档 user@flink.apache.org
用户支持以及问题咨询邮件列表 订阅 订阅 退订 发送 归档 user-zh@flink.apache.org
中文用户支持以及问题咨询邮件列表 订阅 订阅 退订 发送 归档 dev@flink.</description>
    </item>
    
    <item>
      <title>Kubernetes Operator Main (snapshot)</title>
      <link>https://flink.apache.org/zh/documentation/flink-kubernetes-operator-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/flink-kubernetes-operator-master/</guid>
      <description> Flink Kubernetes Operator documentation (latest snapshot) # You can find the Flink Kubernetes Operator documentation for the latest snapshot here. </description>
    </item>
    
    <item>
      <title>Security</title>
      <link>https://flink.apache.org/zh/what-is-flink/security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink/security/</guid>
      <description>Security # Security Updates # This section lists fixed vulnerabilities in Flink.
CVE ID Affected Flink versions Notes CVE-2020-1960 1.1.0 to 1.1.5, 1.2.0 to 1.2.1, 1.3.0 to 1.3.3, 1.4.0 to 1.4.2, 1.5.0 to 1.5.6, 1.6.0 to 1.6.4, 1.7.0 to 1.7.2, 1.8.0 to 1.8.3, 1.9.0 to 1.9.2, 1.10.0 Users are advised to upgrade to Flink 1.9.3 or 1.10.1 or later versions or remove the port parameter from the reporter configuration (see advisory for details).</description>
    </item>
    
    <item>
      <title>Paimon(incubating) (formely Flink Table Store) Master</title>
      <link>https://flink.apache.org/zh/documentation/paimon-incubating-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/documentation/paimon-incubating-master/</guid>
      <description> Paimon(incubating) (formely Flink Table Store) documentation (latest snapshot) # You can find the Paimon(incubating) (formely Flink Table Store) documentation for the latest snapshot here. </description>
    </item>
    
    <item>
      <title>如何参与贡献</title>
      <link>https://flink.apache.org/zh/how-to-contribute/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/overview/</guid>
      <description> 如何参与贡献 # Apache Flink 是由一个开放友好的社区开发的。我们诚挚地欢迎每个人加入社区并为 Apache Flink 做出贡献。与社区交流和为 Flink 做贡献的方式包括：提问题、报告 bug、提议新特性、参与邮件列表的讨论、贡献代码或文档、改进网站和测试候选发布版本。
你想做什么？ # 为 Apache Flink 做贡献不仅仅包括贡献代码。下面列出来不同的贡献形式：
可以贡献的领域 详细说明 报告 Bug 要报告 Flink 的问题，请登录 Flink’s Jira，然后点击顶部红色的 Create 按钮。 请提供你遇到问题的详细信息，如果可以，请附上能够帮助我们复现问题的描述。 贡献代码 请阅读 </description>
    </item>
    
    <item>
      <title>贡献代码</title>
      <link>https://flink.apache.org/zh/how-to-contribute/contribute-code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/contribute-code/</guid>
      <description>贡献代码 # Apache Flink 是一个通过志愿者贡献的代码来维护、改进和扩展的项目。我们欢迎给 Flink 做贡献，但由于项目的规模大，以及为了保持高质量的代码库，我们要求贡献者遵循本文所阐述的贡献流程。
请随时提出任何问题！ 可以发送邮件到 dev mailing list，也可以对正在处理的 Jira issue 发表评论。
重要提示：在开始准备代码贡献之前，请仔细阅读本文档。请遵循如下所述的流程和指南，为 Apache Flink 做贡献并不是从创建 pull request 开始的。我们希望贡献者先和我们联系，共同讨论整体方案。如果没有与 Flink committers 达成共识，那么贡献可能需要大量返工或不予审核通过。
寻找可贡献的内容 # 如果你已经有好的想法可以贡献，可以直接参考下面的 &amp;ldquo;代码贡献步骤&amp;rdquo;。 如果你在寻找可贡献的内容，可以通过 Flink 的问题跟踪列表 浏览处于 open 状态且未被分配的 Jira 工单，然后根据 &amp;ldquo;代码贡献步骤&amp;rdquo; 中的描述来参与贡献。 如果你是一个刚刚加入到 Flink 项目中的新人，并希望了解 Flink 及其贡献步骤，可以浏览 适合新手的工单列表 。 这个列表中的工单都带有 starter 标记，适合新手参与。
代码贡献步骤 # 注意：最近（2019 年 6 月），代码贡献步骤有改动。社区决定将原来直接提交 pull request 的方式转移到 Jira 上，要求贡献者在创建 pull request 之前需在 Jira 上达成共识（通过分配到的工单来体现），以减轻 PR review 的压力。 1讨论 在 Jira 上创建工单或邮件列表讨论并达成共识</description>
    </item>
    
    <item>
      <title>审核 Pull Request</title>
      <link>https://flink.apache.org/zh/how-to-contribute/reviewing-prs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/reviewing-prs/</guid>
      <description>如何审核 Pull Request # 本指南适用于希望帮助审核代码的所有提交者和贡献者。感谢你的努力 - 良好的审核是开源项目中最重要也是最关键的部分之一。本文旨在协助社区开展代码审核工作，以达到下列目的：
让贡献者拥有良好的贡献体验。 将审核过程结构化，以涵盖所有需要检查的重要方面。 保持 Flink 代码的高质量。 避免贡献者和审核者花费大量时间完善代码却最终被拒绝提交的情况。 审核清单 # 每次审核都需要检查以下六个方面。 我们建议按照以下顺序进行检查，以避免在还没有就是否添加某项功能或需要改动达成共识之前或没有满足一些正式条件前，就花费时间进行详细的代码质量审核。
1. 贡献的描述是否清晰？ # 检查贡献是否有充分的描述以方便审核，不重要的更改和修复不需要很长的描述。如果实现方案完全是按照之前在 Jira 或 dev 邮件列表上讨论结论进行的话，只需要一个对讨论的简短的引用即可。 如果实现方案与之前达成一致的方案不同的话，关于实现的详细描述是需要的，以便 review 贡献时更深入地讨论。
任何改变功能或行为的 pull request 都需要描述这些改变的重点, 以便知道审核什么内容(并且不必钻研代码来了解更改的作用)。
如果在不查看代码的情况下能回答以下问题2、3、4，则该贡献得到了很好的描述。
2. 是否一致认为这一变更或者功能应该进入 Flink？ # 这个问题要直接在关联的 Jira issue 中回答。对于在达成一致前创建的 pull request 来说，需要先在 Jira 中寻求一致的意见。
对于 [hotfix] 类型的的 pull request，可以在 pull request 中寻求意见一致。
3. 贡献是否需要一些特定的 committer 的关注，这些 committer 有时间投入吗？ # 一些更改需要特定的 committer 的注意和批准。例如，对性能非常敏感或对分布式协调和容错有关键影响的部件中的更改，这需要一个对相应组件非常熟悉的 committer 的审核。
根据经验，当 pull request 描述中对模板里问题 “Does this pull request potentially affect one of the following parts” 的回答为 “yes” 时，需要特别注意。</description>
    </item>
    
    <item>
      <title>代码样式与质量指南</title>
      <link>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-preamble/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-preamble/</guid>
      <description>Apache Flink Code Style and Quality Guide # 序言 # Pull Requests &amp;amp; Changes # 常用编码指南 # Java 语言指南 # Scala 语言指南 # 组件指南 # 格式指南 # 这是对我们想要维护的代码和质量标准的一种尝试。
一次代码贡献(或者任何代码片段)可以从很多角度进行评价：一组评判标准是代码是否正确和高效。这需要正确且良好的解决逻辑或算法问题。
另一组评判标准是代码是否使用了简洁的设计和架构，是否通过概念分离实现了良好的架构，是否足够简单易懂并且明确假设。该评判标准需要良好的解决软件工程问题。一个好的解决方案需要代码是容易被测试的，可以被除了原作者之外的其他人维护的（因为打破之后再维护是非常困难的），同时还需要能够高效的迭代演进的。
不过第一组标准有相当客观的达成条件，相比之下要达到第二组评判标准更加困难，但是对于 Apache Flink 这样的开源项目来说却非常重要。为了能够吸引更多贡献者，为了的开源贡献能够更容易被开发者理解，同时也为了众多开发者同时开发时代码的健壮性，良好工程化的代码是至关重要的。1 对于良好的工程代码来说，更加容易保证代码的正确性和高效不会随着时间的推移受到影响
当然，本指南并不是一份如何写出良好的工程代码的全方位指导。有相当多的书籍尝试说明如何实现良好的代码。本指南只是作为最佳实践的检查清单，包括我们在开发 Flink 过程中遇到的模式，反模式和常见错误。
高质量的开源代码很大一部分是关于帮助 reviewer 理解和双重检查执行结果。所以，本指南的一个重要目的是关于如何为为 review 构建一个良好的 pull request
在早期，我们（Flink 社区）并没有一直对此给予足够的重视，导致 Flink 的一些组件更难进化和贡献。&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    
    <item>
      <title>贡献文档</title>
      <link>https://flink.apache.org/zh/how-to-contribute/contribute-documentation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/contribute-documentation/</guid>
      <description>贡献文档 # 良好的文档对任何类型的软件都至关重要。这对于复杂的软件系统尤其如此，例如 Apache Flink 这样的分布式数据处理引擎。Apache Flink 社区旨在提供简明、精确和完整的文档，并欢迎任何改进 Apache Flink 文档的贡献。
获取文档资源 # Apache Flink 的文档和代码保存在相同的 git 仓库中。这样做是为了确保代码和文档可以轻松保持同步。
贡献文档的最简单方法是在 GitHub 上 Flink 的镜像仓库 页面，通过单击右上角的 fork 按钮讲 Flink 克隆到你自己的 GitHub 帐户中。如果你没有 GitHub 帐户，可以免费创建一个帐户。
接下来，将 fork 的代码克隆到本地计算机。
git clone https://github.com/&amp;lt;your-user-name&amp;gt;/flink.git 文档位于 Flink 代码库的 docs/ 子目录中。
在开始贡献文档之前… # …请确保已经有一个相对应的 Jira issue 存在了。我们要求所有文档更改都需要关联一个 Jira issue，除了一些微不足道的修复，如拼写错误。
同时，先阅读一下 文档样式指南 能够很好的帮助你写出易懂、连贯和全面的文档。
更新或扩展文档 # Flink 文档是用 Markdown 编写的。Markdown 是一种轻量级标记语言，可以通过工具转化成 HTML。
为了更新或扩展文档，你必须修改 Markdown (.md) 文件。请通过在预览模式下启动构建脚本来验证你的更改。
./build_docs.sh -p 该脚本会将 Markdown 文件编译成静态 HTML 页面并在本地启动一个 Web 服务器。在浏览器中打开 http://localhost:1313/ ，查看包含更改文档页面。当你修改并保存 Markdown 文件，然后刷新浏览器，修改过的文档将自动被重新编译和更新。</description>
    </item>
    
    <item>
      <title>文档样式指南</title>
      <link>https://flink.apache.org/zh/how-to-contribute/documentation-style-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/documentation-style-guide/</guid>
      <description>文档样式指南 # 本指南概述了在编辑以及贡献 Flink 文档中必要的样式原则。目的是在你的贡献之旅中可以投入更好的社区精力去改进和扩展既有文档，并使其更 易读、一致 和 全面。
语言 # Flink 同时维护了 英文 和 中文 两种文档，当你拓展或者更新文档时，需要在 pull request 中包含两种语言版本。如果你不熟悉中文，确保本次贡献补充了如下额外操作：
开一个翻译的 JIRA 请求单，并打上 chinese-translation 的标签； 在此请求单上添加到原始 JIRA 请求单的链接。 正在寻求有助于将现有文档翻译成中文的风格指南？请继续查阅 这个翻译规范。
语言风格 # 如下，你可以看到一些初步的原则，这些原则可以确保书写中的可读性和通俗易懂。如果想更深入、更细致的了解语言风格，也可以参考 通用准则。
语态和语气 # 使用主动语态。主动语态简洁，并让内容更具有吸引力。如果你在句子的动词后添加 by zombies 后仍然读的通，那么你用的就是被动语态。
主动语态 &amp;ldquo;You can run this example in your IDE or on the command line.&amp;rdquo; 被动语态 &amp;ldquo;This example can be run in your IDE or on the command line (by zombies).&amp;rdquo; 关于语态</description>
    </item>
    
    <item>
      <title>贡献网站</title>
      <link>https://flink.apache.org/zh/how-to-contribute/improve-website/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/improve-website/</guid>
      <description>改进网站 # Apache Flink 官网 介绍了 Apache Flink 及其社区。包括如下多种用途：
向来访者介绍 Apache Flink 及其特性。 鼓励来访者下载并使用 Flink。 鼓励来访者与社区进行互动。 我们欢迎任何改进官网的贡献。本文档包含了所有改进 Flink 官网所需要的信息。
获取官网源码 # Apache Flink 官网的源码托管在专用的 git 仓库中，并在 Github 中有一个镜像 https://github.com/apache/flink-web.
向官网贡献的最简单方式是通过单击右上角的 fork 按钮，将 Github 上官网的镜像 镜像到自己的仓库中。如果没有 Github 帐户，你可以免费创建一个。
接下来，把你镜像的仓库克隆到本地机器上。
git clone https://github.com/&amp;lt;your-user-name&amp;gt;/flink-web.git flink-web 目录包含了拷贝的仓库。官网的代码位于 asf-site 分支上。运行如下命令切换到 asf-site 分支
cd flink-web git checkout asf-site 目录结构和文件 # Flink 官网使用 Markdown 语言。Markdown 是一种轻量级标记语言，可以转换为 HTML。我们使用 Hugo 从 Markdown 生成静态 HTML 文件。
flink-web git 仓库中的文件和目录具有以下作用：</description>
    </item>
    
    <item>
      <title>获取帮助</title>
      <link>https://flink.apache.org/zh/how-to-contribute/getting-help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/getting-help/</guid>
      <description>获取帮助 # 有问题吗? # Apache Flink 社区每天都会回答许多用户的问题。你可以从历史存档中搜索答案和建议，也可以联系社区寻求帮助和指导。
用户邮件列表 # 许多 Flink 用户、贡献者和提交者都订阅了 Flink 的用户邮件列表。用户邮件列表是一个寻求帮助的好地方。
在发送邮件到邮件列表之前，你可以搜索以下网站的邮件列表存档，从中找到你关注问题的相关讨论。
Apache Pony 邮件存档 如果你想发送到邮件列表，你需要：
发送电子邮件至 user-subscribe@flink.apache.org 来订阅邮件列表 通过回复确认邮件来确认订阅 发送你的电子邮件到 user@flink.apache.org. 请注意，如果你没有订阅邮件列表，你将不会收到邮件的回复。
Slack # 你可以通过 此链接 加入 Apache Flink 社区专属的 Slack 工作空间。 在成功加入后，不要忘记在 #introductions 频道介绍你自己。 Slack 规定每个邀请链接最多可邀请 100 人，如果遇到上述链接失效的情况，请联系 Dev 邮件列表。 所有已经加入社区 Slack 空间的成员同样可以邀请新成员加入。
在 Slack 空间交流时，请遵守以下规则：
保持尊重 - 这是最重要的规则 所有重要的决定和结论 必须在邮件列表中有所体现。 &amp;ldquo;没有发生在邮件列表上的事情，即视为没有发生。&amp;rdquo; - Apache 准则 使用 Slack 消息列（Thread 使频道（Channel）中的多组同时进行的对话保持有序。 Use either #pyflink (for all Python Flink questions) or #troubleshooting (for all other Flink questions).</description>
    </item>
    
    <item>
      <title>Apache Flink 代码样式和质量指南 — 组件</title>
      <link>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-components/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-components/</guid>
      <description>Apache Flink 代码样式和质量指南 — 组件 # 序言 # Pull Requests &amp;amp; Changes # 常用编码指南 # Java 语言指南 # Scala 语言指南 # 组件指南 # 格式指南 # 组件特定指南 # 关于特定组件更改的附加指南。
配置更改 # 配置选项应该放在哪里？
‘flink-conf.yaml’: 所有属于可能要跨作业标准的执行行为的配置。可以将其想像成 Ops 的工作人员或为其他团队提供流处理平台的工作人员设置的参数。 ‘ExecutionConfig’: 执行期间算子需要特定于单个 Flink 应用程序的参数，典型的例子是水印间隔，序列化参数，对象重用。 ExecutionEnvironment (in code): 所有特定于单个 Flink 应用程序的东西，仅在构建程序/数据流时需要，在算子执行期间不需要。 如何命名配置键：
配置键名应该分层级。将配置视为嵌套对象（JSON 样式）
taskmanager: { jvm-exit-on-oom: true, network: { detailed-metrics: false, request-backoff: { initial: 100, max: 10000 }, memory: { fraction: 0.1, min: 64MB, max: 1GB, buffers-per-channel: 2, floating-buffers-per-gate: 16 } } } 因此生成的配置键应该：</description>
    </item>
    
    <item>
      <title>Code Style and Quality Guide — Common Rules</title>
      <link>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-common/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-common/</guid>
      <description>Code Style and Quality Guide — Common Rules # 序言 # Pull Requests &amp;amp; Changes # 常用编码指南 # Java 语言指南 # Scala 语言指南 # 组件指南 # 格式指南 # 1. Copyright # Each file must include the Apache license information as a header.
/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.</description>
    </item>
    
    <item>
      <title>Code Style and Quality Guide — Formatting Guide</title>
      <link>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-formatting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-formatting/</guid>
      <description>Code Style and Quality Guide — Formatting Guide # 序言 # Pull Requests &amp;amp; Changes # 常用编码指南 # Java 语言指南 # Scala 语言指南 # 组件指南 # 格式指南 # Java Code Formatting Style # We recommend to set up the IDE to automatically check the code style. Please follow the IDE Setup Guide to set up spotless and checkstyle .
License # Apache license headers. Make sure you have Apache License headers in your files.</description>
    </item>
    
    <item>
      <title>Code Style and Quality Guide — Java</title>
      <link>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-java/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-java/</guid>
      <description>Code Style and Quality Guide — Java # 序言 # Pull Requests &amp;amp; Changes # 常用编码指南 # Java 语言指南 # Scala 语言指南 # 组件指南 # 格式指南 # Java Language Features and Libraries # Preconditions and Log Statements # Never concatenate strings in the parameters Don’t: Preconditions.checkState(value &amp;lt;= threshold, &amp;quot;value must be below &amp;quot; + threshold) Don’t: LOG.debug(&amp;quot;value is &amp;quot; + value) Do: Preconditions.checkState(value &amp;lt;= threshold, &amp;quot;value must be below %s&amp;quot;, threshold) Do: LOG.</description>
    </item>
    
    <item>
      <title>Code Style and Quality Guide — Pull Requests &amp; Changes</title>
      <link>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-pull-requests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-pull-requests/</guid>
      <description>Code Style and Quality Guide — Pull Requests &amp;amp; Changes # 序言 # Pull Requests &amp;amp; Changes # 常用编码指南 # Java 语言指南 # Scala 语言指南 # 组件指南 # 格式指南 # Rationale: We ask contributors to put in a little bit of extra effort to bring pull requests into a state that they can be more easily and more thoroughly reviewed. This helps the community in many ways:
Reviews are much faster and thus contributions get merged sooner.</description>
    </item>
    
    <item>
      <title>Code Style and Quality Guide — Scala</title>
      <link>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-scala/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/how-to-contribute/code-style-and-quality-scala/</guid>
      <description>Code Style and Quality Guide — Scala # 序言 # Pull Requests &amp;amp; Changes # 常用编码指南 # Java 语言指南 # Scala 语言指南 # 组件指南 # 格式指南 # Scala 语言特性 # 在哪儿使用（和不使用） Scala # 对于 Scala 的 API 或者纯 Scala libraries，我们会选择使用 Scala。
在 core API 和 运行时的组件中，我们不使用 Scala。我们的目标是从这些组件中删除现有的 Scala 使用(代码和依赖项)。
⇒ 这并不是因为我们不喜欢 Scala，而是考虑到“用正确的工具做正确的事”的结果（见下文）。
对于 API，我们使用 Java 开发基础内容，并在上层使用 Scala。
这在传统上为 Java 和 Scala 提供了最佳的互通性 这意味着要致力于保持 Scala API 的更新 为什么我们不在 Core API 和 Runtime 中使用 Scala ？</description>
    </item>
    
    <item>
      <title>flink-packages.org</title>
      <link>https://flink.apache.org/zh/flink-packages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/flink-packages/</guid>
      <description>What is the Flink Kubernetes Operator? # All information on the flink-packages can be found on the flink-packages website.</description>
    </item>
    
    <item>
      <title>What is Flink ML?</title>
      <link>https://flink.apache.org/zh/what-is-flink-ml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink-ml/</guid>
      <description>What is Stateful Functions? # All information on Flink ML can be found on the Flink ML website.</description>
    </item>
    
    <item>
      <title>What is Paimon(incubating) (formerly Flink Table Store)?</title>
      <link>https://flink.apache.org/zh/what-is-flink-table-store/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-flink-table-store/</guid>
      <description>What is Apache Paimon (formerly Flink Table Store)? # The Flink Table Store had joined Apache Incubator as Apache Paimon(incubating). All information on the Apache Paimon(incubating) can be found on the Paimon website.</description>
    </item>
    
    <item>
      <title>What is Stateful Functions?</title>
      <link>https://flink.apache.org/zh/what-is-stateful-functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-stateful-functions/</guid>
      <description>What is Stateful Functions? # All information on Stateful Functions can be found on the Stateful Functions project website.</description>
    </item>
    
    <item>
      <title>What is the Flink Kubernetes Operator?</title>
      <link>https://flink.apache.org/zh/what-is-the-flink-kubernetes-operator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/what-is-the-flink-kubernetes-operator/</guid>
      <description>What is the Flink Kubernetes Operator? # All information on the Flink Kubernetes Operator can be found on the Flink Kubernetes Operator website.</description>
    </item>
    
    <item>
      <title>素材</title>
      <link>https://flink.apache.org/zh/material/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://flink.apache.org/zh/material/</guid>
      <description>Material # Apache Flink Logos # 我们提供不同大小、不同格式的 Apache Flink logo。你可以 下载所有 logo (7.4 MB)或者从当前页面选择你需要的 logo。
便携式网络图像格式 (PNG) # 彩色 logo 白色填充 logo 黑色轮廓 logo 尺寸 (px): 50x50, 100x100, 200x200, 500x500, 1000x1000 尺寸 (px):: 50x50, 100x100, 200x200, 500x500, 1000x1000
尺寸 (px):: 50x50, 100x100, 200x200, 500x500, 1000x1000 你可以在这个目录 寻找更多 PNG 格式的 logo 或者 下载所有 logo (7.4 MB)。
可扩展矢量图像格式 (SVG) # 彩色 logo 白色填充 logo 黑色轮廓 logo 带黑色文本的彩色 logo (color_black.svg) 白色填充 logo (white_filled.</description>
    </item>
    
  </channel>
</rss>
